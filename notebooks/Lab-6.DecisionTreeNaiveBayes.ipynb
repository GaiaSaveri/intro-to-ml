{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0qGt1RNJ//DSArPqPDZF4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaiaSaveri/intro-to-ml/blob/main/notebooks/Lab-6.DecisionTreeNaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification woth Decision Trees and Naive Bayes "
      ],
      "metadata": {
        "id": "8Lwvjp5PUb6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "tpkP4x7pUkdX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import and pre-process the data using `pandas`.\n",
        "Data source: https://archive-beta.ics.uci.edu/dataset/73/mushroom\n",
        "\n",
        "For practicality, it also on github: https://github.com/GaiaSaveri/intro-to-ml/blob/main/data/agaricus-lepiota.data\n",
        "\n",
        "with associated informations: https://github.com/GaiaSaveri/intro-to-ml/blob/main/data/agaricus-lepiota.names"
      ],
      "metadata": {
        "id": "MCghdR1LUqSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide features and labels and split the dataset into train and test."
      ],
      "metadata": {
        "id": "trKzFVZ2aFVp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkF6XEd1aU22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees\n",
        "\n",
        "Make classification with **Decision Trees** using **ID3** (Iterative Dichotomiser 3)\n",
        "\n",
        "Recall: \n",
        "\n",
        "* the decision is built from the dataset and each node is used either to make a decision (inteernal node) or to represent an outcome (leaves);\n",
        "\n",
        "* ID3 is a top-down (i.e. the tree is constructed starting from the root) greedy (i.e. we consider only the current step in selecting best features) algorithm to build decision trees;\n",
        "\n",
        "* at each step features are divided into two or more groups by computing the **infromation gain**: the feature with the highest information gain is the best one.\n",
        "\n",
        "  Entropy: $H(S) = \\sum_{i=1}^D -p_i \\log p_i$, $p_i$ proportion of each category\n",
        "\n",
        "  Infromation Gain: $IG(S, j)=H(s) - \\sum_j \\frac{|S_j|}{|S|}H(S_j)$\n",
        "\n",
        "* a node is selected as leaf if all data in the node belong to the same class;\n",
        "\n",
        "* repeat until the tree has all leaf nodes (or features are over).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VZOkgoysVJpV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UH7kMkSOZKCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes\n",
        "\n",
        "Make classification using the **Naive Bayes** algorithm.\n",
        "\n",
        "Recall: \n",
        "\n",
        "* Bayes Theorem: $p(class|data) \\propto p(data|class)\\cdot p(class)$\n",
        "\n",
        "* prior ($p(class)$) is just the ratio of the number of datapoints belonging to the class;\n",
        "\n",
        "* to make predictions (i.e. compute the posterior $p(class|data)$) we consider the likelihood of each class ($p(data|class)$) to be a Gaussian $\\frac{1}{\\sqrt(\\sigma^2 2 \\pi)} \\exp (\\frac{1}{2} \\frac{(x-\\mu)}{\\sigma^2})$, where $\\mu, \\sigma^2$ are the mean and the variance for each class;\n",
        "\n",
        "* we work in the log-space so that predictions will be the class maximizing the sum of prior and likelihood.\n"
      ],
      "metadata": {
        "id": "3mo9W9qaZKVH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ets-ledrcTIL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}