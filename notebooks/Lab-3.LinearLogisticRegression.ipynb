{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaiaSaveri/intro-to-ml/blob/main/notebooks/Lab-3.LinearLogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning: Linear and Logistic Regression"
      ],
      "metadata": {
        "id": "n4CaXj2i45dD"
      },
      "id": "n4CaXj2i45dD"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7970569b",
      "metadata": {
        "id": "7970569b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy.linalg as LA\n",
        "from numpy.linalg import inv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cebeee6",
      "metadata": {
        "id": "2cebeee6"
      },
      "source": [
        "## Data Geneation (cfr Lab 1)\n",
        "\n",
        "Linear model $y=wx+\\varepsilon$, with $ɛ \\sim \\mathcal{N}(0,\\sigma)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb91a4f0",
      "metadata": {
        "id": "bb91a4f0"
      },
      "outputs": [],
      "source": [
        "def datagen(d, points, m, M, w, sigma):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    d : int\n",
        "        Dimension of each data sample\n",
        "    points : int\n",
        "        Number of points to be generated\n",
        "    m : float\n",
        "        Lower bound for the domain of the data points\n",
        "    M : float\n",
        "        Upper bound for the domain of the data points\n",
        "    w : float array of dim d \n",
        "        Vector of weights of the linear model\n",
        "    sigma : float\n",
        "        Standard deviation of the noise eps\n",
        "    \"\"\"\n",
        "    X = np.zeros((points, d))\n",
        "    for i in range(points):\n",
        "        X[i,:] = np.random.uniform(m, M, d)\n",
        "    eps = np.random.normal(0, sigma, points)\n",
        "    y = np.dot(X, w) + eps \n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6023feb",
      "metadata": {
        "id": "f6023feb"
      },
      "outputs": [],
      "source": [
        "# usage example \n",
        "d = 1\n",
        "w = np.random.normal(0, 1, d)\n",
        "sigma = 1\n",
        "points = 100\n",
        "m = -10\n",
        "M = 10\n",
        "\n",
        "X, y = datagen(d, points, m, M, w, sigma)\n",
        "\n",
        "# plotting the generated dataset\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X, y, c='b')\n",
        "ax.plot(X, np.dot(X, w), '--', c='r')\n",
        "ax.set_title('Data')\n",
        "plt.ylim([m, M])\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa3f9f3",
      "metadata": {
        "id": "dfa3f9f3"
      },
      "source": [
        "## Linear Regression \n",
        "\n",
        "Given a dataset $\\mathcal{D}=\\{(x_i, y_i)\\}_{i=1}^n ⊆ \\mathbb{R}^d \\times \\mathbb{R}$ we want to compute the best fitting line, i.e. the parameters $w$ of the line $y=w\\cdot X$.\n",
        "\n",
        "### Analytic Solution\n",
        "\n",
        "* **Ordinary Linear Regression (i.e. no regularization):**\n",
        "\n",
        "$$\n",
        "y=Xw,\\;\\;\\;\\; w=X^{-1}y,\\;\\;\\;\\;X^{-1}=(X^{T}X)^{-1}X^{T}\n",
        "$$\n",
        "\n",
        "$$\n",
        "w=(X^{T}X)^{-1}X^{T}y\n",
        "$$\n",
        "\n",
        "**Recall**: we cannot invert a matrix which is not squared, so we need to resort to the Moore-Penrose pseudo-inverse, i.e. $X^{-1}\\approx (X^TX)^{-1}X^T$\n",
        "\n",
        "* **Ridge Regression**\n",
        "\n",
        "  If $X^{T}X$ is not invertible (i.e. some eigenvalues are zero) then we regularize by adding $\\lambda \\mathbb{I}$, with $\\mathbb{I}$ being the identity matrix of size $d\\times d$ making the zero\n",
        "eigenvalues $\\lambda$.\n",
        "\n",
        "$$\n",
        "w=(X^{T}X+\\lambda Id)^{-1}X^{T}y\n",
        "$$\n",
        "\n",
        "  The line that we find with Ridge Regression takes the form: $y = w\\cdot X + \\lambda \\|w\\|_2$ (i.e. $L_2$ penalty)\n",
        "\n",
        "Interesting blog post showing this analytic derivation and its implementation: https://knork.medium.com/linear-regression-in-python-from-scratch-with-kernels-e9c37f7975b9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a0fa67",
      "metadata": {
        "id": "b9a0fa67"
      },
      "outputs": [],
      "source": [
        "def CloseRegression(X, y):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of float dim n x d\n",
        "        Matrix containing the dataset\n",
        "    y : array of float of dim n\n",
        "        Vector containing the ground truth value of each data point\n",
        "    \"\"\"\n",
        "    w = (inv(X.T@X)@X.T)@y \n",
        "    return w\n",
        "\n",
        "def CloseRegressionReg(X, y, lam):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of float dim n x d\n",
        "        Matrix containing the dataset\n",
        "    y : array of float of dim n\n",
        "        Vector containing the ground truth value of each data point\n",
        "    lam : float\n",
        "        Lambda term of Ridge regularization\n",
        "    \"\"\"\n",
        "    d = np.shape(X)[1]\n",
        "    w = (inv(X.T@X+lam*np.eye(d))@X.T)@y\n",
        "    return w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef54f955",
      "metadata": {
        "id": "ef54f955"
      },
      "outputs": [],
      "source": [
        "wclose = CloseRegression(X, y)\n",
        "lam = 0.1\n",
        "wcloseReg = CloseRegressionReg(X, y, lam)\n",
        "wcloseReg, wclose, np.abs(wclose-wcloseReg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6462a01b",
      "metadata": {
        "id": "6462a01b"
      },
      "outputs": [],
      "source": [
        "# plot the solution of both regressions\n",
        "xlin=np.linspace(-10,10,100)\n",
        "ypred= wclose*xlin\n",
        "ypredreg = wcloseReg*xlin\n",
        "\n",
        "fig,ax=plt.subplots()\n",
        "ax.plot(X,y,'o')\n",
        "ax.plot(X,np.dot(X,w), label='Gorund Truth')\n",
        "ax.plot(xlin,ypred,label='No Regularization')\n",
        "ax.plot(xlin,ypredreg, label='Ridge Regularization')\n",
        "plt.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Try to vary the parameter $\\lambda$ and check what happens to the magnitude of the found $w$ (try to inspect it visually with a plot)"
      ],
      "metadata": {
        "id": "9Y2sgenV_zPZ"
      },
      "id": "9Y2sgenV_zPZ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ps1N3mOY__XX"
      },
      "id": "ps1N3mOY__XX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "af5b951f",
      "metadata": {
        "id": "af5b951f"
      },
      "source": [
        "## Gradient Descent (GD) method\n",
        "\n",
        "**Recall**: the gradient indicates the direction of maximal ascent of a function. Hence to find the minimum of a function we have to follow the direction of the negative gradient. In GD we take a step of size $\\gamma$ (learning rate). Given a loss function $\\mathcal{L}(w)$ it works as:\n",
        "\n",
        "1. Initialize $w_0$\n",
        "2. while (condition):\n",
        "   * compute gradient of the loss at t $\\nabla_w \\mathcal{L}|_t$\n",
        "   * update weights as $w_{t+1} = w_t - \\gamma \\nabla_w \\mathcal{L}|_t $\n",
        "\n",
        "\n",
        "In the case of linear regression the squared erro loss and its gradient read as:\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathcal{L}=\\frac{1}{n}\\|y-Xw\\|_{2}^{2},\\;\\;\\;\\;\\nabla_{w} \\mathcal{L} = -\\frac{2}{n}X(y-Xw)\n",
        "$$\n",
        "\n",
        "As condition for the while loop we use a predefined number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d4428bd",
      "metadata": {
        "id": "5d4428bd"
      },
      "outputs": [],
      "source": [
        "def SquareLoss(X, y, w):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of float dim n x d\n",
        "        Matrix containing the dataset\n",
        "    y : array of float of dim n\n",
        "        Vector containing the ground truth value of each data point\n",
        "    w : array of float of dim d\n",
        "        Weights of the fitted line\n",
        "    \"\"\"\n",
        "    return LA.norm(y-X@w,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e9cbc9",
      "metadata": {
        "id": "83e9cbc9"
      },
      "outputs": [],
      "source": [
        "def GD(X, y, iter, gamma, points, d):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of float dim n x d\n",
        "        Matrix containing the dataset\n",
        "    y : array of float of dim n\n",
        "        Vector containing the ground truth value of each data point\n",
        "    iter : int\n",
        "        Number of GD iterations\n",
        "    gamma : float\n",
        "        Learning rate\n",
        "    points : int\n",
        "        Number of points in our dataset\n",
        "    d : int\n",
        "        Dimensionality of each data point in the dataset\n",
        "    \"\"\"\n",
        "    # initialization\n",
        "    W = np.zeros((d, iter)) # to store weights\n",
        "    L = np.zeros(iter) # to store losses\n",
        "    w = np.random.normal(0, 0.1, d)\n",
        "    # GD iterations\n",
        "    for i in range(iter):\n",
        "        W[:,i] = w\n",
        "        w = w + (2*gamma/points)*((y-X@w)@X)\n",
        "        L[i] = SquareLoss(X,y,w)\n",
        "    return W,L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1caaff9b",
      "metadata": {
        "id": "1caaff9b"
      },
      "outputs": [],
      "source": [
        "# example of usage\n",
        "d = np.shape(X)[1]\n",
        "iter = 1000\n",
        "n_points = 100\n",
        "gamma = 0.001\n",
        "\n",
        "wgd, L = GD(X, y, iter, gamma, points, d)\n",
        "# the last stored weights are the most updated ones\n",
        "wpred = wgd[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f406e89e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f406e89e",
        "outputId": "215f9f98-e561-4bba-a8c5-10698b19b4bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.98143046]), array([1.99881315]), array([0.01738269]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "w, wpred, np.abs(w-wpred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1250baa8",
      "metadata": {
        "id": "1250baa8"
      },
      "source": [
        "***Plot the Loss***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2bae5e6",
      "metadata": {
        "id": "c2bae5e6"
      },
      "outputs": [],
      "source": [
        "plt.plot(L)\n",
        "plt.xlabel('Iter')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Squared Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18365ceb",
      "metadata": {
        "id": "18365ceb"
      },
      "source": [
        "***Plot the predictions***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e053017",
      "metadata": {
        "id": "7e053017"
      },
      "outputs": [],
      "source": [
        "xlin = np.linspace(-10, 10, 100)\n",
        "ypred = wpred*xlin\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(X,y,'o')\n",
        "ax.plot(X,np.dot(X,w), label='Ground Truth')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.plot(xlin,ypred, label= 'Predicted')\n",
        "plt.legend()\n",
        "fig.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36765332",
      "metadata": {
        "id": "36765332"
      },
      "source": [
        "### Polynomial regression\n",
        "\n",
        "Polynomial relationship between $x$ and $y$:\n",
        "\n",
        "$$\n",
        "y = w_0 + \\sum_{j=1}^{\\text{deg}-1} w_j x^j\n",
        "$$\n",
        "\n",
        "The squared loss is: \n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w) = \\frac{1}{n} \\sum_{i=0}^n \\big(y_i - \\big(w_0 + \\sum_{j=1}^{\\text{deg}-1} w_j x_i^j\\big)\\big)^2 = \\frac{1}{n} \\sum_{i=0}^n \\big(y_i - \\hat{y}_i\\big)^2\n",
        "$$\n",
        "\n",
        "In order to perform GD, we need the partial derivative of $\\mathcal{L}$ w.r.t. each coefficient $w_j$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial\\mathcal{L}}{\\partial w_j} = - \\frac{2}{n} \\sum_{i=0}^n x_i^j \\cdot (y_i - \\hat{y}_i)\n",
        "$$\n",
        "\n",
        "Then each coefficient is updated separately: \n",
        "\n",
        "$$\n",
        "w_j^{(t+1)} = w_j^{(t)} - \\gamma \\frac{\\partial\\mathcal{L}}{\\partial w_j}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-10, 10, 100)\n",
        "deg = 3  # this is the actual degree - 1 because we have to consider the intercept\n",
        "w = np.random.rand(deg)\n",
        "y = np.polyval(w, x)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x, y, '-')\n",
        "fig.show();"
      ],
      "metadata": {
        "id": "tEjIm-SHbt7n"
      },
      "id": "tEjIm-SHbt7n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement GD for polynomial regression\n",
        "\n",
        "  **Hint**: start with polynomials of degree $2$ then try to generalize to polynomials of arbitrary degree"
      ],
      "metadata": {
        "id": "a_x6rtZTgdkV"
      },
      "id": "a_x6rtZTgdkV"
    },
    {
      "cell_type": "code",
      "source": [
        "def poly_squared_loss(x, y, w):\n",
        "  pass\n",
        "\n",
        "def poly_GD(X, y, iter, gamma, points, d):\n",
        "  pass"
      ],
      "metadata": {
        "id": "DCxnu29cf1c6"
      },
      "id": "DCxnu29cf1c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8372ae88",
      "metadata": {
        "id": "8372ae88"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "### Generate the data and plot (noisy) data (cfr Lab 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd080980",
      "metadata": {
        "scrolled": true,
        "id": "cd080980"
      },
      "outputs": [],
      "source": [
        "def mixGauss(means, sigmas, n):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    means : matrix/list of float of dim n_classes x dim_data (d)\n",
        "        Means of the Gaussian functions\n",
        "    sigmas : array/list of float of dim n_classes\n",
        "        Standard deviation of the Gaussian functions\n",
        "    n : int\n",
        "        Number of points for each class\n",
        "    \"\"\"\n",
        "    means = np.array(means)\n",
        "    sigmas = np.array(sigmas)\n",
        "\n",
        "    d = np.shape(means)[1] # the means matrix is of n_classes x dim_data\n",
        "    num_classes = sigmas.size # the number of variances is the number of classes\n",
        "    \n",
        "    data = np.full((n * num_classes, d), np.inf)\n",
        "    labels = np.zeros(n * num_classes)\n",
        "\n",
        "    # iterate over classes\n",
        "    for idx, sigma in enumerate(sigmas):\n",
        "        # generates n points around means[idx] with cov sigma[idx] \n",
        "        data[idx * n:(idx + 1) * n] = np.random.multivariate_normal(\n",
        "            mean=means[idx], cov=np.eye(d) * sigmas[idx] ** 2, size=n)   \n",
        "        labels[idx * n:(idx + 1) * n] = idx \n",
        "        \n",
        "    if(num_classes == 2):\n",
        "        labels[labels == 0] = -1\n",
        "\n",
        "    return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# usage example \n",
        "means = [[3,0],[0,6]]\n",
        "sigmas = [0.9,0.9]\n",
        "n = 100\n",
        "\n",
        "X, labels = mixGauss(means, sigmas, n)\n",
        "\n",
        "# plotting the generated dataset\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:,0], X[:,1], s=70, c=labels, alpha=0.8)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "V0muWNywhH7h"
      },
      "id": "V0muWNywhH7h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67c7ad6e",
      "metadata": {
        "id": "67c7ad6e"
      },
      "outputs": [],
      "source": [
        "def labelsnoise(perc, labels):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    perc : float \n",
        "        Percentage of labels to be flipped\n",
        "    labels: array of int of dim n_classes\n",
        "        Array containing labels idxs\n",
        "    \"\"\"\n",
        "    points = np.shape(labels)[0]\n",
        "    noisylabels = np.copy(np.squeeze(labels))\n",
        "    n_flips = int(np.floor(points * perc / 100)) # floor: nearest integer by defect\n",
        "    idx_to_flip = np.random.choice(points, size=n_flips, replace=False) # replace is false since the same index cannot be chosen twice\n",
        "    noisylabels[idx_to_flip] = -noisylabels[idx_to_flip] # for binary this turns -1 into 1 and viceversa\n",
        "    return noisylabels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noisylabels = labelsnoise(20, labels)\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:,0], X[:,1], s=70, c=noisylabels, alpha=0.8)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "2DMZCf2EhUJg"
      },
      "id": "2DMZCf2EhUJg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7d7524ee",
      "metadata": {
        "id": "7d7524ee"
      },
      "source": [
        "***Sigmoidal function***\n",
        "\n",
        "$$\n",
        "\\sigma(wx_i) = \\frac{1}{1 + e^{-wx_i}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9eeb3b5",
      "metadata": {
        "id": "a9eeb3b5"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x, w):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : array of dim n\n",
        "        Array containing the datapoint\n",
        "    w : float\n",
        "        Number representing the 'temperature' of the sigmoid\n",
        "    \"\"\"\n",
        "    y = 1/(1+np.exp(-np.dot(x,w)))\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# usage example\n",
        "x = np.linspace(-3,3,100)\n",
        "w = 3\n",
        "y = sigmoid(x, w)\n",
        "\n",
        "fig,ax=plt.subplots()\n",
        "ax.plot(x, y, label='sigmoid function')\n",
        "plt.legend();\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "fiYzknMoijUm"
      },
      "id": "fiYzknMoijUm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d0b44ba1",
      "metadata": {
        "id": "d0b44ba1"
      },
      "source": [
        "## Logistic Loss\n",
        "\n",
        "* For regression the hypothesis was: $h(x)=w^Tx$\n",
        "* For logistic regression is $h(x)=\\sigma(wx)=\\frac{1}{1+e^{-w^Tx}}$\n",
        "\n",
        "\n",
        "$$\n",
        "h(x) = \n",
        "\\begin{cases}\n",
        "  >0.5, & w^{T}x>0\\\\\n",
        "  <0.5, & w^{T}x<0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The goal of the logistic regression algorithm is to learn $w$. \n",
        "\n",
        "Case specific (logarithmic) cost:\n",
        "\n",
        "\n",
        "$$\n",
        "cost = \n",
        "\\begin{cases}\n",
        "  -log(h(x)), & y=1\\\\\n",
        "  -log(1-h(x)), & y=0\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45f66b5",
      "metadata": {
        "scrolled": true,
        "id": "e45f66b5"
      },
      "outputs": [],
      "source": [
        "# plot case-specific cost\n",
        "w = 1\n",
        "x = np.linspace(-10,10,100)\n",
        "plt.plot(x, -np.log(sigmoid(x,w)),label=\"log(sigmoid)\")\n",
        "plt.plot(x, -np.log(1-sigmoid(x,w)),label=\"log(1-sigmoid)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29cc5f05",
      "metadata": {
        "id": "29cc5f05"
      },
      "source": [
        "**Logistic loss**\n",
        "\n",
        "Use to measure the goodness of fit of a logistic regression model.\n",
        "\n",
        "$$\n",
        "L=-y\\cdot \\log(h(x,w))-(1 - y)\\cdot \\log(1-h(x,w))\n",
        "$$\n",
        "\n",
        "\n",
        "**Minimization of the Logistic loss**\n",
        "\n",
        "The gradient is given by:\n",
        "\n",
        "$$\n",
        "w_{j}\\leftarrow w_{j}-\\frac{\\gamma}{n}\\sum_{i=1}^{n}(y^{i}-h(x^{i}))x^{i}_{j}\n",
        "$$\n",
        "In compact form:\n",
        "\n",
        "$$\n",
        "w\\leftarrow w -\\frac{\\gamma}{n}X^{T}(h(X)-y)\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f85626d5",
      "metadata": {
        "id": "f85626d5"
      },
      "outputs": [],
      "source": [
        "def sigmoidM(X, w):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of dim n x d \n",
        "        Matrix containing the dataset\n",
        "    w : array of dim d\n",
        "        Vector representing the coefficients of the logistic model\n",
        "    \"\"\"\n",
        "    y = 1/(1+np.exp(-np.matmul(X,w)))\n",
        "    return y\n",
        "\n",
        "\n",
        "def LogisticLoss(X, labels, w):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of dim n x d\n",
        "        Matrix containing the dataset\n",
        "    y : array of dim n\n",
        "        Vector representing the ground truth label of each data point\n",
        "    w : array of dim d\n",
        "        Vector representing the coefficients of the logistic model\n",
        "    \"\"\"\n",
        "    n = np.shape(X)[0]\n",
        "    cost = -(1/n)*np.sum(labels*np.log(sigmoidM(X,w)) + (1-labels)*np.log(1-sigmoidM(X,w)))\n",
        "    return cost\n",
        "\n",
        "def GDLogistic(X, labels, iter, gamma):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of dim n x d\n",
        "        Matrix containing the dataset\n",
        "    y : array of dim n\n",
        "        Vector representing the ground truth label of each data point\n",
        "    iter : int\n",
        "        Number of GD iterations\n",
        "    gamma : float\n",
        "        Learning rate\n",
        "    \"\"\"\n",
        "    d = np.shape(X)\n",
        "    cost = np.zeros(iter)\n",
        "    w = np.random.uniform(0, 0.01, d[1])\n",
        "    W = np.zeros((2,iter))\n",
        "    for i in range(iter):\n",
        "        W[:,i] = w\n",
        "        w = w-(2*gamma/d[0])*(np.transpose(X)@(sigmoidM(X,w)-labels))\n",
        "        cost[i] = LogisticLoss(X,labels,w)\n",
        "    return W,cost "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d80d30b",
      "metadata": {
        "scrolled": true,
        "id": "9d80d30b"
      },
      "outputs": [],
      "source": [
        "# usage example\n",
        "iter = 500\n",
        "gamma = 0.01\n",
        "W, cost = GDLogistic(X, noisylabels, iter, gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e89e405",
      "metadata": {
        "id": "3e89e405"
      },
      "outputs": [],
      "source": [
        "fig,ax =plt.subplots()\n",
        "ax.plot(cost, 'go')\n",
        "ax.set_title('Loss')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57317e4c",
      "metadata": {
        "id": "57317e4c"
      },
      "source": [
        "***Plotting the decision boundary***\n",
        "\n",
        "The decision boundary is used to predict which class a new point might correspond to.\n",
        "\n",
        "Its derivation is:\n",
        "\n",
        "$$0=w^{T}x=x_{1}w_{1}+x_{2}w_{2}\\rightarrow x_{2}=-\\frac{x_{1}w_{1}}{w_{2}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70c3a9c0",
      "metadata": {
        "scrolled": false,
        "id": "70c3a9c0"
      },
      "outputs": [],
      "source": [
        "w = W[:,iter-1]\n",
        "y = -(np.dot(X[:,0], w[0])/w[1])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:,0], X[:,1], s=70, c=noisylabels, alpha=0.8)\n",
        "ax.plot(X[:,0], y)\n",
        "plt.ylim([m, M])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0421f916",
      "metadata": {
        "id": "0421f916"
      },
      "source": [
        "## Accuracy\n",
        "\n",
        "The accuracy is a metric used for classification problems defined as the number of correct predictions divided by the total number of predictions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f3d230",
      "metadata": {
        "scrolled": true,
        "id": "78f3d230"
      },
      "outputs": [],
      "source": [
        "def accuracy(X, w, labels):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of dim n x d\n",
        "        Matrix containing the dataset\n",
        "    w : array of dim d\n",
        "        Vector representing the coefficients of the logistic model\n",
        "    labels : array of dim n\n",
        "        Vector representing the ground truth label of each data point\n",
        "    \"\"\"\n",
        "    a = labels-np.round(sigmoidM(X,w))\n",
        "    acc = np.sum(a==0)/np.shape(X)[0]\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(X, w, noisylabels)"
      ],
      "metadata": {
        "id": "9YrVem6FnoiL"
      },
      "id": "9YrVem6FnoiL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f826f706",
      "metadata": {
        "id": "f826f706"
      },
      "source": [
        "### Generate binary data with a separator\n",
        "\n",
        "3. Repeat the experiment generating data with a linear separator (take the generating function from Lab 1)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E8Wi4mjzn-Hi"
      },
      "id": "E8Wi4mjzn-Hi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explore the `scikit-learn` functions for linear and logistic regression."
      ],
      "metadata": {
        "id": "fTIPdc7An-Us"
      },
      "id": "fTIPdc7An-Us"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ENdYPoIioFx-"
      },
      "id": "ENdYPoIioFx-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}