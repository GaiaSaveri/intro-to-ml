{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaiaSaveri/intro-to-ml/blob/main/notebooks/Lab-10.KernelPCA_KernelSVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58a29d00",
      "metadata": {
        "id": "58a29d00"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import linalg as LA\n",
        "\n",
        "from math import sin\n",
        "from math import cos\n",
        "\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from scipy import exp\n",
        "from scipy.linalg import eigh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37afbc06",
      "metadata": {
        "id": "37afbc06"
      },
      "source": [
        "Kernel regression\n",
        "===============\n",
        "\n",
        "https://knork.medium.com/linear-regression-in-python-from-scratch-with-kernels-e9c37f7975b9\n",
        "\n",
        "**Recap**: the solution of the linear regression $y=wX$ can be also written as:\n",
        "\n",
        "$$\n",
        "w=(X^{T}X)^{-1}X^{T}y=X^{T}(XX^{T})^{-1}y\n",
        "$$\n",
        "\n",
        "\n",
        "Let $X\\in R^{N\\times d}$, then $X^{T}X\\in R^{d\\times d}$ and $K=XX^{T}\\in R^{N\\times N}$: it all depends on the ratio $\\frac{d}{N}$ which matrix is more convenient (i.e. less computational demanding) to compute. \n",
        "\n",
        "For predictions, we plug the above for a test point $z$ and get:\n",
        "\n",
        "$$\n",
        "f(z)=z^{T}w=z^{T} X^{T}(XX^{T})^{-1}y= \\alpha^{T}(z)K^{-1}y\n",
        "$$\n",
        "\n",
        "with $\\alpha(z)=z^{T}X^{T}=K(z,X)\\in R^{1\\times N}$.\n",
        "\n",
        "This is the case for linear regression. How can we extend to the non-linear case?  $x\\rightarrow \\phi(x)$, and the reasoning above repeats.\n",
        "\n",
        "\n",
        "Also: https://github.com/luigicarratino/Tutorial_Kernels_MLSS2019_London/blob/master/Tutorial%20Kernel.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap on different kernel functions \n",
        "\n",
        "See also: https://www.cs.toronto.edu/~duvenaud/cookbook/\n",
        "\n",
        "1. **Linear Kernel**: $k(x, y) = <x, y> = x\\cdot y$ fast to compute, hence convenient when there are a lot of features and no need to map points in a higher dimensional space. Usually combined with other kernels to increase their expressiveness. \n",
        "\n",
        "2. **Polynomial Kernel**: $k(x, y)= (1 + x\\cdot y)^p$ ($p$ is the degree) non-linear function if $p>1$, it allows to cnsider interactions among features up to order $p$.\n",
        "\n",
        "3. **Radial Basis Funtion / Gaussian Kernel**: $k(x, y) = \\bigg( -\\frac{||x - y||_2}{2 \\sigma^2} \\bigg)$ it is arguably the most used, as it can map points potentially to infinite dimenional space (think about Taylor expansion) hence it is potentially able to fit any dataset. "
      ],
      "metadata": {
        "id": "Ziaaw8uz4pjf"
      },
      "id": "Ziaaw8uz4pjf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a3bfd85",
      "metadata": {
        "id": "7a3bfd85"
      },
      "outputs": [],
      "source": [
        "def kernelFunc(x1, x2, kernel_type, param):\n",
        "    if kernel_type == 'Polynomial':\n",
        "        return pow((1 + np.dot(x1, x2)), param)\n",
        "    elif kernel_type == 'Gaussian':\n",
        "        return np.exp(- param * np.dot(x1-x2,x1-x2))\n",
        "    elif kernel_type == 'Linear':\n",
        "        return np.dot(x1,x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b85c7613",
      "metadata": {
        "id": "b85c7613"
      },
      "source": [
        "RBF Kenel PCA\n",
        "============="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27b5929",
      "metadata": {
        "id": "e27b5929"
      },
      "source": [
        "***PCA recap***\n",
        "\n",
        "Normalization\n",
        "$$\n",
        "x\\rightarrow x-\\frac{1}{d}\\sum_{j}(x)_{j}\\\n",
        "$$\n",
        "\n",
        "Eigenvectors of the covariance matrix are the new coordinates (thanks to normalization):\n",
        "$$\n",
        "Cv=\\lambda v\\;\\;\\;\\;\\;\\;\\;C=\\frac{1}{N}\\sum_{i}x_{i}x^{T}_{i}\n",
        "$$\n",
        "\n",
        "The first $d$ eigenvalues provide information about the amount of information (i.e. variance) retained if we use only the first $d$ eigendirection."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1P6Ev5bzJuB_RGQaYtaHwo9_toauMuAO7\" alt= “” width=\"300\" height=\"300\">\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B7tIdAVoteaX"
      },
      "id": "B7tIdAVoteaX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "However it may not be applicable to data at hand:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1n9esj5-Inwv1lt3gOkDfligsH0y-1P_C\" alt= “” width=\"300\" height=\"600\">\n",
        "\n"
      ],
      "metadata": {
        "id": "HdFDQAz2uUAD"
      },
      "id": "HdFDQAz2uUAD"
    },
    {
      "cell_type": "markdown",
      "id": "3b1ec05c",
      "metadata": {
        "id": "3b1ec05c"
      },
      "source": [
        "***Kernel PCA***\n",
        "\n",
        "**Intuition**: use a kernel function to map features in a different (often higher dimensional) feature space, then perform dimensionality reduction in this new space using PCA.\n",
        "\n",
        "New Features\n",
        "$$\n",
        "x\\rightarrow \\phi(x)\n",
        "$$\n",
        "\n",
        "Covariance eigenvalues and eigenvectors:\n",
        "\n",
        "$$\n",
        "C_{K}v=\\lambda v\\;\\;\\;\\;\\;\\;\\;\\;C_{F}=\\frac{1}{N}\\sum_{i}\\phi(x_{i})\\phi^{T}(x_{i})\n",
        "$$\n",
        "\n",
        "One can prove that eigenvectors are linear combinations of features: \n",
        "\n",
        "$$\n",
        "v=\\sum_{i}\\alpha_{i}\\phi(x_{i})\\;\\;\\; \\alpha_i = \\frac{\\phi(x_i)^T v}{N\\lambda}\n",
        "$$\n",
        "\n",
        "Multiplying both sides of $C_{K}v=\\lambda v$ by $\\phi(x_{k})$ we have the following eigendecomposition:\n",
        "\n",
        "$$\n",
        "N\\lambda \\alpha = K \\alpha\n",
        "$$\n",
        "\n",
        "Normalization\n",
        "$$\n",
        "\\phi(x)\\rightarrow \\phi(x)-\\frac{1}{d}\\sum_{i}(\\phi(x))_{i} \n",
        "$$\n",
        "\n",
        "The reuslting kernel is\n",
        "\n",
        "$$\n",
        "K\\rightarrow K-2Id_{1/n}K + Id_{1/n}K Id_{1/n}\n",
        "$$\n",
        "\n",
        "with $Id_{1/n}$ the matrix with entries $1/n$.\n",
        "\n",
        "\n",
        "See also  https://sdat.ir/en/sdat-blog/python-kernel-tricks-and-nonlinear-dimensionality-reduction-via-rbf-kernel-pca\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing PCA in a higher dimension might be more suitable:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1OhPCVwMJI3xzvsJCq5oMbOk4dazE_I_y\" alt= “” width=\"300\" height=\"600\">"
      ],
      "metadata": {
        "id": "jOhCl8f02zKU"
      },
      "id": "jOhCl8f02zKU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, given a $M\\times M$ kernel matrix $K$ for a dataset of $M$ points, to get its principal components we should compute the following:\n",
        "\n",
        "1. Center the kernel matrix by computing $K\\rightarrow K-2Id_{1/n}K + Id_{1/n}K Id_{1/n}$\n",
        "3. Find ordered eigenvalues and eigenvectors of $K$ "
      ],
      "metadata": {
        "id": "GJmUWtjUx26V"
      },
      "id": "GJmUWtjUx26V"
    },
    {
      "cell_type": "code",
      "source": [
        "# very famous example of non-linearly-separable dataset\n",
        "from sklearn.datasets import make_moons\n",
        "X, y = make_moons(n_samples=100, random_state=123)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "\n",
        "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\n",
        "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\n",
        "\n",
        "plt.title('A nonlinear 2Ddataset')\n",
        "plt.ylabel('y coordinate')\n",
        "plt.xlabel('x coordinate')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "26k8Y4374RGy"
      },
      "id": "26k8Y4374RGy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10da778f",
      "metadata": {
        "id": "10da778f"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "scikit_pca = PCA(n_components=2)\n",
        "X_spca = scikit_pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_spca[y==0, 0], X_spca[y==0, 1], color='red', alpha=0.5)\n",
        "plt.scatter(X_spca[y==1, 0], X_spca[y==1, 1], color='blue', alpha=0.5)\n",
        "\n",
        "plt.title('First 2 principal components after Linear PCA')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85b1b27b",
      "metadata": {
        "id": "85b1b27b"
      },
      "outputs": [],
      "source": [
        "scikit_pca = PCA(n_components=1)\n",
        "X_spca = scikit_pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_spca[y==0, 0], np.zeros((50,1)), color='red', alpha=0.5)\n",
        "plt.scatter(X_spca[y==1, 0], np.zeros((50,1)), color='blue', alpha=0.5)\n",
        "\n",
        "plt.title('First principal component after Linear PCA')\n",
        "plt.xlabel('PC1')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_type = 'Gaussian'\n",
        "param = 15\n",
        "\n",
        "# implement KPCA with our defined RBF kernel and fir to moon dataset"
      ],
      "metadata": {
        "id": "PAg4YLhM80tA"
      },
      "id": "PAg4YLhM80tA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(rbf_kpca[y==0, 0], rbf_kpca[y==0, 1], color='red', alpha=0.5)\n",
        "plt.scatter(rbf_kpca[y==1, 0], rbf_kpca[y==1, 1], color='blue', alpha=0.5)\n",
        "\n",
        "plt.title('First 2 principal components after RBF Kernel PCA')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "foZ5hhRgAisu"
      },
      "id": "foZ5hhRgAisu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "330d7bc9",
      "metadata": {
        "id": "330d7bc9"
      },
      "source": [
        "## Kernel SVM \n",
        "\n",
        "**Recall**: Suppport Vector Machines (SVM) separates data by finding an hyperplane which maximizes the margin between classes.\n",
        "\n",
        "Kernel SVM use kernels to map data in a higher dimensional feature space, where the maximal margin hyperplane is learned. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa66359",
      "metadata": {
        "id": "efa66359"
      },
      "outputs": [],
      "source": [
        "# from svm lab\n",
        "def hingeFunction(z):\n",
        "    return np.maximum(1-z,0)\n",
        "\n",
        "\n",
        "def hingesubgrad(z):\n",
        "    g = np.zeros(z.shape)\n",
        "    g[z < 1] = -1\n",
        "    return g\n",
        "\n",
        "def ReghingeLoss(X,labels,w, lam):\n",
        "    d = np.shape(X)[0]\n",
        "    L = np.mean(np.maximum(np.ones(d) - labels*(X@w), 0)) + lam * LA.norm(w,2) \n",
        "    return L\n",
        "\n",
        "def svmGrad(X, labels, w, lam):\n",
        "    g1 = hingesubgrad(np.diag(labels)@(X@w))\n",
        "    g2 = np.diag(labels)@X\n",
        "    return g1.dot(g2) + 2*lam*w\n",
        "\n",
        "def svm_training(X, labels, lam, Iter, gamma):\n",
        "    # get dimensions\n",
        "    d = np.shape(X)[1]\n",
        "    # initialise weight vector and set first column to a random vector\n",
        "    W = np.zeros((d,Iter))\n",
        "    W[:,0] = np.random.normal(0,0.01,d)\n",
        "    # initialise loss vector\n",
        "    L = np.zeros(Iter)\n",
        "\n",
        "    for i in range(Iter-1):\n",
        "        # update weights using subgradient descent algorithm\n",
        "        W[:,i+1] = W[:,i] - gamma * svmGrad(X, labels, W[:,i], lam)\n",
        "\n",
        "        # save loss\n",
        "        L[i] = ReghingeLoss(X,labels,W[:,i],lam)\n",
        "\n",
        "    return W, L"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get SVM's predictions\n",
        "def predlabels(X, w): \n",
        "    y = np.sign(X@w)\n",
        "    return y\n",
        "\n",
        "# calculate accuracy\n",
        "def acc(labels, predlabels):\n",
        "     return np.count_nonzero(labels == predlabels)/len(labels)   "
      ],
      "metadata": {
        "id": "M8x6VkCCGZ-D"
      },
      "id": "M8x6VkCCGZ-D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "kernel_type = 'Gaussian'\n",
        "param = 15\n",
        "\n",
        "# implement and test kernel SVM with our defined kernel and SVM \n"
      ],
      "metadata": {
        "id": "3u8z686fGdjS"
      },
      "id": "3u8z686fGdjS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "\n",
        "plt.scatter(X_test[y_pred==0, 0], X_test[y_pred==0, 1], color='red', alpha=0.5)\n",
        "plt.scatter(X_test[y_pred==1, 0], X_test[y_pred==1, 1], color='blue', alpha=0.5)\n",
        "\n",
        "plt.title('Kernel SVM Binary Classification')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "07PiwqdmIu1f"
      },
      "id": "07PiwqdmIu1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sklearn Kernel SVM Function**"
      ],
      "metadata": {
        "id": "dk-Xy-stI97m"
      },
      "id": "dk-Xy-stI97m"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC(kernel='rbf', gamma=15)\n",
        "svc.fit(X_train, y_train)\n",
        "y_pred_sk = svc.predict(X_test)\n",
        "print('Accuracy with sklearn: ', acc(y_test, y_pred_sk))"
      ],
      "metadata": {
        "id": "PZldiWumI7lT"
      },
      "id": "PZldiWumI7lT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "\n",
        "plt.scatter(X_test[y_pred_sk==0, 0], X_test[y_pred_sk==0, 1], color='red', alpha=0.5)\n",
        "plt.scatter(X_test[y_pred_sk==1, 0], X_test[y_pred_sk==1, 1], color='blue', alpha=0.5)\n",
        "\n",
        "plt.title('Kernel SVM Binary Classification (sklearn)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-VzmescJJt4m"
      },
      "id": "-VzmescJJt4m",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}