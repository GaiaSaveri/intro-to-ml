{"cells":[{"cell_type":"markdown","id":"ca165409","metadata":{"id":"ca165409"},"source":["# Autoencoders\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/GaiaSaveri/intro-to-ml/blob/main/notebooks/Lab-13.Autoencoders.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","An autoencoder is a type of neural network architecture that finds the function mapping the features $x$ to itself. This objective is known as reconstruction, and is accomplished through an encoder and a decoder:\n","* the encoder compresses the input data into a lower-dimensional representation that captures the most important features or patterns;\n","* the decoder reconstructs the original data from the compressed representation.\n","\n","The compressed representation learned by an autoencoder can be visualized as a lower-dimensional space, where similar inputs are mapped to nearby points and dissimilar inputs are mapped to distant points. This makes autoencoders useful for tasks such as dimensionality reduction, anomaly detection, and generative modeling.\n","\n","**NOTE:** *If linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to principal component analysis (PCA). However, the potential of autoencoders resides in their non-linearity, allowing the model to learn more powerful generalizations compared to PCA, and to reconstruct the input with significantly lower information loss.*"]},{"cell_type":"code","execution_count":null,"id":"69f0cee2","metadata":{"id":"69f0cee2"},"outputs":[],"source":["# import all needed packages \n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torchvision\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from time import time\n","import random\n","\n","# reproducibility\n","torch.manual_seed(1)\n","np.random.seed(2)\n","random.seed(3)"]},{"cell_type":"code","execution_count":null,"id":"5616f9bd","metadata":{"id":"5616f9bd"},"outputs":[],"source":["# Set arguments\n","batch_size = 128\n","\n","# DataLoader Train\n","trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True,\n","    transform=torchvision.transforms.ToTensor())\n","\n","trainloader = data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","\n","\n","# DataLoader Test\n","testset = torchvision.datasets.MNIST(root='./data', train=False, download=True,\n","    transform=torchvision.transforms.ToTensor())\n","\n","testloader = data.DataLoader(testset, batch_size=16, shuffle=False)"]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"-db4fr6Jb4Fl"},"id":"-db4fr6Jb4Fl"},{"cell_type":"markdown","source":["##### **1. Define the model architecture of an AutoEncoder** \n"," As mentioned, it is divided in two parts: encoder and decoder.  \n","  The encoder takes as input a flattened image of size $28x28$ and applies a series of fully connected layers with Tanh activation function. The output of its last layer has dimension $2$.   \n","  The decoder takes the 2-dimensional encoded representation and applies a series of fully connected layers with Tanh activation function in the reverse order. The last layer applies a sigmoid activation function to map the reconstructed output to the range $[0,1]$."],"metadata":{"id":"q2kjJyRcV78X"},"id":"q2kjJyRcV78X"},{"cell_type":"code","execution_count":null,"id":"f3ad5d38","metadata":{"id":"f3ad5d38"},"outputs":[],"source":["# YOUR CODE"]},{"cell_type":"code","source":["def train_model(model, num_epochs, trainloader, criterion, optimizer):\n","\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    model = model.to(device)\n","\n","    losses = []\n","   \n","    for epoch in range(num_epochs):\n","        train_running_loss = 0.0\n","\n","        # Set the model to training mode\n","        model = model.train()\n","        start = time()\n","        ## training step\n","        for i, (images, labels) in enumerate(trainloader):\n","            \n","            # difference here! Similar to .flatten()\n","            images = images.view(-1, 28*28)\n","\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            ## forward + backprop + loss\n","            codes, decoded = model(images)\n","            loss = criterion(decoded, images)\n","\n","            # Reset the gradients to zero\n","            optimizer.zero_grad()\n","\n","            loss.backward()\n","\n","            ## update model params\n","            optimizer.step()\n","\n","            train_running_loss += loss.item()\n","        \n","            \n","        losses.append(train_running_loss / i)\n","        \n","        model.eval()\n","        print(f\"Epoch: {epoch+1} | Loss: {train_running_loss / i:.4f} | Time: {time()-start:.2f}\") \n","    \n","    return losses"],"metadata":{"id":"WOXCJJv5Wrpq"},"id":"WOXCJJv5Wrpq","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For image data with AutoEncoders it is common to use the mean squared error (MSE) loss function, as it measures the difference between the input image pixels and the reconstructed ones. We are not concerned with labels in this case."],"metadata":{"id":"MQxW-w5Wg0Hy"},"id":"MQxW-w5Wg0Hy"},{"cell_type":"code","execution_count":null,"id":"bf133a0d","metadata":{"id":"bf133a0d"},"outputs":[],"source":["# Optimizer and loss function\n","model = AutoEncoder()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.MSELoss()"]},{"cell_type":"code","source":["L = train_model(model, 15, trainloader, criterion, optimizer)"],"metadata":{"id":"CZUM-PuAW1a8"},"id":"CZUM-PuAW1a8","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test effects\n","Now that we have trained the AutoEncoder model, letâ€™s take a look at the picture we restored from the compression. Does it look like the original picture?"],"metadata":{"id":"wdvVfxqTcAe_"},"id":"wdvVfxqTcAe_"},{"cell_type":"code","source":["# Show images function\n","def show_images(images):\n","    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n","\n","    for index, image in enumerate(images):\n","        plt.subplot(sqrtn, sqrtn, index+1)\n","        plt.imshow(image.reshape(28, 28), cmap='gray')\n","        plt.axis('off')"],"metadata":{"id":"y9qGilzXcCn6"},"id":"y9qGilzXcCn6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Test\n","with torch.no_grad():\n","    for images, _ in testloader:\n","        print(\"Originals\")\n","        inputs = images.view(-1, 28*28)\n","        show_images(inputs)\n","        plt.show()\n","\n","        print(\"Produced by AutoEncoder\")\n","        inputs = inputs.to(device)\n","        code, outputs = model(inputs)\n","        show_images(outputs.cpu())\n","        plt.show()\n","        break"],"metadata":{"id":"5_DIduzCcjxU"},"id":"5_DIduzCcjxU","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualise compressed data\n","Earlier, we set the dimension of the output of the encoder to $2$. These two values can be thought of as the $x$ and $y$ coordinates of a point in a two-dimensional space that represents a compressed version of the input image.  \n","Let's generate a scatter plot of these encoded representations of the images in the test set.\n"],"metadata":{"id":"r6k8yp1reMgd"},"id":"r6k8yp1reMgd"},{"cell_type":"code","execution_count":null,"id":"7a9421ca","metadata":{"id":"7a9421ca"},"outputs":[],"source":["axis_x = []\n","axis_y = []\n","answers = []\n","\n","# turn off gradient calculation to speed up the process\n","with torch.no_grad():\n","    for images, labels in testloader:\n","        inputs = images.view(-1, 28*28)\n","        answers += labels.tolist()\n","\n","        inputs = inputs.to(device)\n","        code, outputs = model(inputs)\n","        code = code.cpu()\n","        axis_x += code[:, 0].tolist()\n","        axis_y += code[:, 1].tolist()\n","\n","\n","plt.scatter(axis_x, axis_y, c=answers)\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"markdown","source":["The data compressed by the Autoencoder has been able to grasp the existence of different features in each number."],"metadata":{"id":"L4DTYx7RfMlf"},"id":"L4DTYx7RfMlf"},{"cell_type":"markdown","source":["## One step further: different types of Autoencoders\n","\n","There are several types of autoencoders, each designed for a specific purpose. Here are some of the most common types:\n","\n","* **Convolutional autoencoder**: This type of autoencoder is specifically designed for image data. It uses convolutional layers in the encoder and decoder to take advantage of the spatial structure in the data.\n","* **Recurrent autoencoder**: This type of autoencoder is designed for sequential data, such as time series or text. It uses recurrent neural networks (RNNs) in the encoder and decoder to capture the temporal dependencies in the data.\n","* **Denoising autoencoder**: This type of autoencoder is designed to remove noise from input data. During training, the model is fed noisy input data and is trained to reconstruct the original, noise-free input.\n","* **Variational autoencoder** (VAE): This type of autoencoder is designed to generate new data samples that are similar to the original input data. It uses a probabilistic approach to generate data by sampling from a learned distribution."],"metadata":{"id":"n-hb-Mi-zOhD"},"id":"n-hb-Mi-zOhD"},{"cell_type":"markdown","source":["##### **2. Create a convolutional autoencoder and use it on MNIST. See how the reconstructed images change and how the encoded representations are scattered in a plane.**\n","\n","Its decoder uses transposed convolutional layers to increase the width and height of the input layers. They work almost exactly the same as convolutional layers, but in reverse. A stride in the input layer results in a larger stride in the transposed convolution layer. For example, if you have a $3x3$ kernel, a $3x3$ patch in the input layer will be reduced to one unit in a convolutional layer. Comparatively, one unit in the input layer will be expanded to a $3x3$ path in a transposed convolution layer. PyTorch provides us with an easy way to create these layers, [`nn.ConvTranspose2d`](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)."],"metadata":{"id":"jlBtvGV8zuvV"},"id":"jlBtvGV8zuvV"},{"cell_type":"code","source":["# YOUR CODE"],"metadata":{"id":"sdufR21m_69r"},"id":"sdufR21m_69r","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}