{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaiaSaveri/intro-to-ml/blob/main/notebooks/Lab-4.RegressionAndRegularizations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Regularized Linear Regression and $K$-Fold Cross Validation"
      ],
      "metadata": {
        "id": "AHu99jMepOwx"
      },
      "id": "AHu99jMepOwx"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7970569b",
      "metadata": {
        "id": "7970569b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy.linalg as LA\n",
        "from numpy.linalg import inv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5b2873b",
      "metadata": {
        "id": "d5b2873b"
      },
      "source": [
        "Tasks for this lab:\n",
        "\n",
        "* Generate linearly separable data with different amount of noise;\n",
        "\n",
        "* Calculate the Least Square Regression (LSR) solution without or with regularization (Ridge, Lasso and Elastic Net);\n",
        "\n",
        "* Test which of the three methods achieve bettter test error."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cebeee6",
      "metadata": {
        "id": "2cebeee6"
      },
      "source": [
        "## Data Generation for Regression (cfr Lab 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bb91a4f0",
      "metadata": {
        "id": "bb91a4f0"
      },
      "outputs": [],
      "source": [
        "def datagen(d, points, m, M, w, sigma):\n",
        "    X = np.zeros((points,d))\n",
        "    for i in range(points):\n",
        "        X[i,:] = np.random.uniform(m, M, d)\n",
        "    eps = np.random.normal(0, sigma, points)\n",
        "    y = np.dot(X,w) + eps \n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f6023feb",
      "metadata": {
        "id": "f6023feb"
      },
      "outputs": [],
      "source": [
        "# generate multi-dimensional regression dataset\n",
        "d = 100\n",
        "w = np.random.normal(0, 1, d)\n",
        "sigma = 0.1\n",
        "points = 1000\n",
        "m = -10\n",
        "M = 10\n",
        "\n",
        "X, y = datagen(d, points, m, M, w, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent for (Regularized) Linear Regression \n",
        "\n",
        "Squared error loss and its gradient for **non-regularized linear regression** (cfr Lab 3):\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathcal{L}=\\frac{1}{n}\\|y-Xw\\|_{2}^{2},\\;\\;\\;\\;\\nabla_{w} \\mathcal{L} = -\\frac{2}{n}X(y-Xw)\n",
        "$$"
      ],
      "metadata": {
        "id": "T_DDs-6nqtTL"
      },
      "id": "T_DDs-6nqtTL"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ce46fb6f",
      "metadata": {
        "id": "ce46fb6f"
      },
      "outputs": [],
      "source": [
        "def SquareLoss(X, y, w):\n",
        "    return LA.norm(y-X@w,2)\n",
        "\n",
        "\n",
        "def OLSGradient(X, y, w, points):\n",
        "    return (-2/points)*((y-X@w)@X)\n",
        "\n",
        "\n",
        "def GD(X, y, iter, gamma, points, d):\n",
        "    W = np.zeros((d,iter))\n",
        "    L = np.zeros(iter)\n",
        "    w = np.random.normal(0, 0.1, d)\n",
        "    for i in range(iter):\n",
        "        W[:,i] = w\n",
        "        w = w - gamma*OLSGradient(X, y, w, points)\n",
        "        L[i] = SquareLoss(X,y,w)\n",
        "    return W, L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d4832d",
      "metadata": {
        "id": "54d4832d"
      },
      "outputs": [],
      "source": [
        "d = np.shape(X)[1]\n",
        "iter = 1000\n",
        "points = 100\n",
        "gamma = 0.001\n",
        "\n",
        "wgd, L = GD(X, y, iter, gamma, points, d)\n",
        "\n",
        "wpred = wgd[:,-1]\n",
        "\n",
        "print('L2 Norm of the Difference Between Ground Truth Weigths and Predicted Weights: ', LA.norm(w-wpred,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa2126dd",
      "metadata": {
        "id": "fa2126dd"
      },
      "source": [
        "Squared error loss and its gradient for **Ridge regression**:\n",
        "\n",
        "$$\n",
        "L=\\|y-Xw\\|_{2}^{2}+\\lambda\\|w\\|_{2}^{2},\\;\\;\\;\\;\\nabla_{w} L = -X(y-Xw)+2\\lambda w\n",
        "$$\n",
        "\n",
        "Ridge Regression performs $L2$ regularization, i.e. it minimizes the penalized sum of squares in such a way that smaller $w$ are preferred, indeed the closer the $w$ are to $0$, the smaller is the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "98b8e844",
      "metadata": {
        "id": "98b8e844"
      },
      "outputs": [],
      "source": [
        "def RidgeSquareLoss(X, y, w, lam):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of float dim n x d\n",
        "        Matrix containing the dataset\n",
        "    y : array of float of dim n\n",
        "        Vector containing the ground truth value of each data point\n",
        "    w : array of float of dim d\n",
        "        Weights of the fitted line\n",
        "    lam : float\n",
        "        Weight of the L2 penalty term\n",
        "    \"\"\"\n",
        "    return LA.norm(y-X@w,2) + lam*LA.norm(w,2)\n",
        "\n",
        "\n",
        "def RidgeGradient(w, lam):\n",
        "    return 2*lam*w\n",
        "\n",
        "def GDRidge(X, y, iter, gamma, points, d, lam):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of float dim n x d\n",
        "        Matrix containing the dataset\n",
        "    y : array of float of dim n\n",
        "        Vector containing the ground truth value of each data point\n",
        "    iter : int\n",
        "        Number of GD iterations\n",
        "    gamma : float\n",
        "        Learning rate\n",
        "    points : int\n",
        "        Number of points in our dataset\n",
        "    d : int\n",
        "        Dimensionality of each data point in the dataset\n",
        "    lam : float\n",
        "        Weight of the L2 penalty term\n",
        "    \"\"\"\n",
        "    W = np.zeros((d,iter))\n",
        "    L = np.zeros(iter)\n",
        "    # w = np.random.normal(0,0.1,d)\n",
        "    w = np.zeros(d)\n",
        "    for i in range(iter):\n",
        "        W[:,i] = w\n",
        "        w = w - gamma * (OLSGradient(X, y, w, points) + RidgeGradient(w, lam))\n",
        "        L[i] = RidgeSquareLoss(X,y,w,lam)\n",
        "    return W, L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d191cff8",
      "metadata": {
        "id": "d191cff8"
      },
      "outputs": [],
      "source": [
        "d = np.shape(X)[1]\n",
        "iter = 100\n",
        "points = 100\n",
        "gamma = 0.0001\n",
        "lam = 0.1\n",
        "\n",
        "wgdR, L = GDRidge(X, y, iter, gamma, points, d, lam)\n",
        "wpredR = wgdR[:,-1]\n",
        "print('L2 Norm of the Difference Between Ground Truth Weigths and Predicted Weights: ', LA.norm(w-wpredR,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68e1323c",
      "metadata": {
        "id": "68e1323c"
      },
      "outputs": [],
      "source": [
        "plt.plot(L)\n",
        "plt.title('Loss Ridge')\n",
        "plt.xlabel('Iter')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5983a6e",
      "metadata": {
        "id": "d5983a6e"
      },
      "source": [
        "Squared error loss and its gradient for **Lasso (Least Absolute Shrinkage and Selection Operator) regularization**:\n",
        "\n",
        "$$\n",
        "L=\\|y-Xw\\|_{2}^{2}+\\lambda\\|w\\|_{1},\\;\\;\\;\\;\\nabla_{w} L = pr[-X(y-Xw)]\n",
        "$$\n",
        "\n",
        "This kind of regularization can lead to zero the coefficients, i.e. some of the features of the points could be completely neglected, so it can be used and regareded also as a feature selection strategy. \n",
        "\n",
        "Since we cannot compute the gradient of the penalization term (as it is not differentiable at $0$), we will use **subgradient descent**, that is an algorithm for minimizing a nondifferentialble convex function.\n",
        "\n",
        "We define the sub-differential of the absolute value function as:\n",
        "\n",
        "$$\n",
        "\\partial \\|w\\|_1 = \\left\\{\\begin{matrix}\n",
        "1 & w>0 \\\\  \n",
        "-1 & w<0 \\\\ \n",
        "[-1, 1] & w=0 \n",
        "\\end{matrix}\\right.\n",
        "$$\n",
        "\n",
        "Hence: \n",
        "\n",
        "$$\n",
        "\\nabla_{w} L = -\\frac{2}{n}\\cdot-X(y-Xw))+\\lambda \\partial \\|w\\|_1\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0ffaf90e",
      "metadata": {
        "id": "0ffaf90e"
      },
      "outputs": [],
      "source": [
        "def LassoSquareLoss(X, y, w, lam):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of float dim n x d\n",
        "        Matrix containing the dataset\n",
        "    y : array of float of dim n\n",
        "        Vector containing the ground truth value of each data point\n",
        "    w : array of float of dim d\n",
        "        Weights of the fitted line\n",
        "    lam : float\n",
        "        Weight of the L1 penalty term\n",
        "    \"\"\"\n",
        "    return LA.norm(y-X@w,2) + lam*LA.norm(w,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ae67c0d3",
      "metadata": {
        "id": "ae67c0d3"
      },
      "outputs": [],
      "source": [
        "def L1_subgradient(z):\n",
        "    g = np.ones(z.shape)\n",
        "    g[z < 0.] = -1.0\n",
        "    return g\n",
        "\n",
        "def LassoGradient(w, lam):\n",
        "    return lam * L1_subgradient(w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e74e3bb5",
      "metadata": {
        "id": "e74e3bb5"
      },
      "outputs": [],
      "source": [
        "def GDLasso(X, y, iter, gamma, points, d, lam):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of float dim n x d\n",
        "        Matrix containing the dataset\n",
        "    y : array of float of dim n\n",
        "        Vector containing the ground truth value of each data point\n",
        "    iter : int\n",
        "        Number of GD iterations\n",
        "    gamma : float\n",
        "        Learning rate\n",
        "    points : int\n",
        "        Number of points in our dataset\n",
        "    d : int\n",
        "        Dimensionality of each data point in the dataset\n",
        "    lam : float\n",
        "        Weight of the L1 penalty term\n",
        "    \"\"\"\n",
        "    W = np.zeros((d,iter))\n",
        "    L = np.zeros(iter)\n",
        "    w = np.random.normal(0, 0.1, d)\n",
        "    for i in range(iter):\n",
        "        W[:,i] = w\n",
        "        w = w - gamma * (OLSGradient(X, y, w, points) + LassoGradient(w, lam))\n",
        "        L[i] = LassoSquareLoss(X, y, w, lam)\n",
        "    return W, L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112b21eb",
      "metadata": {
        "id": "112b21eb"
      },
      "outputs": [],
      "source": [
        "d = np.shape(X)[1]\n",
        "iter = 1000\n",
        "points = 100\n",
        "gamma = 0.001\n",
        "lam = 0.1\n",
        "\n",
        "wgdL, L = GDLasso(X, y, iter, gamma, points, d, lam)\n",
        "wpredL = wgdL[:,-1]\n",
        "\n",
        "print('L2 Norm of the Difference Between Ground Truth Weigths and Predicted Weights: ', LA.norm(w-wpredL,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bc9e1c9",
      "metadata": {
        "id": "5bc9e1c9"
      },
      "outputs": [],
      "source": [
        "plt.plot(L)\n",
        "plt.title('Loss Lasso')\n",
        "plt.xlabel('Iter')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a93eed78",
      "metadata": {
        "id": "a93eed78"
      },
      "source": [
        "## Elastic Net\n",
        "\n",
        "Regularized regression method that combines the $L1$ and $L2$ penalties of the Lasso and Ridge methods, respectively, to get the best of both words.\n",
        "The loss function to minimize is defined as:\n",
        "\n",
        "$$\n",
        "L=\\|y-Xw\\|_{2}^{2}+\\lambda_1\\|w\\|_{1} + \\lambda_2 \\|w\\|_2\n",
        "$$\n",
        "\n",
        "Hence in this problem we have $2$ parameters to tune.\n",
        "\n",
        "We can rewrite the problem as follows, so that we have only $1$ parameter $\\lambda$ to tune (i.e. as a convex combination of Ridge and Lasso penalties):\n",
        "\n",
        "$$\n",
        "L=\\|y-Xw\\|_{2}^{2}+(\\lambda_r) \\cdot \\lambda\\|w\\|_{1} + (1 - \\lambda_r) \\cdot \\lambda \\|w\\|_2\n",
        "$$\n",
        "\n",
        "where $\\lambda_r$, which is provided in input, is the _importance_ given to the $L1$ penalty (the opposite is also possible)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Since the derivative of the sum is the sum of the derivatives, put together all the elements seen so far and implement the GD algorithm for ElasticNet"
      ],
      "metadata": {
        "id": "u34ose8j-xQC"
      },
      "id": "u34ose8j-xQC"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3vYZoi_2-8Y8"
      },
      "id": "3vYZoi_2-8Y8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "078c2d66",
      "metadata": {
        "id": "078c2d66"
      },
      "source": [
        "## $K$-fold Cross validation\n",
        "\n",
        "Resampling algorithm for estimating the _goodness_ of a machine learning algorithm on unseen data. \n",
        "\n",
        "$K$ is the number of non-overlapping sets we will partition our dataset into. The general procedure is as follows:\n",
        "\n",
        "\n",
        "1.   Shuffle the dataset randomly;\n",
        "2.   Split the daatset into $K$ groups of almost equal size;\n",
        "3. For every group $k\\in \\{1,\\dots, K\\}$:\n",
        "    *   Use group $k$ as validation set;\n",
        "    *   Use all the other groups as training set;\n",
        "    * Train and evaluate the model on the above-mentioned sets;\n",
        "4. Summarize the results of the $K$-validation runs (usually taking the mean).\n",
        "\n",
        "**Remarks**: every set of data is used $1$ time as validation set and $k-1$ times as part of the training set.\n",
        "\n",
        "There is a bias-variance trade-off in the choice of the hyperparameter $K$ (usually $K\\in \\{5, 10\\}$ is used): lower $K$ might have more bias, while higher $K$ might have more variance. \n",
        "\n",
        "We will use $K$-fold CV to tune the parameter $\\lambda$ of the Ridge Regression algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "580dae1f",
      "metadata": {
        "id": "580dae1f"
      },
      "outputs": [],
      "source": [
        "d = 1\n",
        "sigma = 10\n",
        "points = 100\n",
        "m = -10\n",
        "M = 10\n",
        "iter = 500\n",
        "gamma = 0.001\n",
        "\n",
        "Xtr, ytr = datagen(d, points, m, M, w, sigma) # training dataset\n",
        "Xts, yts = datagen(d, points, m, M, w, sigma) # test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "99bcc287",
      "metadata": {
        "id": "99bcc287"
      },
      "outputs": [],
      "source": [
        "def KFoldCVRLS(Xtr, ytr, K, lam):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    Xtr : matrix of float of dim n x d\n",
        "        Dataset\n",
        "    Ytr : array of float of dim n\n",
        "        Ground truth values\n",
        "    K : int\n",
        "        Number of folds to split the data into\n",
        "    lam : array of float \n",
        "        Tuning hyperparameter under analysis\n",
        "    \"\"\"\n",
        "    num_hpar = len(lam) # number of possible values for the hyperparameter\n",
        "    points = Xtr.shape[0] # total training points\n",
        "    fold_size = int(np.ceil(points/K)) # size of each K-fold\n",
        "    # array containing the mean and std of the training error for each \n",
        "    # hyperparameter accross all K runs of CV\n",
        "    tr_mean = np.zeros(num_hpar)\n",
        "    val_mean = np.zeros(num_hpar)\n",
        "    # array containing the mean and std of the validation error for each \n",
        "    # hyperparameter accross all K runs of CV\n",
        "    tr_std = np.zeros(num_hpar)\n",
        "    val_std = np.zeros(num_hpar)\n",
        "    # random permutation of training data\n",
        "    rand_idx = np.random.choice(points, size=points, replace=False)\n",
        "    # train and validation loss for each of the K runs for each of the\n",
        "    # possible hyperparameter values\n",
        "    train_loss = np.zeros((num_hpar,K))\n",
        "    val_loss = np.zeros((num_hpar,K))\n",
        "   \n",
        "    # loop across all hyperparameters values\n",
        "    for l_idx, l in enumerate(lam): \n",
        "        first = 0\n",
        "        # K-fold CV runs\n",
        "        for fold in range(K):\n",
        "            # create a mask to distinguish train and validation set for this run\n",
        "            flags = np.zeros(points) \n",
        "            flags[first:first + fold_size] = 1;  \n",
        "            # construct the  training and validation set      \n",
        "            X = Xtr[rand_idx[flags==0]]\n",
        "            y = ytr[rand_idx[flags==0]]\n",
        "            X_val = Xtr[rand_idx[flags==1]]\n",
        "            y_val = ytr[rand_idx[flags==1]] \n",
        "\n",
        "            # compute the training error of the Ridge regression for the given value of lambda\n",
        "            W, L = GDRidge(X, y, iter, gamma, points, d, l)\n",
        "            train_loss[l_idx, fold] = L[-1]\n",
        "            \n",
        "            # compute the validation error of the Ridge regression for the given value of lambda\n",
        "            val_error = SquareLoss(X_val, y_val, W[:, -1])\n",
        "            val_loss[l_idx, fold] = val_error\n",
        "            \n",
        "            # update the first position to select the next fold\n",
        "            first = first + fold_size                \n",
        "\n",
        "        # summarize performance metrics\n",
        "        tr_mean[l_idx] = np.mean(train_loss[l_idx,:])\n",
        "        val_mean[l_idx] = np.mean(val_loss[l_idx,:])\n",
        "        \n",
        "        tr_std[l_idx] = np.std(train_loss[l_idx,:])\n",
        "        val_std[l_idx] = np.std(val_loss[l_idx,:])\n",
        "    \n",
        "    best_par_idx = np.argmin(val_mean)\n",
        "    best_par = lam[best_par_idx]\n",
        "\n",
        "    return best_par, val_mean, val_std, tr_mean, tr_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "570d9405",
      "metadata": {
        "id": "570d9405"
      },
      "outputs": [],
      "source": [
        "K = 10\n",
        "lam = np.linspace(0, 1, 11)\n",
        "print('Lambda is chosen among: ', lam)\n",
        "\n",
        "best_par, val_mean, val_std, tr_mean, tr_std = KFoldCVRLS(Xtr, ytr, K, lam)\n",
        "print('Best lambda: ', best_par)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a3adc4",
      "metadata": {
        "id": "e9a3adc4"
      },
      "outputs": [],
      "source": [
        "plt.errorbar(lam, tr_mean, yerr=tr_std, label= 'Training Loss')\n",
        "plt.errorbar(lam, val_mean, yerr=val_std, label= 'Validation Loss')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show();"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}