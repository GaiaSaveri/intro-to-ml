{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaiaSaveri/intro-to-ml/blob/main/notebooks/Lab-5.UnsupervisedLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7085776",
      "metadata": {
        "id": "a7085776"
      },
      "source": [
        "# Unsupervised Learning on the Dry Bean Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7470076a",
      "metadata": {
        "id": "7470076a"
      },
      "source": [
        "In this lab we will try to obtain valuable information using Unsupervised Learning techniques.\n",
        "\n",
        "The original data has been downloaded from https://archive-beta.ics.uci.edu/dataset/602/dry+bean+dataset (Dry Bean Dataset. (2020). UCI Machine Learning Repository)\n",
        "\n",
        "**Data Set Description**:\n",
        "\n",
        "Images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. \n",
        "A total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb238ed2",
      "metadata": {
        "id": "fb238ed2"
      },
      "source": [
        "## Loading and Pre-treatment of the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8caaf657",
      "metadata": {
        "id": "8caaf657"
      },
      "outputs": [],
      "source": [
        "# Load libraries and modules\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FFILE = './Dry_Bean_Dataset.xlsx'\n",
        "if os.path.isfile(FFILE): \n",
        "    print(\"File already exists\")\n",
        "    if os.access(FFILE, os.R_OK):\n",
        "        print (\"File is readable\")\n",
        "    else:\n",
        "        print (\"File is not readable, removing it and downloading again\")\n",
        "        !rm FFILE\n",
        "        !wget \"https://raw.github.com/alexdepremia/ML_IADA_UTs/main/Lab5/Dry_Bean_Dataset.xlsx\"\n",
        "else:\n",
        "    print(\"Either the file is missing or not readable, download it\")\n",
        "    !wget \"https://raw.github.com/alexdepremia/ML_IADA_UTs/main/Lab5/Dry_Bean_Dataset.xlsx\""
      ],
      "metadata": {
        "id": "AkGFWaHZOi-l"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AkGFWaHZOi-l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56d03c3c",
      "metadata": {
        "id": "56d03c3c"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = pd.read_excel('./Dry_Bean_Dataset.xlsx')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afdd3bf6",
      "metadata": {
        "id": "afdd3bf6"
      },
      "outputs": [],
      "source": [
        "# Transform the data to use it as numpy arrays. \n",
        "X = data.iloc[:,:-1].values\n",
        "label = data.iloc[:,16].values\n",
        "print(X.shape)\n",
        "N = X.shape[0]  # Number of data points\n",
        "nc = X.shape[1]  # Number of features/components\n",
        "print(np.unique(label)) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordinal encoder for the ground truth labels\n",
        "enc = preprocessing.OrdinalEncoder()\n",
        "enc.fit(label.reshape(-1, 1))\n",
        "y = enc.transform(label.reshape(-1, 1))\n",
        "print(y)  # Encoded labels"
      ],
      "metadata": {
        "id": "IEQuXTH-pyRV"
      },
      "id": "IEQuXTH-pyRV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "480d1d88",
      "metadata": {
        "id": "480d1d88"
      },
      "outputs": [],
      "source": [
        "# Rescale the features of the data since the units are different: substract the average and divide by the standard deviation\n",
        "scaler = preprocessing.StandardScaler().fit(X)\n",
        "X_scaled = scaler.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7790c6b",
      "metadata": {
        "id": "c7790c6b"
      },
      "source": [
        "## Principal Component Analysis (PCA)\n",
        "\n",
        "**Objective**: find the set of orthogonal directions along which the variance of the data is the highest.\n",
        "\n",
        "Summary of the method:\n",
        "\n",
        "* Center the data feature matrix $X$;\n",
        "\n",
        "* Compute the covariance matrix $C$ of the data as $C=X^T X$;\n",
        "\n",
        "* Compute eigenvalues and eigenvectors of the covariance matrix $C$ (using the function `eigh` of the `scipy.LA` module), and sort them according to the decreasing order of the eigenvalues. Arrange them as coumn of a matrix $A$.\n",
        "\n",
        "**Recall**: eigenvalues are the variance of the data along the direction of the corresponding eigenvector. \n",
        "\n",
        "* Compute principal components as $X\\cdot A$"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bhJk5iYvfICv"
      },
      "id": "bhJk5iYvfICv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Assessment: choose the number of PCs to keep\n",
        "\n",
        "**Objective**: choose the final dimension $d$ of the transformed data.\n",
        "\n",
        "1. Proportion of variance explained: given eigenvalues $\\lambda_i$ of the covariance matrix and a threshold $t\\in [0, 1]$, choose $d$ s.t. the ratio $\\chi_d = \\frac{\\sum_{i=1}^d \\lambda_i}{\\sum_{i=1}^D \\lambda_i} > t$ \n",
        "\n",
        "2. Check the existence of a gap in the spectrum of the covariance matrix."
      ],
      "metadata": {
        "id": "Ob2sydGDyCca"
      },
      "id": "Ob2sydGDyCca"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZIzV-JjbfIne"
      },
      "id": "ZIzV-JjbfIne",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "eb4d30c5",
      "metadata": {
        "id": "eb4d30c5"
      },
      "outputs": [],
      "source": [
        "# Perform PCA with sklearn \n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "projection = pca.transform(X_scaled)\n",
        "cumul = np.zeros(nc)\n",
        "for i in range(nc):\n",
        "   cumul[i] = np.sum(pca.explained_variance_ratio_[:i+1])\n",
        "lambs = pca.explained_variance_\n",
        "comp = np.arange(nc) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00dcd9e",
      "metadata": {
        "id": "d00dcd9e"
      },
      "outputs": [],
      "source": [
        "f, [ax1 ,ax2] = plt.subplots(1, 2,figsize = (14, 7))\n",
        "ax1.set_title('Spectrum')\n",
        "ax1.scatter(comp, lambs)\n",
        "ax1.set_xticks(comp)\n",
        "ax2.set_title('Explained variance')\n",
        "ax2.scatter(comp, cumul)\n",
        "ax2.set_xticks(comp)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "551c4b17",
      "metadata": {
        "id": "551c4b17"
      },
      "outputs": [],
      "source": [
        "# Number of components depending on the explained variance threshold.\n",
        "for t in [0.8, 0.85, 0.9, 0.95, 0.97, 0.99, 0.999]:\n",
        "    nc = np.argmax(cumul - t > 0.) + 1\n",
        "    print(t, nc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02876b8e",
      "metadata": {
        "id": "02876b8e"
      },
      "outputs": [],
      "source": [
        "# plotting the data set in 2D (i.e. keep only 2 PCs) colored by its ground truth label\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "ax.scatter(projection[:,0],projection[:,1], c=y)\n",
        "ax.set_title('PCA')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed9d71c7",
      "metadata": {
        "id": "ed9d71c7"
      },
      "outputs": [],
      "source": [
        "# Now in 3D (i.e. keep only 3 PCs)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(projection[:,0],projection[:,1], projection[:,2],c=y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cc6cab0",
      "metadata": {
        "id": "5cc6cab0"
      },
      "source": [
        "1. What would happen if we don't rescale the features?\n",
        "\n",
        "2. Which is the Intrinsic Dimension (ID) of the data set?\n",
        "\n",
        "  **Recall**: the ID of a dataset is the minimum number of dimensions we need to describe the data in a accurate way.\n",
        "\n",
        "3. Could you compute the two-NN estimate of the ID?\n",
        "\n",
        "  The procedure works as follows:\n",
        "\n",
        "  1. Compute pairwise distances among points;\n",
        "  2. For each point $i$, extract the distance from its two closest neighbors $r_{i1}, r_{i2}$ respectively;\n",
        "  3. Compute the ratio $\\mu_i = \\frac{r_{i2}}{r_i1}$;\n",
        "  4. Compute the empirical cumulative distribution $\\mathcal{F}(\\mu)$ of $\\mu$;\n",
        "  5. Find the best fitting line for the dataset $\\{\\log \\mu_i, \\log (1 - \\mathcal{F}(\\mu_i)\\}_{i=1}^N$\n",
        "  6. The intrinsic dimension is given by the slope of this fitted line."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c81ebe4",
      "metadata": {
        "id": "8c81ebe4"
      },
      "source": [
        "## K-means\n",
        "\n",
        "Flat clustering algorithm whose goal is to minimize the intracluster distance while maximizing the intercluster distance. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "539441d4",
      "metadata": {
        "id": "539441d4"
      },
      "source": [
        "We will compute the k-means clustering using two types of initialization:\n",
        "    \n",
        "  1. Random initialization: cluster centroids are initialized picking random points from the dataset;\n",
        "  \n",
        "  2. k-means++: choose first cluster center at random, then choose new cluster centers in such a way that they are far from existing centers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa1958a9",
      "metadata": {
        "id": "fa1958a9"
      },
      "outputs": [],
      "source": [
        "def k_means_internal(k, X, init):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    k : int\n",
        "      Number of clusters\n",
        "    X : matrix of dimension N x D\n",
        "      Dataset \n",
        "    init : str either '++' or 'random'\n",
        "      Type of initialization for k-means algorithm\n",
        "    '''\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e95e433",
      "metadata": {
        "id": "0e95e433"
      },
      "outputs": [],
      "source": [
        "def k_means(k, X, init='++', n_init=20):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    k : int\n",
        "      Number of clusters\n",
        "    X : matrix of dimension N x D\n",
        "      Dataset \n",
        "    init : str either '++' or 'random'\n",
        "      Type of initialization for k-means algorithm\n",
        "    n_init : int\n",
        "      Number of runs of the algorithms (with different initializations)\n",
        "    '''\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc9e31c1",
      "metadata": {
        "id": "fc9e31c1"
      },
      "source": [
        "### k-means with a fixed number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965b42f9",
      "metadata": {
        "id": "965b42f9"
      },
      "outputs": [],
      "source": [
        "kmeans_labels, l_kmeans = k_means(7, X_scaled, init='++', n_init=20)\n",
        "print(l_kmeans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7928c93",
      "metadata": {
        "id": "b7928c93"
      },
      "outputs": [],
      "source": [
        "# Plot the projection according to the k-means clusters\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(projection[:,0], projection[:,1], projection[:,2], c=kmeans_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f73f0c68",
      "metadata": {
        "id": "f73f0c68"
      },
      "outputs": [],
      "source": [
        "# k is set to the ground truth number of clusters\n",
        "kmeans = KMeans(n_clusters=7, random_state=0, n_init=20).fit(X_scaled)\n",
        "# Plot the projection according to the k-means clusters\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(projection[:,0], projection[:,1], projection[:,2], c=kmeans.labels_)\n",
        "plt.show()\n",
        "normalized_mutual_info_score(kmeans.labels_, y.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cluster Validation \n",
        "\n",
        "Several methods: \n",
        "\n",
        "* (Normalized) Mutual Information: it measure the agreement of the label assegned by k-means vs true labels;\n",
        "\n",
        "* Scree Plot: perform k-means with different number of clusters, register the loss. Plot the loss as function of the number of the classes. An elbow in the scree plot should provide useful information about the parameter $k$."
      ],
      "metadata": {
        "id": "n5T9c7a_JjNL"
      },
      "id": "n5T9c7a_JjNL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661e7348",
      "metadata": {
        "id": "661e7348"
      },
      "outputs": [],
      "source": [
        "# Compute the normalized mutual information between the predicted and the ground truth classification\n",
        "normalized_mutual_info_score(kmeans_labels, y.flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9b8c21",
      "metadata": {
        "id": "2a9b8c21"
      },
      "outputs": [],
      "source": [
        "# scree plot\n",
        "nk_base = np.arange(2,21) # possible values for k in k-means\n",
        "loss = np.zeros(nk_base.shape[0])\n",
        "i = 0\n",
        "for nk in nk_base:\n",
        "    ll,l_kmeans = k_means(nk, X_scaled, init='++', n_init=20)\n",
        "    loss[i] = l_kmeans\n",
        "    i = i + 1\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "ax.scatter(nk_base, np.log(loss), c='b')\n",
        "ax.set_xticks(nk_base)\n",
        "ax.set_title('Scree Plot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d0b86e2",
      "metadata": {
        "id": "1d0b86e2"
      },
      "source": [
        "### Build the scree plot\n",
        "\n",
        "In order to locate approximately the elbow, we fit the first, let's say 4 points with a line and the last 4 points with a line, then the elbow will be approximately at the intersection. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7a5012d",
      "metadata": {
        "id": "a7a5012d"
      },
      "outputs": [],
      "source": [
        "nk_base = np.arange(2,21)\n",
        "loss = np.zeros(nk_base.shape[0])\n",
        "i = 0\n",
        "for nk in nk_base:\n",
        "    kmeans = KMeans(n_clusters=nk, random_state=0,n_init=20).fit(X_scaled)\n",
        "    loss[i] = kmeans.inertia_\n",
        "    i = i + 1\n",
        "reg = LinearRegression().fit(nk_base[:4].reshape(-1, 1),np.log(loss[:4]))\n",
        "aa=reg.predict(nk_base[:8].reshape(-1, 1))\n",
        "reg2 = LinearRegression().fit(nk_base[16:20].reshape(-1, 1),np.log(loss[16:20]))\n",
        "bb=reg2.predict(nk_base.reshape(-1, 1))\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "ax.scatter(nk_base,np.log(loss),c='b')\n",
        "ax.set_xticks(nk_base)\n",
        "ax.set_title('Scree Plot')\n",
        "ax.plot(nk_base[:8],aa[:8])\n",
        "ax.plot(nk_base,bb)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d3090c",
      "metadata": {
        "id": "a8d3090c"
      },
      "outputs": [],
      "source": [
        "# linear fit of first 4 points\n",
        "reg = LinearRegression().fit(nk_base[:4].reshape(-1, 1), np.log(loss[:4]))\n",
        "aa = reg.predict(nk_base[:8].reshape(-1, 1))\n",
        "# linear fit of last 4 points\n",
        "reg2 = LinearRegression().fit(nk_base[16:20].reshape(-1, 1), np.log(loss[16:20]))\n",
        "bb = reg2.predict(nk_base.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b14d66e",
      "metadata": {
        "id": "8b14d66e"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "ax.scatter(nk_base, np.log(loss), c='b')\n",
        "ax.set_xticks(nk_base)\n",
        "ax.set_title('Scree Plot')\n",
        "ax.plot(nk_base[:8], aa[:8])\n",
        "ax.plot(nk_base, bb)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5efee22b",
      "metadata": {
        "id": "5efee22b"
      },
      "source": [
        "Which is the optimal number of clusters according with the scree plot?\n",
        "\n",
        "What happens if you don't initialize many times the algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ada54e6",
      "metadata": {
        "id": "0ada54e6"
      },
      "source": [
        "## Play with other algorithms dimensionality reduction/clustering algorithms.\n",
        "\n",
        "Try to obtain more information using other algorithms that we have seen during the lectures. Among the suggested algorithms, you can use ISOMAP or t-SNE for dimensional reduction, ward's hierarchical clustering, GMM or DBSCAN for clustering. You don't need to implement these algorithms, use any of the libraries in which them are already implemented (sklearn/scipy). \n",
        "\n",
        "Since it's relatively easy, you can try to implement Density Peaks clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t-SNE (t-distributed stpchastoc neighbor embedding)\n",
        "\n",
        "**Goal**: estimate, from the distances in the high- dimensional space, the probability of each point to be a neighbor of each other point. Then, the algorithm goal is to obtain a set of projected coordinates in which these “neighborhood probabilities” are as similar as possible to the ones in the original space."
      ],
      "metadata": {
        "id": "kK6NJ9sJBStW"
      },
      "id": "kK6NJ9sJBStW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a261f6",
      "metadata": {
        "id": "d6a261f6"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
        "                  init='random', perplexity=15).fit_transform(X_scaled)\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "ax.scatter(X_embedded[:,0], X_embedded[:,1], c=y)\n",
        "ax.set_title('t-SNE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN (density-based spatial clustering of application with noise)\n",
        "\n",
        "**Goal**: clusters are connected regions with density above a threshold surrounded by regions with a density below this threshold. The density threshold is defined by two parameters: a neighborhood distance (*eps*) and the minimum number of configurations within this distance needed for considering a given configuration above the density threshold (*min_samples*)"
      ],
      "metadata": {
        "id": "gWk3pqLaC7W-"
      },
      "id": "gWk3pqLaC7W-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd77b18",
      "metadata": {
        "id": "6bd77b18"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "dbscan = DBSCAN(eps=0.7, min_samples=12).fit(X_scaled)\n",
        "\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(projection[:,0], projection[:,1], projection[:,2], c=dbscan.labels_)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agglomerative Clustering (hierarchical clustering)\n",
        "\n",
        "**Goal**: each configuration is initially assigned to a different cluster. At every iteration two existing clusters (i.e. the two closest according to the chosen criterion) are combined, so the next level of the dendrogram has one fewer cluster. \n",
        "\n"
      ],
      "metadata": {
        "id": "g0Kfaw4dD-5y"
      },
      "id": "g0Kfaw4dD-5y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ead2d2e",
      "metadata": {
        "id": "9ead2d2e"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    # model.children_[i][0] and model.children_[i][1] are merged at level i\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  \n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "    # linkage should contain [children_0, children_1, distance, n_points]\n",
        "    linkage_matrix = np.column_stack(\n",
        "        [model.children_, model.distances_, counts]\n",
        "    ).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)\n",
        "\n",
        "\n",
        "# setting distance_threshold=0 ensures we compute the full tree.\n",
        "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
        "\n",
        "model = model.fit(X_scaled)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "# plot the top three levels of the dendrogram\n",
        "plot_dendrogram(model, truncate_mode=\"level\", p=4)\n",
        "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ward** linkage criterionminimizes the variance of the clusters being merged."
      ],
      "metadata": {
        "id": "6L6ZDzvrHEYp"
      },
      "id": "6L6ZDzvrHEYp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58aef084",
      "metadata": {
        "id": "58aef084"
      },
      "outputs": [],
      "source": [
        "ward = AgglomerativeClustering(n_clusters=7).fit(X_scaled) # ward is default linkage criterion\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(projection[:,0],projection[:,1], projection[:,2],c=ward.labels_)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c9b5a9",
      "metadata": {
        "id": "10c9b5a9"
      },
      "outputs": [],
      "source": [
        "print (\"kmeans\", normalized_mutual_info_score(kmeans_labels, y.flatten()))\n",
        "print (\"dbscan\", normalized_mutual_info_score(dbscan.labels_, y.flatten()))\n",
        "print (\"ward's\", normalized_mutual_info_score(ward.labels_, y.flatten()))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}