{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaiaSaveri/intro-to-ml/blob/main/solved-notebooks/SOLVED-Lab-5.UnsupervisedLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7085776",
      "metadata": {
        "id": "a7085776"
      },
      "source": [
        "# Unsupervised Learning on the Dry Bean Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7470076a",
      "metadata": {
        "id": "7470076a"
      },
      "source": [
        "In this lab we will try to obtain valuable information using Unsupervised Learning techniques.\n",
        "\n",
        "The original data has been downloaded from https://archive-beta.ics.uci.edu/dataset/602/dry+bean+dataset (Dry Bean Dataset. (2020). UCI Machine Learning Repository)\n",
        "\n",
        "**Data Set Description**:\n",
        "\n",
        "Images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. \n",
        "A total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb238ed2",
      "metadata": {
        "id": "fb238ed2"
      },
      "source": [
        "## Loading and Pre-treatment of the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8caaf657",
      "metadata": {
        "id": "8caaf657"
      },
      "outputs": [],
      "source": [
        "# Load libraries and modules\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FFILE = './Dry_Bean_Dataset.xlsx'\n",
        "if os.path.isfile(FFILE): \n",
        "    print(\"File already exists\")\n",
        "    if os.access(FFILE, os.R_OK):\n",
        "        print (\"File is readable\")\n",
        "    else:\n",
        "        print (\"File is not readable, removing it and downloading again\")\n",
        "        !rm FFILE\n",
        "        !wget \"https://raw.github.com/alexdepremia/ML_IADA_UTs/main/Lab5/Dry_Bean_Dataset.xlsx\"\n",
        "else:\n",
        "    print(\"Either the file is missing or not readable, download it\")\n",
        "    !wget \"https://raw.github.com/alexdepremia/ML_IADA_UTs/main/Lab5/Dry_Bean_Dataset.xlsx\""
      ],
      "metadata": {
        "id": "AkGFWaHZOi-l"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AkGFWaHZOi-l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56d03c3c",
      "metadata": {
        "id": "56d03c3c"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = pd.read_excel('./Dry_Bean_Dataset.xlsx')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afdd3bf6",
      "metadata": {
        "id": "afdd3bf6"
      },
      "outputs": [],
      "source": [
        "# Transform the data to use it as numpy arrays. \n",
        "X = data.iloc[:,:-1].values\n",
        "label = data.iloc[:,16].values\n",
        "print(X.shape)\n",
        "N = X.shape[0]  # Number of data points\n",
        "nc = X.shape[1]  # Number of features/components\n",
        "print(np.unique(label)) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordinal encoder for the ground truth labels\n",
        "enc = preprocessing.OrdinalEncoder()\n",
        "enc.fit(label.reshape(-1, 1))\n",
        "y = enc.transform(label.reshape(-1, 1))\n",
        "print(y)  # Encoded labels"
      ],
      "metadata": {
        "id": "IEQuXTH-pyRV"
      },
      "id": "IEQuXTH-pyRV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "480d1d88",
      "metadata": {
        "id": "480d1d88"
      },
      "outputs": [],
      "source": [
        "# Rescale the features of the data since the units are different: substract the average and divide by the standard deviation\n",
        "scaler = preprocessing.StandardScaler().fit(X)\n",
        "X_scaled = scaler.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7790c6b",
      "metadata": {
        "id": "c7790c6b"
      },
      "source": [
        "## Principal Component Analysis (PCA)\n",
        "\n",
        "**Objective**: find the set of orthogonal directions along which the variance of the data is the highest.\n",
        "\n",
        "Summary of the method:\n",
        "\n",
        "* Center the data feature matrix $X$;\n",
        "\n",
        "* Compute the covariance matrix $C$ of the data as $C=X^T X$;\n",
        "\n",
        "* Compute eigenvalues and eigenvectors of the covariance matrix $C$ (use function `eigh` of `scipy.LA`), and sort them according to the decreasing order of the eigenvalues. Arrange them as coumn of a matrix $A$.\n",
        "\n",
        "**Recall**: eigenvalues are the variance of the data along the direction of the corresponding eigenvector. \n",
        "\n",
        "* Compute principal components as $X\\cdot A$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33818ba7",
      "metadata": {
        "id": "33818ba7"
      },
      "outputs": [],
      "source": [
        "# Since the features are already centered, we can compute the covariance matrix as:\n",
        "# C=X^t X\n",
        "Cov = np.matmul(X_scaled.transpose(), X_scaled)\n",
        "lamb, v = LA.eigh(Cov)  # eigenvector and eigenvalues of the covariance matrix\n",
        "idx = np.argsort(-lamb)\n",
        "lambs = lamb[idx]\n",
        "vs = v[:,idx]\n",
        "print(lambs)\n",
        "projection = np.matmul(X_scaled, vs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Assessment: choose the number of PCs to keep\n",
        "\n",
        "**Objective**: choose the final dimension $d$ of the transformed data.\n",
        "\n",
        "1. Proportion of variance explained: given eigenvalues $\\lambda_i$ of the covariance matrix and a threshold $t\\in [0, 1]$, choose $d$ s.t. the ratio $\\chi_d = \\frac{\\sum_{i=1}^d \\lambda_i}{\\sum_{i=1}^D \\lambda_i} > t$ \n",
        "\n",
        "2. Check the existence of a gap in the spectrum of the covariance matrix."
      ],
      "metadata": {
        "id": "Ob2sydGDyCca"
      },
      "id": "Ob2sydGDyCca"
    },
    {
      "cell_type": "code",
      "source": [
        "# prportion of variance explained\n",
        "cumul = np.zeros(nc)\n",
        "total = np.sum(lambs)\n",
        "for i in range(nc):\n",
        "    cumul[i] = np.sum(lambs[:i+1])/total\n",
        "comp = np.arange(nc) + 1"
      ],
      "metadata": {
        "id": "5wZfvpyDx__4"
      },
      "id": "5wZfvpyDx__4",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "eb4d30c5",
      "metadata": {
        "id": "eb4d30c5"
      },
      "outputs": [],
      "source": [
        "# PCA with sklearn \n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "projection = pca.transform(X_scaled)\n",
        "cumul = np.zeros(nc)\n",
        "for i in range(nc):\n",
        "    cumul[i] = np.sum(pca.explained_variance_ratio_[:i+1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00dcd9e",
      "metadata": {
        "id": "d00dcd9e"
      },
      "outputs": [],
      "source": [
        "f, [ax1, ax2] = plt.subplots(1, 2,figsize = (14, 7))\n",
        "ax1.set_title('Spectrum')\n",
        "ax1.scatter(comp, lambs)\n",
        "ax1.set_xticks(comp)\n",
        "ax2.set_title('Explained variance')\n",
        "ax2.scatter(comp, cumul)\n",
        "ax2.set_xticks(comp)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "551c4b17",
      "metadata": {
        "id": "551c4b17"
      },
      "outputs": [],
      "source": [
        "# Number of components depending on the explained variance threshold.\n",
        "for t in [0.8, 0.85, 0.9, 0.95, 0.97, 0.99, 0.999]:\n",
        "    nc = np.argmax(cumul - t > 0.) + 1\n",
        "    print(t, nc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02876b8e",
      "metadata": {
        "id": "02876b8e"
      },
      "outputs": [],
      "source": [
        "# plotting the data set in 2D (i.e. keep only 2 PCs) colored by its ground truth label\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "ax.scatter(projection[:,0],projection[:,1], c=y)\n",
        "ax.set_title('PCA')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed9d71c7",
      "metadata": {
        "id": "ed9d71c7"
      },
      "outputs": [],
      "source": [
        "# Now in 3D (i.e. keep only 3 PCs)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(projection[:,0],projection[:,1], projection[:,2],c=y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cc6cab0",
      "metadata": {
        "id": "5cc6cab0"
      },
      "source": [
        "1. What would happen if we don't rescale the features?\n",
        "\n",
        "2. Which is the Intrinsic Dimension (ID) of the data set?\n",
        "\n",
        "  **Recall**: the ID of a dataset is the minimum number of dimensions we need to describe the data in a accurate way.\n",
        "\n",
        "3. Could you compute the two-NN estimate of the ID?\n",
        "\n",
        "  The procedure works as follows:\n",
        "\n",
        "  1. Compute pairwise distances among points;\n",
        "  2. For each point $i$, extract the distance from its two closest neighbors $r_{i1}, r_{i2}$ respectively;\n",
        "  3. Compute the ratio $\\mu_i = \\frac{r_{i2}}{r_i1}$;\n",
        "  4. Compute the empirical cumulative distribution $\\mathcal{F}(\\mu)$ of $\\mu$;\n",
        "  5. Find the best fitting line for the dataset $\\{\\log \\mu_i, \\log (1 - \\mathcal{F}(\\mu_i)\\}_{i=1}^N$\n",
        "  6. The intrinsic dimension is given by the slope of this fitted line."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute matrix distance\n",
        "X_scaled_u = np.unique(X_scaled, axis=0)\n",
        "dist = cdist(X_scaled_u, X_scaled_u)\n",
        "dist.sort(axis=1)\n",
        "mu_i = np.divide(dist[:, 2], dist[:, 1])\n",
        "log_mu_i = np.log(mu_i)\n",
        "two_nn = 1 / np.mean(log_mu_i)\n",
        "print(two_nn)"
      ],
      "metadata": {
        "id": "JIT9oG90M22p"
      },
      "id": "JIT9oG90M22p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8c81ebe4",
      "metadata": {
        "id": "8c81ebe4"
      },
      "source": [
        "## K-means\n",
        "\n",
        "Flat clustering algorithms whose goal is to minimize the intracluster distance while maximizing the intercluster distance. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "539441d4",
      "metadata": {
        "id": "539441d4"
      },
      "source": [
        "We will compute the k-means clustering using two types of initialization:\n",
        "    \n",
        "  1. Random initialization: cluster centroids are initialized picking random points from the dataset;\n",
        "  \n",
        "  2. k-means++: choose first cluster center at random, then choose new cluster centers in such a way that they are far from existing centers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fa1958a9",
      "metadata": {
        "id": "fa1958a9"
      },
      "outputs": [],
      "source": [
        "def k_means_internal(k, X, init):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    k : int\n",
        "      Number of clusters\n",
        "    X : matrix of dimension N x D\n",
        "      Dataset \n",
        "    init : str either '++' or 'random'\n",
        "      Type of initialization for k-means algorithm\n",
        "    '''\n",
        "    N = X.shape[0] # number of points \n",
        "    nc = X.shape[1] # number of coordinates\n",
        "    ll = np.arange(k) \n",
        "    z = np.zeros(N, dtype='int') # cluster number assigned to each data point\n",
        "    cent = np.zeros([k, nc]) # coordinates of the cluster centers\n",
        "    # k-means++\n",
        "    if (init=='++'):\n",
        "        b = np.random.choice(N, 1, replace=False) # choose the first cluster center at random\n",
        "        cent[0, :] = X[b, :]\n",
        "        nchosen = 1 # number of cluster centers already set\n",
        "        while (nchosen < k):\n",
        "            dist = cdist(cent[:nchosen, :], X) # distance of each point from the cluster centers\n",
        "            dmin = np.min(dist, axis=0) # min distance btw point and cluster centers\n",
        "            prob = dmin**2 \n",
        "            prob = prob/np.sum(prob) \n",
        "            # choose next center according to the computed prob\n",
        "            b = np.random.choice(N, 1, replace=False, p=prob) \n",
        "            cent[nchosen, :] = X[b,:]\n",
        "            nchosen = nchosen + 1\n",
        "    # random initialization\n",
        "    else:\n",
        "        b = np.random.choice(N, k, replace=False)  # choose the k centers randomly \n",
        "        for i in ll:\n",
        "            cent[i, :] = X[b[i],:]\n",
        "    dist = cdist(cent, X)  # distance of each point from cluster centers \n",
        "    z_new = np.argmin(dist, axis=0) # assign each point to cluster with closest center\n",
        "    dmin = np.min(dist,axis=0)\n",
        "    niter = 0\n",
        "    L = np.sum((dmin)**2) # loss function evaluation\n",
        "    while (z_new != z).any(): # until a stable configuration is reached\n",
        "        z = np.copy(z_new)\n",
        "        for i in range(k):\n",
        "            cent[i, :] = np.average(X[z==i,:],axis=0) # compute cluster centroids\n",
        "        dist = cdist(cent, X) # update distances from cluster centers\n",
        "        z_new = np.argmin(dist, axis=0) # find cluster with min centroid distance\n",
        "        dmin = np.min(dist, axis=0)\n",
        "        L = np.sum(dmin**2) # loss function evaluation\n",
        "        niter = niter + 1\n",
        "    return (z_new, L, niter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0e95e433",
      "metadata": {
        "id": "0e95e433"
      },
      "outputs": [],
      "source": [
        "def k_means(k, X, init='++', n_init=20):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    k : int\n",
        "      Number of clusters\n",
        "    X : matrix of dimension N x D\n",
        "      Dataset \n",
        "    init : str either '++' or 'random'\n",
        "      Type of initialization for k-means algorithm\n",
        "    n_init : int\n",
        "      Number of runs of the algorithms (with different initializations)\n",
        "    '''\n",
        "    lmin = 9.9*10**99\n",
        "    for i in range(n_init):\n",
        "        labels, loss, niter = k_means_internal(k, X_scaled, init=init)\n",
        "        if (loss < lmin):  # store the best performing iteration\n",
        "            lmin = loss\n",
        "            labels_opt = labels\n",
        "    return (labels_opt, lmin)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc9e31c1",
      "metadata": {
        "id": "fc9e31c1"
      },
      "source": [
        "### k-means with a fixed number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965b42f9",
      "metadata": {
        "id": "965b42f9"
      },
      "outputs": [],
      "source": [
        "kmeans_labels, l_kmeans = k_means(7, X_scaled, init='++', n_init=20)\n",
        "print(l_kmeans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7928c93",
      "metadata": {
        "id": "b7928c93"
      },
      "outputs": [],
      "source": [
        "# Plot the projection according to the k-means clusters\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(projection[:,0], projection[:,1], projection[:,2], c=kmeans_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cluster Validation \n",
        "\n",
        "Several methods: \n",
        "\n",
        "* (Normalized) Mutual Information: it measure the agreement of the label assegned by k-means vs true labels;\n",
        "\n",
        "* Scree Plot: perform k-means with different number of clusters, register the loss. Plot the loss as function of the number of the classes. An elbow in the scree plot should provide useful information about the parameter $k$."
      ],
      "metadata": {
        "id": "n5T9c7a_JjNL"
      },
      "id": "n5T9c7a_JjNL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661e7348",
      "metadata": {
        "id": "661e7348",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d887e6bd-f058-45d1-ed8f-e50054ed21ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7139632055034242"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Compute the normalized mutual information between the predicted and the ground truth classification\n",
        "normalized_mutual_info_score(kmeans_labels, y.flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9b8c21",
      "metadata": {
        "id": "2a9b8c21"
      },
      "outputs": [],
      "source": [
        "# scree plot\n",
        "nk_base = np.arange(2,21) # possible values for k in k-means\n",
        "loss = np.zeros(nk_base.shape[0])\n",
        "i = 0\n",
        "for nk in nk_base:\n",
        "    ll,l_kmeans = k_means(nk, X_scaled, init='++', n_init=20)\n",
        "    loss[i] = l_kmeans\n",
        "    i = i + 1\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "ax.scatter(nk_base, np.log(loss), c='b')\n",
        "ax.set_xticks(nk_base)\n",
        "ax.set_title('Scree Plot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f73f0c68",
      "metadata": {
        "id": "f73f0c68"
      },
      "outputs": [],
      "source": [
        "# k is set to the ground truth number of clusters\n",
        "kmeans = KMeans(n_clusters=7, random_state=0, n_init=20).fit(X_scaled)\n",
        "# Plot the projection according to the k-means clusters\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(projection[:,0], projection[:,1], projection[:,2], c=kmeans.labels_)\n",
        "plt.show()\n",
        "normalized_mutual_info_score(kmeans.labels_, y.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d0b86e2",
      "metadata": {
        "id": "1d0b86e2"
      },
      "source": [
        "### Build the scree plot\n",
        "\n",
        "In order to locate approximately the elbow, we fit the first, let's say 4 points with a line and the last 4 points with a line, then the elbow will be approximately at the intersection. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d3090c",
      "metadata": {
        "id": "a8d3090c"
      },
      "outputs": [],
      "source": [
        "# linear fit of first 4 points\n",
        "reg = LinearRegression().fit(nk_base[:4].reshape(-1, 1), np.log(loss[:4]))\n",
        "aa = reg.predict(nk_base[:8].reshape(-1, 1))\n",
        "# linear fit of last 4 points\n",
        "reg2 = LinearRegression().fit(nk_base[16:20].reshape(-1, 1), np.log(loss[16:20]))\n",
        "bb = reg2.predict(nk_base.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b14d66e",
      "metadata": {
        "id": "8b14d66e"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "ax.scatter(nk_base, np.log(loss), c='b')\n",
        "ax.set_xticks(nk_base)\n",
        "ax.set_title('Scree Plot')\n",
        "ax.plot(nk_base[:8], aa[:8])\n",
        "ax.plot(nk_base, bb)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7a5012d",
      "metadata": {
        "id": "a7a5012d"
      },
      "outputs": [],
      "source": [
        "nk_base = np.arange(2,21)\n",
        "loss = np.zeros(nk_base.shape[0])\n",
        "i = 0\n",
        "for nk in nk_base:\n",
        "    kmeans = KMeans(n_clusters=nk, random_state=0, n_init=20).fit(X_scaled)\n",
        "    loss[i] = kmeans.inertia_\n",
        "    i = i+1\n",
        "reg = LinearRegression().fit(nk_base[:4].reshape(-1, 1),np.log(loss[:4]))\n",
        "aa = reg.predict(nk_base[:8].reshape(-1, 1))\n",
        "reg2 = LinearRegression().fit(nk_base[16:20].reshape(-1, 1), np.log(loss[16:20]))\n",
        "bb = reg2.predict(nk_base.reshape(-1, 1))\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "ax.scatter(nk_base,np.log(loss), c='b')\n",
        "ax.set_xticks(nk_base)\n",
        "ax.set_title('Scree Plot')\n",
        "ax.plot(nk_base[:8], aa[:8])\n",
        "ax.plot(nk_base,bb)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5efee22b",
      "metadata": {
        "id": "5efee22b"
      },
      "source": [
        "Which is the optimal number of clusters according with the scree plot?\n",
        "\n",
        "What happens if you don't initialize many times the algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ada54e6",
      "metadata": {
        "id": "0ada54e6"
      },
      "source": [
        "## Play with other algorithms dimensionality reduction/clustering algorithms.\n",
        "\n",
        "Try to obtain more information using other algorithms that we have seen during the lectures. Among the suggested algorithms, you can use ISOMAP or t-SNE for dimensional reduction, ward's hierarchical clustering, GMM or DBSCAN for clustering. You don't need to implement these algorithms, use any of the libraries in which them are already implemented (sklearn/scipy). \n",
        "\n",
        "Since it's relatively easy, you can try to implement Density Peaks clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a261f6",
      "metadata": {
        "id": "d6a261f6"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
        "                  init='random', perplexity=15).fit_transform(X_scaled)\n",
        "fig, ax =plt.subplots(figsize=(7,7))\n",
        "ax.scatter(X_embedded[:,0],X_embedded[:,1], c=y)\n",
        "ax.set_title('t-SNE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd77b18",
      "metadata": {
        "id": "6bd77b18"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "dbscan = DBSCAN(eps=0.7, min_samples=12).fit(X_scaled)\n",
        "\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(projection[:,0],projection[:,1], projection[:,2],c=dbscan.labels_)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ead2d2e",
      "metadata": {
        "id": "9ead2d2e"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack(\n",
        "        [model.children_, model.distances_, counts]\n",
        "    ).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)\n",
        "\n",
        "\n",
        "# setting distance_threshold=0 ensures we compute the full tree.\n",
        "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
        "\n",
        "model = model.fit(X_scaled)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "# plot the top three levels of the dendrogram\n",
        "plot_dendrogram(model, truncate_mode=\"level\", p=4)\n",
        "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58aef084",
      "metadata": {
        "id": "58aef084"
      },
      "outputs": [],
      "source": [
        "ward = AgglomerativeClustering(n_clusters=7).fit(X_scaled)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(projection[:,0],projection[:,1], projection[:,2],c=ward.labels_)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c9b5a9",
      "metadata": {
        "id": "10c9b5a9"
      },
      "outputs": [],
      "source": [
        "print (\"kmeans\", normalized_mutual_info_score(kmeans_labels, y.flatten()))\n",
        "print (\"dbscan\", normalized_mutual_info_score(dbscan.labels_, y.flatten()))\n",
        "print (\"ward's\", normalized_mutual_info_score(ward.labels_, y.flatten()))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}