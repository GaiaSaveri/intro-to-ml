{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNd9UwbpHN35msQ8jchz2r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaiaSaveri/intro-to-ml/blob/main/solved-notebooks/SOLVED-Lab-7.KNNGaussianNaiveBayesTrees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification with KNN, Trees and Gaussian Naive Bayes"
      ],
      "metadata": {
        "id": "Lo66foonmvxg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "v3kH50wbmjBL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and split the data from the Unsupervise Learning Dataset (Lab 5, Dry Bean Dataset):"
      ],
      "metadata": {
        "id": "S_cnE38km64G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FFILE = './Dry_Bean_Dataset.xlsx'\n",
        "if os.path.isfile(FFILE): \n",
        "    print(\"File already exists\")\n",
        "    if os.access(FFILE, os.R_OK):\n",
        "        print (\"File is readable\")\n",
        "    else:\n",
        "        print (\"File is not readable, removing it and downloading again\")\n",
        "        !rm FFILE\n",
        "        !wget \"https://raw.github.com/alexdepremia/ML_IADA_UTs/main/Lab5/Dry_Bean_Dataset.xlsx\"\n",
        "else:\n",
        "    print(\"Either the file is missing or not readable, download it\")\n",
        "    !wget \"https://raw.github.com/alexdepremia/ML_IADA_UTs/main/Lab5/Dry_Bean_Dataset.xlsx\""
      ],
      "metadata": {
        "id": "g0eM9w9HnRDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_excel('./Dry_Bean_Dataset.xlsx')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "CPt8p6GsnTGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide features and label. Split the data in train and test set and **after that** normalize them:"
      ],
      "metadata": {
        "id": "H5m3HOy0nny0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.sample(frac=1,random_state=0).reset_index(drop=True) # random shuffle\n",
        "data.head()   "
      ],
      "metadata": {
        "id": "ZpAplcpvnunq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data.iloc[:10000,:]\n",
        "test_data = data.iloc[10000:,:]"
      ],
      "metadata": {
        "id": "K9I8tR3gZanQ"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "id": "w4Sai6BSZkWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize train and test dataset \n",
        "from sklearn import preprocessing\n",
        "label_train = train_data['Class']\n",
        "train_data = train_data.drop('Class', axis=1)\n",
        "columns_name = train_data.columns\n",
        "train_scaler = preprocessing.StandardScaler().fit(train_data)\n",
        "train_data = train_scaler.transform(train_data)\n",
        "train_data = pd.DataFrame(train_data, columns=columns_name)\n",
        "train_data['Class'] = label_train\n",
        "label_test = test_data['Class']\n",
        "test_data = test_data.drop('Class', axis=1)\n",
        "test_scaler = preprocessing.StandardScaler().fit(test_data)\n",
        "test_data = test_scaler.transform(test_data)\n",
        "test_data = pd.DataFrame(test_data, columns=columns_name)\n",
        "test_data['Class'] = label_test"
      ],
      "metadata": {
        "id": "tIl6l3KXZsO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "WdaI84S-eDNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before feeding the data into the following algorithms, try to perform PCA, varying the number of PCs, and check what changes**"
      ],
      "metadata": {
        "id": "YxDH5MYE21sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Nearest Neighbors Classification \n",
        "\n",
        "Implement the KNN algorithm for classification."
      ],
      "metadata": {
        "id": "FptPlXMAnuym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distance(point_one, point_two):\n",
        "    return euclidean(point_one, point_two)\n",
        "\n",
        "def get_neighbors(train_set, test_point, label_col, n_neighbors):\n",
        "  dist = np.array([distance(train_point, test_point) for train_point in train_set])\n",
        "  idx_dist = dist.argsort()\n",
        "  ordered_train = train_set[idx_dist, :]\n",
        "  ordered_label = label_col[idx_dist]\n",
        "  return ordered_train[:n_neighbors], ordered_label[:n_neighbors]\n",
        "\n",
        "def predict(train_set, test_point, labels, n_neighbors):\n",
        "  neigh, neigh_label = get_neighbors(train_set, test_point, labels, n_neighbors)\n",
        "  values, counts = np.unique(neigh_label, return_counts=True)\n",
        "  idx = np.argmax(counts)\n",
        "  return values[idx]\n",
        "\n",
        "def evaluate(train_set, test_set, label, n_neighbors=2):\n",
        "    correct_preditct = 0\n",
        "    wrong_preditct = 0\n",
        "    train_labels = train_set[label].values\n",
        "    train_set = train_set.drop(label, axis=1)\n",
        "    test_labels = test_set[label].values\n",
        "    test_set = test_set.drop(label, axis=1)\n",
        "    for index in range(len(test_set.index)):  # for each row in the dataset\n",
        "        result = predict(train_set.values, test_set.iloc[index].values, train_labels, n_neighbors)  # predict the row\n",
        "        if result == test_labels[index]:  # predicted value and expected value is same or not\n",
        "            correct_preditct += 1  # increase correct count\n",
        "        else:\n",
        "            wrong_preditct += 1  # increase incorrect count\n",
        "    accuracy = correct_preditct / (correct_preditct + wrong_preditct)  # calculating accuracy\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "urjD3tGR3DV7"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_accuracy = evaluate(train_data, test_data, 'Class')"
      ],
      "metadata": {
        "id": "cOUsnIsElYNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees with Numerical Features \n",
        "\n",
        "Modify the implementation of decision trees to account for numerical input features."
      ],
      "metadata": {
        "id": "TZd_CqNr3DnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute H(S)\n",
        "def entropy(train_data, label, class_list):\n",
        "    total_row = train_data.shape[0]  # the total size of the dataset  \n",
        "    total_entr = 0\n",
        "    for c in class_list:  # for each possible class in the label\n",
        "        total_class_count = train_data[train_data[label] == c].shape[0]  # number of points belonging to the class\n",
        "        if total_class_count > 0:\n",
        "          total_class_entr = - (total_class_count/total_row)*np.log2(total_class_count/total_row)  # entropy of the class\n",
        "          total_entr += total_class_entr  # adding the class entropy to the total entropy of the dataset\n",
        "    return total_entr"
      ],
      "metadata": {
        "id": "2yVWtuSI3Q07"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute H(S_j)\n",
        "def feature_entropy(left_data, right_data, label, class_list):\n",
        "    row_count = left_data.shape[0] + right_data.shape[0] # n points considered\n",
        "    p_left = left_data.shape[0] / row_count\n",
        "    p_right = right_data.shape[0] / row_count\n",
        "    ent = p_left * entropy(left_data, label, class_list) + p_right * entropy(right_data, label, class_list)\n",
        "    return ent"
      ],
      "metadata": {
        "id": "bAF8R0lT03Um"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(feature_column, threshold):\n",
        "  left_rows = np.argwhere(feature_column <= threshold).flatten()\n",
        "  right_rows = np.argwhere(feature_column > threshold).flatten()\n",
        "  return left_rows, right_rows"
      ],
      "metadata": {
        "id": "caDIpINC86M3"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def information_gain(data, feature_name, label, class_list, threshold):\n",
        "  left_rows, right_rows = split(data[feature_name].values, threshold)\n",
        "  if len(left_rows)==0 or len(right_rows)==0:\n",
        "    return 0\n",
        "  feat_entropy = feature_entropy(data.iloc[left_rows], data.iloc[right_rows], label, class_list)\n",
        "  return feat_entropy"
      ],
      "metadata": {
        "id": "M73J3Pq39UxT"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_split_thresholds(feature_column, n_thresholds):\n",
        "  feature_column = feature_column.values\n",
        "  n_data = len(feature_column)\n",
        "  sorted_column = np.sort(feature_column)\n",
        "  if len(feature_column) > 1:\n",
        "    partitioned_array = np.array_split(feature_column, n_thresholds + 1)\n",
        "    thresholds = [(partitioned_array[i][-1] + partitioned_array[i+1][0])/2 for i in range(len(partitioned_array)-1)]\n",
        "  else:\n",
        "    thresholds = [feature_column[0]]\n",
        "  return thresholds"
      ],
      "metadata": {
        "id": "ceevhRt4M8a3"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def most_informative_feature(train_data, label, class_list, n_thresholds):\n",
        "    feature_list = train_data.columns.drop(label)\n",
        "    min_entropy = 99999\n",
        "    min_entropy_feature = None\n",
        "    min_entropy_threshold = None\n",
        "    for feature in feature_list:\n",
        "      thresholds = get_split_thresholds(train_data[feature], n_thresholds)\n",
        "      for t in thresholds:\n",
        "        info_gain = information_gain(train_data, feature, label, class_list, t)\n",
        "        if info_gain < min_entropy:\n",
        "          min_entropy = info_gain\n",
        "          min_entropy_feature = feature\n",
        "          min_entropy_threshold = t\n",
        "    return min_entropy_feature, min_entropy_threshold"
      ],
      "metadata": {
        "id": "s0_szcme8WPS"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_leaf(train_data, label):\n",
        "  classes_in_node = np.unique(train_data[label])\n",
        "  if len(classes_in_node) == 1:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "metadata": {
        "id": "SFkkyLqj67Ce"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def leaf_class(train_data, label):\n",
        "    class_list, count_class = np.unique(train_data[label], return_counts=True)\n",
        "    idx = count_class.argmax()\n",
        "    return class_list[idx]"
      ],
      "metadata": {
        "id": "nmSk5xUd8SgM"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tree(train_data, label, class_list, n_thresholds, cur_depth, min_samples, max_depth):\n",
        "  if is_leaf(data, label) or cur_depth>=max_depth or len(train_data)<=min_samples:\n",
        "    return leaf_class(train_data, label)\n",
        "  else:\n",
        "    cur_depth += 1\n",
        "    split_feature, split_threshold = most_informative_feature(train_data, label, class_list, n_thresholds)\n",
        "    left_rows, right_rows = split(train_data[split_feature].values, split_threshold)\n",
        "    if len(left_rows)==0 or len(right_rows)==0:\n",
        "      return leaf_class(train_data, label)\n",
        "    else:\n",
        "      # build sub tree\n",
        "      split_condition = \"{} <= {}\".format(split_feature, split_threshold)\n",
        "      sub_tree = {split_condition : []}\n",
        "      # recursive call\n",
        "      left_branch = make_tree(train_data.iloc[left_rows], label, class_list, n_thresholds, cur_depth, min_samples, max_depth)\n",
        "      right_branch = make_tree(train_data.iloc[right_rows], label, class_list, n_thresholds, cur_depth, min_samples, max_depth)\n",
        "      if left_branch == right_branch:\n",
        "        sub_tree = left_branch\n",
        "      else:\n",
        "        # grow the tree\n",
        "        sub_tree[split_condition].append(left_branch)\n",
        "        sub_tree[split_condition].append(right_branch)\n",
        "      return sub_tree"
      ],
      "metadata": {
        "id": "kkFc6PyRSTry"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# id3 call\n",
        "def id3(train_data_m, label, n_thresholds=1, min_samples=4, max_depth=6):\n",
        "    train_data = train_data_m.copy()  # getting a copy of the dataset\n",
        "    class_list = train_data[label].unique()  # getting unqiue classes of the label\n",
        "    tree = make_tree(train_data, label, class_list, n_thresholds, 0, min_samples, max_depth)  # start calling recursion\n",
        "    return tree"
      ],
      "metadata": {
        "id": "DrEcz2r31Mek"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = id3(train_data, 'Class')\n",
        "print(t)"
      ],
      "metadata": {
        "id": "9qYx-Fk6CAMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test_point, tree):\n",
        "    if not isinstance(tree, dict):\n",
        "      return tree\n",
        "    question = list(tree.keys())[0]\n",
        "    attribute, value = question.split(\" <= \")\n",
        "    if test_point[attribute] <= float(value):\n",
        "        answer = tree[question][0]\n",
        "    else:\n",
        "        answer = tree[question][1]\n",
        "    return predict(test_point, answer)\n",
        "\n",
        "def evaluate(tree, test_data, label):\n",
        "    correct_preditct = 0\n",
        "    wrong_preditct = 0\n",
        "    for index in range(len(test_data.index)):  # for each row in the dataset\n",
        "        result = predict(test_data.iloc[index], tree)  # predict the row\n",
        "        if result == test_data[label].iloc[index]:  # predicted value and expected value is same or not\n",
        "            correct_preditct += 1  # increase correct count\n",
        "        else:\n",
        "            wrong_preditct += 1  # increase incorrect count\n",
        "    accuracy = correct_preditct / (correct_preditct + wrong_preditct)  # calculating accuracy\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "ZTgPotzITQL7"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian Naive Bayes \n",
        "Modufy the implemntation of naive Bayes to accout for numerical input features. The likelihood of each class ($p(data|class)$) is assumed to be a Gaussian $\\frac{1}{\\sqrt(\\sigma^2 2 \\pi)} \\exp (\\frac{1}{2} \\frac{(x-\\mu)}{\\sigma^2})$, where $\\mu, \\sigma^2$ are the mean and the variance for each class;"
      ],
      "metadata": {
        "id": "eL_3XYt33RH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prior(train_data, label):\n",
        "  priors = train_data.groupby(by=label).apply(lambda x: len(x)/len(train_data))\n",
        "  return np.log(priors).values\n",
        "\n",
        "def mean_variance(train_data, label):\n",
        "  mean = train_data.groupby(by=label).apply(lambda x: x.mean(axis=0))\n",
        "  variance = train_data.groupby(by=label).apply(lambda x: x.var(axis=0))\n",
        "  return (mean.values, variance.values)\n",
        "\n",
        "def gaussian_density(mean, variance, point):\n",
        "  d = (1 / np.sqrt(2*np.pi*variance)) * np.exp((-(point - mean)**2) / (2*variance))\n",
        "  return d\n",
        "\n",
        "def train_gaussian_naive_bayes(train_data, label):\n",
        "  mean, variance = mean_variance(train_data, label)\n",
        "  priors = prior(train_data, label)\n",
        "  unique_labels = train_data[label].unique()\n",
        "  n_labels = len(unique_labels)\n",
        "  return {'n_labels': n_labels, 'unique_labels': unique_labels, 'n_classes': n_labels, 'mean': mean, \n",
        "          'variance': variance, 'prior': priors}"
      ],
      "metadata": {
        "id": "T_QIqQgUn9X8"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gaus_bayes = train_gaussian_naive_bayes(train_data, 'Class')"
      ],
      "metadata": {
        "id": "m3sq6De4qIhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def posterior(point, mean, variance, class_list, n_classes, n_feat):\n",
        "  posteriors = []\n",
        "  for i in range(n_classes):\n",
        "    posterior = 0\n",
        "    for j in range(n_feat):\n",
        "      posterior += np.log(gaussian_density(mean[i][j], variance[i][j], point[j]))\n",
        "    posteriors.append(posterior)\n",
        "  return posteriors\n",
        "\n",
        "def predict(test_data, label, gaus_bayes):\n",
        "  predictions = []\n",
        "  n_feat = len(test_data.columns) - 1\n",
        "  for i in range(len(test_data)):\n",
        "    pr = gaus_bayes['prior']\n",
        "    post = posterior(test_data.iloc[i, :-1], gaus_bayes['mean'], gaus_bayes['variance'], \n",
        "                     gaus_bayes['unique_labels'], gaus_bayes['n_classes'], n_feat)\n",
        "    prob = pr + post\n",
        "    max_prob_class_idx = np.argmax(prob)\n",
        "    predictions.append(gaus_bayes['unique_labels'][max_prob_class_idx])\n",
        "  return predictions \n",
        "\n",
        "def evaluate(test_data, label, gaus_bayes):\n",
        "  gaus_pred = predict(test_data, label, gaus_bayes)\n",
        "  correct_predict = 0\n",
        "  wrong_predict = 0\n",
        "  for index in range(len(test_data.index)):  # for each row in the dataset\n",
        "        if gaus_pred[index] == test_data[label].iloc[index]:  # predicted value and expected value is same or not\n",
        "            correct_predict += 1  # increase correct count\n",
        "        else:\n",
        "            wrong_predict += 1  # increase incorrect count\n",
        "  accuracy = correct_predict / (correct_predict + wrong_predict)  # calculating accuracy\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "sK09w28tqO_l"
      },
      "execution_count": 167,
      "outputs": []
    }
  ]
}