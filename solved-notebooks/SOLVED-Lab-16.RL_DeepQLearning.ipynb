{"cells":[{"cell_type":"markdown","source":["# Reinforcement Learning ep.2: Deep Q-Learning\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/GaiaSaveri/intro-to-ml/blob/main/solved-notebooks/SOLVED-Lab-16.RL_DeepQLearning.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>"],"metadata":{"id":"xo8vgwMKyAIP"},"id":"xo8vgwMKyAIP"},{"cell_type":"markdown","source":["*Goal: use Deep Q-Learning to play CartPole*\n","\n","## Introduction to Cartpole\n","\n","A pole is attached by an un-actuated joint to a cart, which moves along a 1-dim frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n","\n","\n","#### Action space\n","Breakout has the action space `Discrete(2)` with the table below lists the meaning of each action's meanings. \n","\n","- 1: Left\n","- 2: Right\n","\n","\n","#### Observations\n","By default, the environment returns the RGB image that is displayed to human players as an observation.\n","\n","#### Rewards\n","In this task, rewards are +1 for every incremental timestep. This means better performing scenarios will run for longer duration, accumulating larger return.\n","\n","#### Episode termination\n","* Pole Angle is more than 12 degrees\n","* Cart Position is more than 2.4 (center of the cart reaches the edge of the display)\n","* Episode length is greater than 500, or another value set by the variable *env.\\_max\\_episode\\_steps*\n","\n","#### State space\n","\n","At each game frame, the game engine provides information of the state. This state is then used to compute the features the agent will use. \n","\n","As starting state, all observations are assigned a uniformly random value in $(-0.05, 0.05)$.\n","\n","The CartPole task is designed so that the inputs to the agent are 4 real values representing the environment state `(cart position, cart velocity, pole position, pole velocity)`. We take these 4 inputs without any scaling and pass them through a small fully-connected network with 2 outputs, one for each action. \n","\n","\n","## Implementation\n","First, let's install the version of Gymnasium containing this environment.\n"],"metadata":{"id":"4NqOGKPh8kpR"},"id":"4NqOGKPh8kpR"},{"cell_type":"code","source":["%%capture\n","!pip install \"gymnasium[classic_control]\""],"metadata":{"id":"cjVwdcuLeMKq","executionInfo":{"status":"ok","timestamp":1684945363597,"user_tz":-120,"elapsed":10675,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"cjVwdcuLeMKq","execution_count":1,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","\n","from IPython import display\n","from IPython.display import clear_output\n","\n","import random\n","import math\n","from time import sleep\n","from collections import namedtuple, deque\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"metadata":{"id":"Q90tidOxcKw2","executionInfo":{"status":"ok","timestamp":1684945368053,"user_tz":-120,"elapsed":4464,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"Q90tidOxcKw2","execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["The environment is the following:"],"metadata":{"id":"P6AXjBTIcqls"},"id":"P6AXjBTIcqls"},{"cell_type":"code","source":["env = gym.make('CartPole-v1', render_mode='rgb_array').env\n","\n","env.reset()\n","img = env.render()\n","plt.axis('off')\n","plt.imshow(img)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":405},"id":"dTgrgZKc-oSi","executionInfo":{"status":"ok","timestamp":1684945368434,"user_tz":-120,"elapsed":385,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}},"outputId":"43bd578e-da19-4ec4-f7ed-f7763d180890"},"id":"dTgrgZKc-oSi","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f065e0de710>"]},"metadata":{},"execution_count":3},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["We can see the size of the State space and of the Action space:"],"metadata":{"id":"PSGdadwAcfMX"},"id":"PSGdadwAcfMX"},{"cell_type":"code","source":["print(\"Action Space: {}\".format(env.action_space))\n","print()\n","\n","print(\"State space: \", env.observation_space)\n","print()\n","\n","low_bounds, high_bounds = (env.observation_space.low, env.observation_space.high)\n","print(\"1st element:\\tPosition of the cart along the x-axis. Bounds: [%2.1f, %2.1f]\" %(low_bounds[0], high_bounds[0]))\n","print(\"2nd element:\\tCart velocity. Not bounded\")\n","print(\"3rd element:\\tPole angle. Bounds in radiants: [%2.1f, %2.1f]\" %(low_bounds[2], high_bounds[2]))\n","print(\"4th element:\\tPole velocity at its tip. Not bounded\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_74g6A9q_IeU","executionInfo":{"status":"ok","timestamp":1684945368435,"user_tz":-120,"elapsed":15,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}},"outputId":"749a27b2-3cb5-4efd-828e-964a2a48d3be"},"id":"_74g6A9q_IeU","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Action Space: Discrete(2)\n","\n","State space:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n","\n","1st element:\tPosition of the cart along the x-axis. Bounds: [-4.8, 4.8]\n","2nd element:\tCart velocity. Not bounded\n","3rd element:\tPole angle. Bounds in radiants: [-0.4, 0.4]\n","4th element:\tPole velocity at its tip. Not bounded\n"]}]},{"cell_type":"markdown","source":["## Trying a naive strategy\n","One can define a naive strategy (i.e. the action to take by knowing the current state) based on the physical intuition of the problem. \n","\n","Remember that the best performance is having an episode cumulative reward of 500, because after 500 steps the environment automatcally resets (see episode termination above).\n","Having a smaller reward means that the episode has ended before 500 steps because (1) the angle of the pole has become too large, (2) the cart is outside the boundaries."],"metadata":{"id":"J_8j4zOM-sn7"},"id":"J_8j4zOM-sn7"},{"cell_type":"code","source":["def my_bad_policy(state):\n","    \"\"\"\n","    If the pole angle is less than 0 (bent towards left) I apply a force towards left.\n","    \"\"\"\n","    if state[2] < 0:\n","        return 0\n","    else:\n","        return 1 \n","    \n","def random_policy(state):\n","    return env.action_space.sample()"],"metadata":{"id":"mmf3QDnf_81I","executionInfo":{"status":"ok","timestamp":1684945368435,"user_tz":-120,"elapsed":9,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"mmf3QDnf_81I","execution_count":5,"outputs":[]},{"cell_type":"code","source":["def run(env, n_episodes, strategy):\n","    \"\"\"\n","    Running the enviroment for a given number of episodes, according to a given strategy.\n","    It returns the average reward over all the episodes and the plotting frames for each episode.\n","    \"\"\"\n","    done = False\n","    average_reward = 0   # cumulative reward averaged over all the episodes\n","    episodeframes = []   # list to save frames for each episode\n","    \n","    for _ in range(n_episodes): # cycle over all the episodes\n","        \n","        frames = []            # save frames\n","        state = env.reset()[0] # episode initialization\n","        ep_reward = 0          # reward for the episode\n","        \n","        while not done: \n","            action = strategy(state) # Getting the action from the heuristic policy\n","            state, reward, done, truncated, info = env.step(action)\n","            ep_reward += reward\n","\n","            frames.append(env.render())\n","            \n","            if truncated: # Check if the state is terminal\n","                break\n","                \n","        average_reward += ep_reward / float(n_episodes) \n","        episodeframes.append(frames)\n","        \n","    return average_reward, episodeframes"],"metadata":{"id":"N827I2EB_GAt","executionInfo":{"status":"ok","timestamp":1684945368435,"user_tz":-120,"elapsed":8,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"N827I2EB_GAt","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Printing the average reward of the heuristic policy over some episodes:"],"metadata":{"id":"UYChV5UlA_Kh"},"id":"UYChV5UlA_Kh"},{"cell_type":"code","source":["n_episodes = 100\n","\n","badpolicy = run(env, n_episodes, my_bad_policy)\n","randompolicy = run(env, n_episodes, random_policy)\n","\n","print(badpolicy[0])\n","print(randompolicy[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-fmsRblx_MsB","executionInfo":{"status":"ok","timestamp":1684945369331,"user_tz":-120,"elapsed":904,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}},"outputId":"e14ed57a-a199-4d22-ef49-ba5e20fe2895"},"id":"-fmsRblx_MsB","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0.39\n","0.17\n"]}]},{"cell_type":"code","source":["def display_animation(frames):\n","    # turn off matplotlib plot in notebook\n","    plt.ioff()\n","\n","    fps = 5   # Set frames per seconds\n","    dpi = 300  # Set dots per inch\n","    interval = 100  # Interval between frames (in ms)\n","\n","    # Retrieve frames from experience buffer\n","    allframes = []\n","    for frame in frames:\n","        allframes.append(frame)\n","\n","    # Fix frame size\n","    plt.figure(figsize=(allframes[0].shape[1] / dpi, allframes[0].shape[0] / dpi), dpi=dpi)\n","    patch = plt.imshow(allframes[0])\n","    plt.axis('off')\n","\n","    # Generate animation\n","    def animate(i):\n","        patch.set_data(allframes[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(allframes), interval=interval)\n","\n","    video = anim.to_html5_video()\n","    html = display.HTML(video)\n","    return html"],"metadata":{"id":"e2DrgtiEBX5b","executionInfo":{"status":"ok","timestamp":1684945369332,"user_tz":-120,"elapsed":10,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"e2DrgtiEBX5b","execution_count":8,"outputs":[]},{"cell_type":"code","source":["html = display_animation(badpolicy[1][0])\n","display.display(html)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"id":"YOjp7kzsBYxk","executionInfo":{"status":"ok","timestamp":1684945373211,"user_tz":-120,"elapsed":3886,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}},"outputId":"fddce550-1190-453d-8f95-231d263a013a"},"id":"YOjp7kzsBYxk","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<video width=\"600\" height=\"400\" controls autoplay loop>\n","  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAAYjW1kYXQAAAKuBgX//6rcRem9\n","5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTU1IHIyOTE3IDBhODRkOTggLSBILjI2NC9NUEVHLTQg\n","QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE4IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n","eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n","MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n","PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n","b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9MyBsb29r\n","YWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFj\n","ZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJh\n","bWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdl\n","aWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MTAgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVz\n","aD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBx\n","cG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAHvZYiE\n","ABH//veIHzLLafk613IR560urR9Q7kZxXqS9/iAAAAMAAAMAAhx4K/IdYwN+m6AAADWgBpw0wnAn\n","4tRHx8jmRn0sk/mRE7ZgAPqkBo5O4fPoI/26f3nn5FT2GXF0tZ9tP3MvymQE0sTK8vL3q6tYvlRl\n","vNG6C4n2T3yrkEHCjcHuJI9UIgXALouNSJJHaCK52PKBBGKF8TWX2XTbWG5T+EZpNultH3m0YRZI\n","nRtgLhEw/Z5lSo/46C79wrturp5OipEBJl05Y0BJkEW+4dN5Vu2bDdbMQAxUH8ZpbwEmJMoh6P4s\n","Py0ueJvBp6puhGjTbNoqTKNDBTdjj2QX2dpXPP8ESkRy9s5E2TuyIay1PP7eqoDNzAXomE7sqRqm\n","qrxef/r7vA7ltpKe/atoXtap3aePrSKH7oSxBvHcvrZDsCsUqR+O5u4JADRobUX7u8EyIBLuQyNw\n","05O56qb/wzWhkCirR7C/+fgBKgk4Qsr18lLL+H8lHwjzdTWw4jgsB25f1UGJ+LoPMhsJwpusRhSj\n","0eNakde+CpNHgviZv61NfCMOhcHPCzURgKp9YQtnCE1uqT3H9rvof6tub9mTpoBQdpz1fZk3XE3c\n","+Y9QfZihxS9nW94xvYtjFIgYM80oGxmAQgAAAwAAAwAAAwGdAAAArkGaI2xBH/61KoAAAGzYRQvg\n","ArXfU6aaxB6qs2ttHzDlg31XbbzWY/nl4OTTGb+lDypt4ySpDNn2ifc5aD4wZnH+XKS57My+dv4L\n","kaoFiwbenvmXFevkJ9xEsblvTfPDV5x2z+9qOjpumL2q02Eq8ggOJzo8O1obEN/+zoq+EL9zzbzI\n","EE/JEDJyQmYiOri3XODAr2YA1QYcoW5JktI4Z6rbIXElk+4RQFQTLqdTQAAAAEhBnkF4hv8AAAMC\n","VGcU5IAcaAL81MfTyufTPzhmd1gBw2mUp6Zovt4W3Xn5kjTSjC7a7vneS5W0DMqjPkVBkDhmws2X\n","J2Qz6kEAAAA3AZ5iakN/AAADAlteuYhaTImDd/Y+amEAE7cEkYB5RtrR+WixROtqs3gCyHJxMuYM\n","adHoQpFSQAAAASVBmmVJqEFomUwU8Ef//rUqgAAAbPwuvwwAWDgTauxp9SZPO2gbAhGXnCFJs14u\n","5W2ibAT77EKOWNGKBW4po0xGbIzL2sQ/szyerQLn17rD9Fu4zSZrIuYzDsFVvugP5q1a+CZLq+v8\n","EZ+DZtuECa1NHNuTxjpviDLkAdp6qQIFEPEdR+evDOw96vwhLi8ZiptehLxMV2YSpguBBXDr5lFg\n","auSo7b62DUixJW7pIk6bQC4jkfD/1RSSSdpzupDbm/ZJrOQdj50QkRqHs+o3PHjTEsNtVShamx+u\n","XHktgXut1snNKbQOASaQ8hNpPi0awQD6Tl6HY6BAfNofzdswk5fQvMtlV1WvKOD7+L7t5FxJ1GOc\n","o9VPrVIKgAHxviSKpVqp1SbKpwAAAFcBnoRqQ38AAAMCW16Agm6Gmc+yZ8B9AB5gUlYoC7NTQQm0\n","x5JpWzR4UT6WeE5Obmykne7Hb1sLn79m0gAqHpekVU/9rUvQxt2WkCU3JJYeB4A/+OB6Yy8AAAEu\n","QZqJSeEKUmUwII///rUqgAAAbSuKVdsYAjoywzgCRFHy/uRYBe3eHV/8i67F+TFekMEjMv8SDLuw\n","5aWwX+kt3t7TxwS0RrI4qbl+uIVbxO9FZYGW3JS6y0kUmfnRVEaNm3oNqia9suUEQwDPmmY9nMhm\n","iAmJFkqIqhveuG6LTzRACf8RL24+wY83JhgfgFKQM5qjRv/ZKswEDFfhyZS5cLdrODQhtUbQlohG\n","lD8pqx15fJj2fg2umo/lbVvRiVG/MYZwCqLSzd09gJuweEoqlCT+tM4OCQLGpUjoiwf822W/Biyr\n","QeBC1d3ZMIXjuQSl+/vhWgqyH0rernmoOqORQgToh/IrD70s/d7XujGB2a2/TEKpDbAPHZOFt0sz\n","gVTn3FF0Q9NQc3WLzKfOfkEAAABaQZ6nRTRMO/8AAAMBqFwEH/JjV3CM5V6yhEu9BDK65X6rEf/M\n","f3R/+WOmJAvDFTR95ABtQI6/EosN7CAk7JE/NIzXZSB+eDXqs2Rl5d5ggmPKU3UUR15YngVNAAAA\n","YAGexnRDfwAAAwJaf3Cy4hjnuL53zX74XID7EJ13QxlhOMNNcaUeVAAIhIdDxchFEi1c+w8xfoZI\n","kGLUTeKaxsFjPsgE4VOlYKwAjOWvK+0GInHS+PXCtUlNcqL90G6DFgAAACUBnshqQ38AAAMCWxlK\n","x/hgpFfSr2kw4RrPi/AjwExQAopTJPqwAAAAt0GazUmoQWiZTAgj//61KoAAAG0hLpC9Zx3Y43p9\n","2h1qyIpKAxgy9bOXZwIN8e1v74lO2ZSD2eziJs5CAwWQA25MRVJMSTMBqfH/3iAG2crDigIewOpj\n","gyLpu/N6afkFdr7B3QJ5Xae1ROHSJzbOTTR8nk/C03dlHh8BidmCF5t82yI47+gDEhtoO+4RfTFO\n","V476gFVyD9UgN3jSKVqcLnRYBFJvewrEgRIyy/teyvLfK3kXxTN6gQAAACdBnutFESw7/wAAAwGe\n","2seDgfUEQbEjJYDbfyJd+wXZhsRdGupzB2UAAAAcAZ8KdEN/AAADAkr7J3tsmftCSq28vfytmoQX\n","5gAAADoBnwxqQ38AAAMA6BQmbecPmAM2JY4yFKALTMx6Kazd3OtrGx0Ujd/iAAFqK8udZTjtohdS\n","4/JEIx9XAAABFUGbEUmoQWyZTAgj//61KoAAAGyh52mARRJnzhewA4yTM+/j3fi4aRkTUPr1GqNp\n","OvJD5w4zFz+b4vinV+M+yvtg2npzRwF2ZfDyEGiYlCFtZRl1QmjFl7NImBqqE8prFn2MZy2jQLKi\n","p8LYDNBps3qxjrOSmjf1DsINupIvKLgvMvGpWUbSyZUaTxhCDrMsdqP8tofhLEv1YeAwbpQWanPJ\n","bxMOQwVg9FD6JL3uLeWOQdAVMeQF0ZG3UJS+HleB83IPitx/PQdDIQsckH3cdd/xQLX8Yofym0Vf\n","ZGSTs8+Zh/TFwnmTwndEnkFB61AAh7jPjmRrS+pd1YnPzf07rsJiBnybXoIIzRjoQtV6ULTROj0I\n","d7EAAABoQZ8vRRUsO/8AAAMBqByq8zdK3LKlvFk/X+S+CBeowAt6cwAkFpE6qE2opCMOb+z+Ulsv\n","du5KJ4pEaxA+Aszt9S1kOuOw2GJqvVBZFyNQCCMod3Cyt+bUmooXiVW5EczkSqktuo8TwcEAAABh\n","AZ9OdEN/AAADAlpklkgAXUpHLWqyY9k2aUkh4Jj64Aickc1a8PFH3jqluwuzZSbmegKI+hyHSepQ\n","wvN9B+Bhyaw1LI8d9oehX5ht9X+snknKtBAmxy5DJdMYSrDYAwdy4AAAAE0Bn1BqQ38AAAMCWNSu\n","iRW4AWaQXKxO47RxPGU0C80cnuXLSaxAaNFxpudHISePWlxAtmY9YB0XQXj67nCB1J9Do1nAeR9K\n","nTaap0/XFwAAAVlBm1VJqEFsmUwII//+tSqAAABtHP7WDMYAj4wEJC6wIVSjZPD6Zw6BojeQyu1+\n","shF9G2HH1dhxg/v7XyXj2SbuoV9Egk2AKHMWMrXUuTuGv6ID8dKOOU3YVwjH3Nw4Hw4TW1Q5LXWD\n","6nkXNOhIiDvss3TfgZaMgVikdIf6qZKkxkUKUuzugNWxVw86S5jbiaPQ0lGCKXXNHBrNYrrtizwI\n","Gr83X0wlJaGqfV6dtExW2cVw4WRg5Q3pCRJfvfGZS2CfDD8iC93Z4hmds6lt18omlltekKiK0SCo\n","Xrm6y3hX8vkGKudx4K5EDps/68q6Ft0sRA4BgMPQnPv/kTkRbSBDayM3Au36+132FaMChq5104MT\n","R49120m6/rmVhLXUkpnwkA3J9S2vsCO6pjJKV/FzaqogWZIeETmggoUjs0J/RV1qblSF8Al5fhbW\n","3ss0MKfVqPFYwGmmfEEAAACKQZ9zRRUsO/8AAAMBo8iEP4EDABdVL3bUov8xbysA1psmgUj6SANO\n","3k+YSmHOCJn75K1w6DGTfT5Y3hNgoSOSbvleTGJtUG8bx/sdM2ihJNDnEaeZJ4Ld1oOuSTKvsoGa\n","RQFZ/cHs6Io9KxdkbFXNtF8LFaqndTtpvFA9m+ZzkbF8Uha816t2BnZwAAAAYAGfknRDfwAAAwJT\n","RUWkjxtsIMpqGvfPFkK1HjCJMOiHhfhPy8XrPhLiS4MAJ3AFwo1iMntIem9Li+EWM+gwjM2MRPjk\n","E100tl2JumVprW70o/nQ1CpZ7ic/3s74AFvupAAAAG8Bn5RqQ38AAAMCU25sxNZIAS1UVROaK7Gw\n","2uRo196QTdnuT87HNRjwKBXpRrKCdBjtMZZh5i5ELy/WniP/E/X5VOFguFu9DQEYssToZe9vacwp\n","GPLgwiMUA+e55DPrIwt3QqdTEAvjo9G8t0lZ+MEAAACVQZuZSahBbJlMCCP//rUqgAAAbRuwwsYA\n","Rhz+7Gw5BoC+k2tQTPcfl2FWDPmJAiweXYe62EeGouLw/ESoDCaAi1KIlKqINZCW6j02s/eprHAU\n","ZtIWbOileP9ebQ1xEqEUwki1rMYqAjwJKzp0c1/XIgfzPf2BQGsmXF0ahD266TxHSH+7qm6AZnoj\n","IY0/pq28JrVF37YAAABhQZ+3RRUsO/8AAAMBqNq6zUKTWTsjvLlziV1KY2b+L3YgogXOAGWg4fAq\n","C/pHqvSzriFoZdfDl3ccFfy52mbMlmIzh5AVddw5k04+6mObU+6vmPs7/sK1+nOMOER93z6YuQAA\n","ACEBn9Z0Q38AAAMCWvscM8hLk22SG3/xOP+y3AFgPDJrc9MAAAAcAZ/YakN/AAADAN0XYvPS7dap\n","3StSPJLED5pfSAAAAOdBm91JqEFsmUwIIf/+qlUAAAMA2gYLAEH/MAIx/rc21fEKlqUgjHvQkkj+\n","qn9NvnPM2dKrnyN6zjns3z6O7eULpmZHPeBBfFOyofDDUx9MXAPDOJMpEusc2Anru6Ga6rc32wJk\n","yDP+lBcYswwfqpuzmRy0d9q8HmY/cE6n3naf4V44jTHLSIlG1O+Qpq5tg6jwhsV914eTB0p12bFb\n","UHupaF2zQ19rNkIzVTSP6um6cpkVV9KsH0uxjkf5QE8bdN5PyktN4wHDr3FV/BsmX6kzJezL6ZgN\n","kN1atf/eVA70BTmmrlCjFGUAAAB0QZ/7RRUsO/8AAAMBo8aSV4Ies4bpiWx66yFQks2mQQ+gapRs\n","jlJrXT1A9xuucpIa+yRrDoJGx216Vl3m8x61vkAGsvfZmfWTsqaCZ4+YMnuw9GDYTrrGI26p4mMG\n","ZADRO/q2ZfUDC0g+mjwmpTZuJF3eS6kAAABCAZ4adEN/AAADAlnaguBin/q5d0h+08PzUgP+s/DY\n","+F9Jl9X3AImSL+UkhgcmdJjQ5racvckVch1RBNm+nWePJtUnAAAAWwGeHGpDfwAAAwJafQzmlvy0\n","i2znXdiimYmlvbUQJcHS7NexV+dUcFAFsABbLjOsnRohBjTrYjoPNRVof1nMY9LEOTVTBSMrimVT\n","lOOKK6UITYYInMlCIuvFzi0AAADpQZofSahBbJlMFEwQ//6qVQAAAwDaItPc+YxoAONbMEmWvGCB\n","Q7MB9g+zqBriDqqfo8G5audtChC/giBYIh0VrjGGw/KF2fCe++90eGqS/IviJA7QLatZiNH31BK9\n","VUPkDT/SSAA6CTDwHQz+J3yTQwSIaDpwS8xxJivYNLpXQPm+2zJhfTmOzfHS1gwQVKHEKWaDFQf1\n","82MIAmp984WTtoCGIktEum9INZomQvGhUitj4ovaNCmVaIoBz4D0Fg/bOTNh2+S1JEVMnGHDjsPl\n","uWDelW4opo8Bouuw2kxPli6zeESi0KLQgqAAAABQAZ4+akN/AAADAljT9JyxcII46oG8r9slTivb\n","OHvck+RR1wqR0XNVaIicJF3h6JMwAhDaPVb9d7gYvclbXfPhXbFWfaAw4BRxjXC6O3wTCpgAAADP\n","QZogSeEKUmUwIIf//qpVAAADANoF4CtUwA2IxWMQvW/fCoOmQuie8mu1LjDvokPnO4wBi7a6IHMM\n","n5C2QYDh2GAafvz+z09OrW84lnb1qSeCFkudxWv0pyuIONvhjHFtGoJLE+QYrqiDnEX4j9NG/h10\n","cLIrmuyo+PJihRsCsGjTNtdovgYNduxHfSk7s2B1Tcttd6cr/O6DO9DKp1hAzblkmWIklNX2084u\n","ltspFd2mlW2QvzE2OqoLOFX26FcRb8oIZV0teEnUGWfam+OBAAABCUGaREnhDomUwIf//qmWAAAD\n","A1IK/NvZ1PABbfNQ0bWaoGKXHE13bplml7xp5Taw2P22gZPzk8zAHUUr2GgDqkSTpbeEwUg57ppo\n","wZmsYNubGBTADGiGBn402Lq1zxhpegEFaSBA+MuqfG99tthFawGWbQ1HKNCGr+xW+VFVXuk6AqQt\n","6T3iuSjapOb+Rzlf6WxkHVmLRljVmDU8ngJcCAzRMUVZTA3dw2YwLUVZO1S/wg247hH/SYGRg4Wk\n","CS/3KkmRPN0J5ihUl/mm7tyBvtqLzRIdpNAsEsFDeRXr8l/gyDM/fljkL8Sye2WTU2erDnXGhjTX\n","3KIFHrV/uiKXRDpPGGjAP7aybS8AAAC+QZ5iRRE8O/8AAAMBswP7ViJs5l0ykQ/8zQAcLXvn48vm\n","OsS0SrcL5Eziidlz9wyC7Ou14OWY88kXNkvs5xTy8M/lEw1PMj3v+MdEVrlYUlQZjvF83kgaVzQC\n","fTqro53XXsb14jxeSkoaLMiE2LYq/W2CjHXMBDI6hSZMVU3lwnloKbrkj+2b/RJaS1FSh4iCJ5CR\n","d9xV56Q172IXYxkKRSUioCy12ts5VqmmAjDmh4TNC4cPdGM4hQzy7ScicQAAAEYBnoF0Q38AAAMC\n","WL0UyHB2gI2FA4aIqZvCgxscbU5yLap3d0TjAU5QsZ3McNxYtyYAHC0CQK5/58XSF66cn8Mtxefu\n","pypgAAAAWwGeg2pDfwAAAwJrXh7RKix3rMJrs54J8U0a0tBHO0+AAsiX7l+uK5QFexiXL1UZu8gf\n","916oD6ARvc0vqOlUAG1T8t3NuKQkwlu6Z0YfJACvIzw60/9ZO2G/KmEAAACJQZqGSahBaJlMFPDf\n","/qeEAAAGnB7KjNIvUAJFHfR1I0I7xIiNtBtK7muS6PYPAv0a9PfCkzLYny/nrLx0AJk6yuCNuLmm\n","NnQci48TeQwtZ/IobJ1uzTPZvuqFVRn0RbsDE2mRMZYZfI1201nR29+aOT4ajZvJMJS7d6ZiDucH\n","8Yy200r7iRhFZREAAABdAZ6lakN/AAADAmuRXaOdOGCzmJqKMG2daFy+NVSZS6rJuZJGdAci2j0A\n","e8CZr0xkqWEl7OBwAath1h0gtNkbNySEMZ2c6C5nj3ku8OFGa9WtN4/3Iog04mIQkathAAAE+21v\n","b3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAAA88AAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAA\n","AAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAQl\n","dHJhawAAAFx0a2hkAAAAAwAAAAAAAAAAAAAAAQAAAAAAAA88AAAAAAAAAAAAAAAAAAAAAAABAAAA\n","AAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAJYAAABkAAAAAAAJGVkdHMAAAAcZWxzdAAA\n","AAAAAAABAAAPPAAACAAAAQAAAAADnW1kaWEAAAAgbWRoZAAAAAAAAAAAAAAAAAAAKAAAAJwAVcQA\n","AAAAAC1oZGxyAAAAAAAAAAB2aWRlAAAAAAAAAAAAAAAAVmlkZW9IYW5kbGVyAAAAA0htaW5mAAAA\n","FHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEAAAMI\n","c3RibAAAALRzdHNkAAAAAAAAAAEAAACkYXZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAJYAZAA\n","SAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAADJhdmND\n","AWQAFv/hABlnZAAWrNlAmDPl4QAAAwABAAADABQPFi2WAQAGaOvjyyLAAAAAHHV1aWRraEDyXyRP\n","xbo5pRvPAyPzAAAAAAAAABhzdHRzAAAAAAAAAAEAAAAnAAAEAAAAABRzdHNzAAAAAAAAAAEAAAAB\n","AAABQGN0dHMAAAAAAAAAJgAAAAEAAAgAAAAAAQAAEAAAAAACAAAEAAAAAAEAAAwAAAAAAQAABAAA\n","AAABAAAUAAAAAAEAAAgAAAAAAQAAAAAAAAABAAAEAAAAAAEAABQAAAAAAQAACAAAAAABAAAAAAAA\n","AAEAAAQAAAAAAQAAFAAAAAABAAAIAAAAAAEAAAAAAAAAAQAABAAAAAABAAAUAAAAAAEAAAgAAAAA\n","AQAAAAAAAAABAAAEAAAAAAEAABQAAAAAAQAACAAAAAABAAAAAAAAAAEAAAQAAAAAAQAAFAAAAAAB\n","AAAIAAAAAAEAAAAAAAAAAQAABAAAAAABAAAMAAAAAAEAAAQAAAAAAQAACAAAAAABAAAUAAAAAAEA\n","AAgAAAAAAQAAAAAAAAABAAAEAAAAAAEAAAwAAAAAAQAABAAAAAAcc3RzYwAAAAAAAAABAAAAAQAA\n","ACcAAAABAAAAsHN0c3oAAAAAAAAAAAAAACcAAASlAAAAsgAAAEwAAAA7AAABKQAAAFsAAAEyAAAA\n","XgAAAGQAAAApAAAAuwAAACsAAAAgAAAAPgAAARkAAABsAAAAZQAAAFEAAAFdAAAAjgAAAGQAAABz\n","AAAAmQAAAGUAAAAlAAAAIAAAAOsAAAB4AAAARgAAAF8AAADtAAAAVAAAANMAAAENAAAAwgAAAEoA\n","AABfAAAAjQAAAGEAAAAUc3RjbwAAAAAAAAABAAAALAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhk\n","bHIAAAAAAAAAAG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAA\n","AABMYXZmNTguMjkuMTAw\n","\">\n","  Your browser does not support the video tag.\n","</video>"]},"metadata":{}}]},{"cell_type":"markdown","source":["## Deep Q Learning\n","\n","The comparison between Q-Learning and Deep Q-Learning is illustrated below.\n","\n","<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/04/Screenshot-2019-04-16-at-5.46.01-PM.png\" alt= “” width=\"700\">\n","\n","Our aim will be to train a policy that tries to maximize the discounted, cumulative reward \n","$$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$$\n","where $R_{t_0}$ is also known as the *reward*.  \n","The discount, $\\gamma$, should be a constant between $0$ and $1$ that ensures the sum converges. A lower $\\gamma$ makes rewards from the uncertain far future less important for our agent than the ones in the near future, of which it can be fairly confident \n","about. It also encourages agents to collect reward closer in time than equivalent rewards that are temporally far away in the future.\n","\n","The main idea behind Q-learning is that if we had a function $Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n","us what our return would be, and if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards:\n","\n","\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n","\n","However, we don't know everything about the world, so we don't have access to $Q^*$. But, since **neural networks are universal function approximators**, we can simply create one and train it to resemble $Q^*$.\n","\n","For our training update rule, we'll use the fact that every $Q$ function for some policy obeys the Bellman equation:\n","\n","\\begin{align}Q^{\\pi}(s, a) = r + \\gamma \\, Q^{\\pi}(s', \\pi(s'))\\end{align}\n","\n","The difference between the two sides of the equality is known as the **temporal difference error**, $\\delta$:\n","\n","\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\, \\max_a' Q(s', a))\\end{align}\n","\n","To minimize this error, we will use the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss). The Huber loss acts\n","like the mean squared error when the error is small, but like the mean absolute error when the error is large - this makes it more robust to outliers when the estimates of $Q$ are very noisy. We calculate this over a batch of transitions, $B$, sampled from the replay memory:\n","\n","\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n","\n","\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n","     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n","     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n","   \\end{cases}\\end{align}\n","Our model will be a feed forward  neural network that takes in the difference between the current and previous screen patches. It has two outputs, representing $Q(s, \\mathrm{left})$ and $Q(s, \\mathrm{right})$ (where $s$ is the input to the network). In effect, the network is trying to predict the *expected return* of taking each action given the current input.\n","\n","\n","### Two networks\n","\n","In deep Q-learning, there are two main components: the policy network (also known as the Q-network or online network) and the target network. These two networks serve different purposes and play specific roles in the learning process.\n","\n","1. Policy Network:\n","   - The policy network is responsible for approximating the action-value function (Q-function) in Q-learning.\n","   - It takes the current state as input and outputs the estimated Q-values for all possible actions.\n","   - The policy network is updated during training by minimising the difference between the predicted Q-values and the target Q-values.\n","   - The updates to the policy network are driven by the temporal difference (TD) error.\n","   - The policy network is updated after each step or episode to continually improve its Q-value predictions.\n","\n","2. Target Network:\n","   - The target network is a separate copy of the policy network that is used as a stable target for estimating the target Q-values during training.\n","   - The target network's parameters are not updated during the training process. Instead, they are updated periodically with a [soft update](https://arxiv.org/pdf/1509.02971.pdf) controlled by the hyperparameter $\\tau$. This is method of gradually updating the parameters of the target network from the parameters of the policy network by blending the parameters of the target network with the parameters of the policy network using a weighted average.\n","   - The purpose of the target network is to provide a more stable and reliable target for training the policy network.\n","   - By using a separate target network, it helps to mitigate the issue of the Q-value estimates being influenced by their own updates, which can lead to instability or divergence during training."],"metadata":{"id":"jz_42J0xBCPa"},"id":"jz_42J0xBCPa"},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"bK06wpbsBJhO","executionInfo":{"status":"ok","timestamp":1684945373618,"user_tz":-120,"elapsed":413,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"bK06wpbsBJhO","execution_count":10,"outputs":[]},{"cell_type":"code","source":["class DQN(nn.Module):\n","\n","    def __init__(self, n_observations, n_actions):\n","        super(DQN, self).__init__()\n","        self.layer1 = nn.Linear(n_observations, 128)\n","        self.layer2 = nn.Linear(128, 128)\n","        self.layer3 = nn.Linear(128, n_actions)\n","\n","    def forward(self, x):\n","        x = F.relu(self.layer1(x))\n","        x = F.relu(self.layer2(x))\n","        return self.layer3(x)"],"metadata":{"id":"Rz5OUMd5Famn","executionInfo":{"status":"ok","timestamp":1684945373619,"user_tz":-120,"elapsed":9,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"Rz5OUMd5Famn","execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Replay Memory\n","\n","We'll be using experience replay memory for training our DQN. It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure.\n","\n","For this, we're going to need two classes:\n","-  ``Transition`` - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.\n","-  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a ``.sample()``  method for selecting a random batch of transitions for training.\n","\n"],"metadata":{"id":"vdanLewyEBt0"},"id":"vdanLewyEBt0"},{"cell_type":"code","source":["Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n","\n","\n","class ReplayMemory(object):\n","\n","    def __init__(self, capacity):\n","        self.memory = deque([], maxlen=capacity)\n","\n","    def push(self, *args):\n","        \"\"\"Save a transition\"\"\"\n","        self.memory.append(Transition(*args))\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"metadata":{"id":"QqbE8jXvD7m8","executionInfo":{"status":"ok","timestamp":1684945373619,"user_tz":-120,"elapsed":8,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"QqbE8jXvD7m8","execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### Training\n","\n","#### Hyperparameters and utilities\n","This cell instantiates our model and its optimizer, and defines the policy:\n","\n","-  ``dqn_policy`` - will select an action accordingly to an epsilon greedy policy. Simply put, we'll sometimes use our model for choosing the action, and sometimes we'll just sample one uniformly. The\n","probability of choosing a random action will start at ``EPS_START`` and will decay exponentially towards ``EPS_END``. ``EPS_DECAY`` controls the rate of the decay."],"metadata":{"id":"WcykUQdGFc3e"},"id":"WcykUQdGFc3e"},{"cell_type":"code","source":["BATCH_SIZE = 128  # number of transitions sampled from the replay buffer\n","GAMMA = 0.99      # discount factor\n","EPS_START = 0.9   # starting value of epsilon\n","EPS_END = 0.05    # final value of epsilon\n","EPS_DECAY = 1000  # rate of exponential decay of epsilon, higher means a slower decay\n","TAU = 0.005       # update rate of the target network\n","LR = 1e-4         # learning rate of the optimizer\n","\n","# Get number of actions from gym action space\n","n_actions = env.action_space.n\n","# Get the number of state observations\n","state, info = env.reset()\n","n_observations = len(state)\n","\n","policy_net = DQN(n_observations, n_actions).to(device)\n","target_net = DQN(n_observations, n_actions).to(device)\n","\n","target_net.load_state_dict(policy_net.state_dict())\n","\n","optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n","memory = ReplayMemory(10000)\n","\n","steps_done = 0\n","\n","def dqn_policy(state):\n","    global steps_done\n","    sample = random.random()\n","    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n","        math.exp(-1. * steps_done / EPS_DECAY)\n","    steps_done += 1\n","    if sample > eps_threshold:\n","        with torch.no_grad():\n","            # t.max(1) will return the largest column value of each row.\n","            # second column on max result is index of where max element was\n","            # found, so we pick action with the larger expected reward.\n","            return policy_net(state).max(1)[1].view(1, 1)\n","    else:\n","        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"],"metadata":{"id":"AlSBLMHVFnKG","executionInfo":{"status":"ok","timestamp":1684945380045,"user_tz":-120,"elapsed":6432,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"AlSBLMHVFnKG","execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["#### Training loop\n","\n","Finally, the code for training our model.\n","\n","Here, we can find an ``optimize_model`` function that performs a single step of the optimization. It first samples a batch, concatenates all the tensors into a single one, computes $Q(s_t, a_t)$ and $V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our loss.  \n","By definition we set $V(s) = 0$ if $s$ is a terminal state.\n"],"metadata":{"id":"qrcn0Jt8F8Ig"},"id":"qrcn0Jt8F8Ig"},{"cell_type":"code","source":["def optimize_model():\n","    if len(memory) < BATCH_SIZE:\n","        return\n","\n","    transitions = memory.sample(BATCH_SIZE)\n","    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for detailed explanation). \n","    # This converts batch-array of Transitions to Transition of batch-arrays.\n","    batch = Transition(*zip(*transitions))\n","\n","    # Compute a mask of non-final states and concatenate the batch elements\n","    # (a final state would've been the one after which simulation ended)\n","    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n","    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n","\n","    state_batch = torch.cat(batch.state)\n","    action_batch = torch.cat(batch.action)\n","    reward_batch = torch.cat(batch.reward)\n","\n","    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken. \n","    # These are the actions which would've been taken for each batch state according to policy_net\n","    state_action_values = policy_net(state_batch).gather(1, action_batch)\n","\n","    # Compute V(s_{t+1}) for all next states.\n","    # Expected values of actions for non_final_next_states are computed based\n","    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n","    # This is merged based on the mask, such that we'll have either the expected\n","    # state value or 0 in case the state was final.\n","    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n","\n","    with torch.no_grad():\n","        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n","    # Compute the expected Q values\n","    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n","\n","    # Compute Huber loss\n","    criterion = nn.SmoothL1Loss()\n","    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n","\n","    # Optimize the model\n","    optimizer.zero_grad()\n","    loss.backward()\n","    # In-place gradient clipping\n","    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n","    optimizer.step()"],"metadata":{"id":"AEBLs9g0GB0y","executionInfo":{"status":"ok","timestamp":1684945380046,"user_tz":-120,"elapsed":10,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"AEBLs9g0GB0y","execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Below, you can find the main training loop. At the beginning we reset\n","the environment and obtain the initial ``state`` Tensor. Then, we sample\n","an action, execute it, observe the next state and the reward (always\n","1), and optimize our model once. When the episode ends (our model\n","fails), we restart the loop.\n","\n","Below, `num_episodes` is set to a higher number if a GPU is available, otherwise less \n","episodes are scheduled so training does not take too long."],"metadata":{"id":"C4El6zdOGHvZ"},"id":"C4El6zdOGHvZ"},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    num_episodes = 200\n","else:\n","    num_episodes = 20\n","\n","\n","for _ in range(num_episodes):\n","    # Initialize the environment and get it's state\n","    state, info = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n","\n","    for t in count():\n","        action = dqn_policy(state)\n","        observation, reward, terminated, truncated, info = env.step(action.item())\n","        reward = torch.tensor([reward], device=device)\n","        done = terminated or truncated\n","\n","        if terminated:\n","            next_state = None\n","        else:\n","            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n","\n","        # Store the transition in memory\n","        memory.push(state, action, next_state, reward)\n","\n","        # Move to the next state\n","        state = next_state\n","\n","        # Perform one step of the optimization (on the policy network)\n","        optimize_model()\n","\n","        # Soft update of the target network's weights\n","        target_net_state_dict = target_net.state_dict()\n","        policy_net_state_dict = policy_net.state_dict()\n","\n","        for key in policy_net_state_dict:\n","            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n","        target_net.load_state_dict(target_net_state_dict)\n","\n","        if done:\n","            break"],"metadata":{"id":"GAx5gXXTGISw","executionInfo":{"status":"ok","timestamp":1684945406035,"user_tz":-120,"elapsed":25995,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"GAx5gXXTGISw","execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### Testing\n","\n","Now we're ready to see what our agent has learnt."],"metadata":{"id":"jfX63aZ6IawB"},"id":"jfX63aZ6IawB"},{"cell_type":"code","source":["def run_dqn(env, n_episodes, strategy = dqn_policy):\n","    \"\"\"\n","    Running the enviroment for a given number of episodes, according to dqn strategy.\n","    It returns the average reward over all the episodes.\n","    \"\"\"\n","    average_reward = 0 # Cumulative reward averaged over all the episodes\n","    episodeframes = []\n","    \n","    for _ in range(n_episodes): # Cycle over all the episodes\n","        \n","        frames = []\n","        state = env.reset()[0] # Episode initialization\n","        ep_reward = 0\n","        \n","        while True: # Cycle over the steps\n","            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n","            action = strategy(state) # Getting the action from the heuristic policy\n","            state, reward, done, truncated, info = env.step(action.item()) # Environmental step\n","            ep_reward += reward\n","\n","            frames.append(env.render())\n","            \n","            if done: # Check if the state is terminal\n","                break\n","                \n","        average_reward += ep_reward / float(n_episodes)\n","        episodeframes.append(frames)\n","        \n","    return average_reward, episodeframes"],"metadata":{"id":"w7rM_GXfKHfH","executionInfo":{"status":"ok","timestamp":1684945406036,"user_tz":-120,"elapsed":10,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"w7rM_GXfKHfH","execution_count":16,"outputs":[]},{"cell_type":"code","source":["n_episodes = 50 # same as eariler\n","\n","dqnpolicy = run_dqn(env, n_episodes)\n","\n","print(dqnpolicy[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71nprMsKIe3A","executionInfo":{"status":"ok","timestamp":1684945425256,"user_tz":-120,"elapsed":19227,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}},"outputId":"0737c199-c856-4933-e190-a0cc5aebe7a0"},"id":"71nprMsKIe3A","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["77.87999999999998\n"]}]},{"cell_type":"code","source":["html = display_animation(dqnpolicy[1][0])\n","display.display(html)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"id":"dRmksbQSJzKd","executionInfo":{"status":"ok","timestamp":1684945430174,"user_tz":-120,"elapsed":4921,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}},"outputId":"66840fdb-fbbc-4a5a-c656-2002ac0e2351"},"id":"dRmksbQSJzKd","execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<video width=\"600\" height=\"400\" controls autoplay loop>\n","  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAAmNW1kYXQAAAKuBgX//6rcRem9\n","5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTU1IHIyOTE3IDBhODRkOTggLSBILjI2NC9NUEVHLTQg\n","QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE4IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n","eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n","MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n","PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n","b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9MyBsb29r\n","YWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFj\n","ZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJh\n","bWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdl\n","aWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MTAgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVz\n","aD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBx\n","cG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAIHZYiE\n","ABH//veIHzLLafk613IR560urR9Q7kZxXqS9/iAAAAMAAAMAAhx4K/IdYwN+m6AAADWgBpw0wnAn\n","4tRHx8jhHdQBsENrtjK8Cz03OzifuuK7M7LQ/bQfHRyezDmtPvbKqhReomr8lh6HzGrCWtcglef/\n","ZVYoP5kETCCMY4F/klsHmOV7cxeI/54LkhUS3fKNaVWdww1Fn6/1vfRaHbTwjgHHNY2YDCjErJzp\n","I/o52rqnCXzGI2KrZ8V3jyrY0YYBOxkcZujPSBzcZqVRbceFzrMvJGrNtGokNVh2n3tuWOo7xXf6\n","jM0uRnOzOLZyzczsQ4dN738EXqHCASOsbxLfcCOAnkIa+4U0lCs3zIpPYIvI+b6+cbQCDNjg6R16\n","c6RSP4moCkQdVExkfNAQF//DWbZJ5dgFW3XeKUipwh/VB1VbkHhPMGt/UgCLrumsPQY38xFjxsOb\n","YRFyHq/Hz5CbreW2xj9CFkeICjRCdXcHZW7fVCZEY5ZWjjVii6G6d8E3Kh6IYiuFsivltwJKbYGJ\n","3/7fZlF3l6sPz5KxDBG2dTO90QkzYIUQeLJGI4IYYw0d/fhf9QYXBPd7i0RUxYuXJ68HlepNqHNi\n","vrCACuHleYwBqtEoAr/K3dgsk0fvpBq//vhtp6PzB5+81PS0YcNCmaUiBqrPUidABzugAAADAAAD\n","AAbNAAABDEGaJGxBH/61KoAAAGyg8hgONgCO/9QtfvvnxC3i+7fMqUQYj1ySEpP3L6wGUnirATRi\n","Rje0LZqFwoDf+gBh6Q9cwfaUc8CaNSgONBojosjvJXOQsZyjiWOJR1FJ9X4I4rOMABIhcu0oDEZR\n","Y33Hu+H9cAn6xiQrT54deIRbJmrM7fpsVQzNeAM6YoaaySLqk1+SZoZwp/16YV1AvKG10DcDRomz\n","rZc+BySLXXajP1mWhFoxbS8qVEL5/im+l3gidzUQhekEIxY/FE7qCGqFWeSXcSJZrCwxO7B79BMp\n","hyOBsfO9sDH+absGkWsTL61hZYL817HjPykaE1tEtZZflVt+fhks2vRsOb4wOnAAAAA/QZ5CeId/\n","AAADAaimnnomwAttfydVA+u8gKPYY/RWFo1q6d0sQ3DBtPqpaa5ORbEbHwZ3TFGt2aGtXnX15kpP\n","AAAAJAGeYXRDfwAAAwJbePPd0BR1MF9fAfJ/i0AHC2DR6f5S+VoKkgAAACcBnmNqQ38AAAMCW163\n","dvqpNn21rpgHzRfBuMNIUggWMDbeNF/OovEAAAErQZpoSahBaJlMCCP//rUqgAAAbKF0XQc9gCOx\n","aZrxPzcefTflQmS6KBAwhNwQNtXJ2yevBLNQXB1iqVrrf/xFlw7e4zLaaTrLxEcfhGBogOxfQvso\n","0IhBNtp9f1Y63C9PQy1xBtNZ2GDfpOdCIjkqF9bFyRewztNh9msYgBPNG44LBX612aJaxs6HCxza\n","KeeRiZRieEAggCx9HvSqj92q6SEePV8t5j/5JuchU7JwcXb6AbFn/7CATPn560NkDTvFCm+AOB8f\n","NDVnPzBVMWkbfLhpzVNYvPpbzKDAOipIKJXsxUh20JvW7objzJ8fhDow96/LH5qK3LOLxt9LBSNh\n","AqLCunZxZhhV+XT4ldv5D1fFe1QphKaM64drAN8UeqXIl30WubCmst2e0SMAAABhQZ6GRREsO/8A\n","AAMBqHjK7wyorUWcTH41n+rrxnveABImqzUbDNkU4XMRfC4BprSd9C9sl/KrvlW1z9hEIu0CvpXK\n","XR15jU53RVRUh6JGmUHH1A1ExDdWnD8grXD8V+8XpwAAADQBnqV0Q38AAAMCW3jzOCpr+L4WjvnU\n","gqyVKEKePM2D+L6sCLJdIlxH/iPBfw1y3qPd/scfAAAAHwGep2pDfwAAAwDilDyrLIewgqqgpx6n\n","Tb72lqO9WhAAAAEQQZqsSahBbJlMCCP//rUqgAAAbKOzZxHPXIsABWoeYdJXU1wNCJdgNNOIjCoN\n","muAAvL9jxEdyvp1VUAlCiv8OMh5MTYfUxeMUnPRKN18V9JaTbesY+MGBlC2ZOB0HIpkfsSEsnswD\n","6fNG5uJliXR+g8y5jsohpDAZkXfqle+V/oI2TDNklTH3WC8BaI6NkkQR++woehZYy1cDXthC1I3f\n","/CxUvppCxPtr3OzzUzblvhU5nogQunczmGj7GFi5MyHB6L0YGB0BEV7k9dXjBMFyH3aXL0PirgzV\n","PGaxmvDKvt757BUKXPenOKXACqujpMUOaHNOJZOrdg+GrxCSuDHntHeA/tC1QCT0W2827vWEAcAA\n","AAA+QZ7KRRUsO/8AAAMBqM+04io+rkhPAoWX9J3hBj8i4GNJfa3dXxgTInWvUs2cmrdzvs4VQAQ4\n","pVZMSGS8x9MAAAAwAZ7pdEN/AAADAljJIAilg5jZtHiZHix5xSE4Ff5yPN7PIiVtV956a/i9ksxA\n","FdPTAAAAHwGe62pDfwAAAwJbGbkxQ38x2VWvLXjXQKnYtEF85NwAAABdQZrvSahBbJlMCCP//rUq\n","gAAAbR1cC7hwgCHD2oKOPr954WIuTaRDP9XMtTXKF3n+ogUDBTEqxm5Xk5cMCQH6pMYHS6jBm3Kg\n","bZWm4eB2Cj9d5b1diWCqUppPoJeBAAAARUGfDUUVLDf/AAADAlsZXvXH6aCyA72r9z7nmXVAQgDI\n","H8MV4P5xYcwvMOwATZoIgwnP526j+HizPxbg4poYPQD3TQCm4QAAABsBny5qQ38AAAMA6BQgRF8B\n","xvbU3MioIm+0d0EAAADuQZszSahBbJlMCCP//rUqgAAAbKKFRiIJOgA4577vPQH7AqCnAxaEMQi3\n","E3ZCfaKUnSg+PKA7v6eqp6Oe3qTCOd7lGjzrmxj1KwK/kl+85o1PXFg63UU/B5ora0r3t8vS3qVg\n","/2gybDAUbQZKQwHGo+e9CtFB2S9jJShudTQL017gC9C1Sjxkf63rT/FJV/zogUznL+vum5U+VvZr\n","QHtdwCGoseS2fCBOBzGH2NcISmlltBhhu/tR0/W72eEiKqJJ4CQFrEGv2HzHN0PS5AzZVO8vUuPO\n","R2NbXxJqxxHS7aOsJc4dtAvqrScjo2IlDQAAAGRBn1FFFSw7/wAAAwGoSyOSgLKaBexluNaKLeoj\n","/zL4Y5qcb0wtY/fWsjIOSedgkp3tqFNW6jcsEdgNBhOC+CWK3w1R50OJoaiLyujVuJF01v20j2WI\n","EF98DW4x/rrUISyT7saAAAAAQAGfcHRDfwAAAwDn6cYn8pPoZ0OOIeFdG7TKPAcFMsuBdXxTLZCA\n","BEHBMbM6syhQbDOMvy1VEIc6UwtlNe0O6kEAAABGAZ9yakN/AAADAlp9G39sJninluXW8HnFe/c1\n","sV/oMhX0pOIkSPgy5VnxehIAQqxDeEgyErpDjPZ1BDYw219KPyZwFb6TgAAAAMxBm3ZJqEFsmUwI\n","I//+tSqAAABs5fLaJgAIlg2n1dgaQjNBi41swvuY7uSB/z2t0UsgQrMspMQGljA3PDTSkuTtNau1\n","0S+WGjwLLK+UPUl80qOHuXnglYo7L87T6gV9IqDDbkPfOPH8z9SnsmZK3/uJWaH9ctqtWj8xBHyP\n","SvxrO4eozGiMpTePT9mipof/h8znKbuiwTDGpgKoDNKcCM7nkr3tbHpJWbAEEeIYB86Th+8yGoMG\n","An7Jh+dvWpW2zcSESdufK/TcRr1ni5gAAAAzQZ+URRUsN/8AAAMCWqjiinTZciLQt0M5V6mcj/+0\n","GJ1KzfTcI1AQYwG3KOptGBlQsEpJAAAANQGftWpDfwAAAwJdFUACdlnPZAo4Jd55pfF0tJzZYWuo\n","2E3EqICS8JVLYUs4TYdE3MSYHSk4AAAAz0GbukmoQWyZTAgh//6qVQAAAwDZhCqV7UWAC/I9SYeJ\n","A/91QCLxTsaHRXTDkG69PZfIujQAMFtJq3fAjPQZfoDqbTqaSoy126NIuExNAB5SH4GDB3c/Qfnn\n","u8Ar2KoYjrExe0u9Y63nKhgfE6ZTa74+2spogVdacQ971LA7oKHEatzQy6V4g39JImsPm+iZkrCR\n","rBDUnYC5I0c6uaVgymOp4XmF1M4S/ZUdtG22SCm68tIkAz58t+YaDsed+KWqCnUhpFU6QUwXQ3sY\n","OxmpdwAAAE9Bn9hFFSw7/wAAAwGoRzOJo6AC2J58ltl6SGR/8RhXFgtkVV7DIe650vt2b2SemnHL\n","4Ew/JZoT3De+SKKg5O//2sZ4LJpdG+0HLg+YbeOBAAAAIgGf93RDfwAAAwJa+ybkQGMEu1rpBbW4\n","bc8kWC1xVUIi3jgAAAAxAZ/5akN/AAADAltXB1QAfztY+fpj8D1Ly+KR+TMyEh/nf2meDgurLY6p\n","dVINWbFJeQAAALFBm/tJqEFsmUwII//+tSqAAABs5a3wZ2uywCCcLa9dm4XR7Kr4gGaQkE7MXQZK\n","kNE6+7ze+btR2T7f9uJPjncxspzNn++dEIcU90fnpMQ3smjrHUXjDrVCzq3OsZ5UJTz6AwxAWjDC\n","x7dNaYEH645h1UXnnGIfoL0kgv0wFcST9t0THMHj+suiytHrIvsce+5eYm14vyyVNiRNEZSEoslQ\n","AwdzoDemYVJcfwf65rpS4mAAAADhQZofSeEKUmUwII///rUqgAAAbKWVs68ZwayAAqNXtElVvf+E\n","BnNk0mPyf4jXJReszwZ3/64getPldepHVFyUlfo3W/YyMfaNaUlvgAhCplbvl8ZaXdAsnHd/I9vB\n","6hFPksBR0/zAHLbAJHZSUZlmy1FAJgQOt7i/DcjqMoPFxxNoU+MMt2fS+fDHOA8+C4vo21RaBt/5\n","URXUn6xyUDDo1VYbxurA1LuhDCmuzCSWuKyyS5ouo6r9jdo7acGSt0+98DP+bKFh4FRvUD1+J3G1\n","lDynrUWFX14+1z5/ilhs1rnhAAAARkGePUU0TDv/AAADAakhOL203c8igTdTX7C2JgA/oc9KDRKu\n","gsYU3x4G3r2KlJ5YW5RkE0z2bTelyIeFkF2aw1c/qOMb69MAAAAqAZ5cdEN/AAADAloIuBeskKyJ\n","+rljaCtOr5P0lXsSO8mnw/HoXrwe9STgAAAAPwGeXmpDfwAAAwJbXmsOy+S0TopCvUzcUL4TKqfD\n","MSvD8be+6V8Rp8DhWbMoO79UADna9Gt+2Z9NeuRB8C9gPAAAAQlBmkNJqEFomUwII//+tSqAAABt\n","HAAYcG0AImaP/AHLXFJqnxtAu9Neu7G2bfgWij7vySyUS2f5i0Gstd7/YcO/Yv/9V1nID415bu3n\n","23Lq8Ip/0A7U5LViyQvH652XQ6yRYXAtFG044fnvObSi76HepE/oz07Fgc4zxnEezy7poFWH9lXq\n","TRM/MPAenwY9BNYrSENGrJ4ZgEk05sH+3UhcyySqkyY/fkoqST/CWhLmTwNauDQ33aJ8nuvFosJg\n","9V/trPN8MC2V5Lpw1l5XM0uj72pQ1I5P6WHB3g1NlGLs5oZSxKkdkACVUVapMLMGxBrDXn9f3aXC\n","bmtRHI1iBa0C9JeUxbmsJIB9AAAAU0GeYUURLDv/AAADAakhOLsbl/4ABuSTl0KxyJgKgLPnhqHI\n","K8T8Lqz8x4a8x3VQkuN5pg8df1C7x21wpuFlu5t788a/pd3RCbLy3nlwsqgYmlTAAAAARQGegHRD\n","fwAAAwJbacpfKNLSRJwNUvRN08cj5ydHgcWxbLUAH89SdEqsHIbByATiabYMnZdnTEvifKyz1EOe\n","jS/+tBtNwQAAAEsBnoJqQ38AAAMCWwgq73PsU1ejnL7OPUoSHnTyWUZqQBYXQogBu0A5Nb462wVH\n","wztqXyi/vO3QGYFCSaDCGzfbCBW2gtvu/YtEJvQAAAD9QZqFSahBbJlMFEwR//61KoAAAG0czxK6\n","vAFAg2npwNR4TPBL8uaffpG4FcOzvvcZER7I0Qu2wlSlij7qiVs+3h/OnNnbg9mNu6/IniTM7jQQ\n","NJfIlTy2f+p9X+Y6n/i0md8JHh5w+2sXEQ+rSc1xFR3p0sxDJLwLWsYkSUG99k+A+Xf1Wal+McFn\n","zhfRcoBy+eocQiDJ1TEeXzXmH+jrr7iHNo9jkRuCXGpQ2ScVq8mg/oc9Ohy1LRBjtuaIy+0H9o2u\n","Itg0v8NE6Ydj5H5FrQ5+8vmS+NzGlJjwlfAcqw18guzxdICGYsoveQlX3If+dYka58POFbJnz0UO\n","UQAAAC0BnqRqQ38AAAMCW1cQ2D9JLjp5xUAja86mGzoVi+4aGx151OJxzb+aoNLHVMEAAADtQZqp\n","SeEKUmUwII///rUqgAAAbTkPjZFmpgnf4YJ69zayE1NWpyQArehNLxF/TJbLr7zJ4jVbpGFoaEVw\n","XtTyu7Va5Dwd5Na+zK6iLTm8hb6Bpx4jj1mpD5b/ngPRrXjQ9vh8ndpyiug65xJTo/KaG9mdx3EQ\n","bQODSBdWZWdO/+89bfWy0ukrWnC994B4s7fZEw14ukHFSn6EwIKMz2qFaD0ps7COluUqEvJbY0P2\n","rYwNS/6Smpce1dfASQqWLy4sWklu5plLz8A9cjwONIJ4FhMyqNFnk2lfDBd3TWo2Pu3YP5pVWsJa\n","rrPiBveBAAAAXEGex0U0TDv/AAADAajaQAPYzSDiIjOXWxvPmEPCgA+9zu7SzEpDGDxNqfYQGXCm\n","CI4fO9XcSQw0OP6+k7Ly6SHfpX4C7nFO2W6L2FtTuySN+2BiMom1v8+aABhPAAAANwGe5nRDfwAA\n","AwJbabdAAk9cyVV2GGEW+DDAVA0H3Hw77Knji6hRlb8RTqEYwMW+igPEaf0+DqwAAAA2AZ7oakN/\n","AAADAksIMY3e5PEQ/91gzNCFogAE36Ml4V047YrKp4Jn238vkI9evh0XSp3nNPdAAAABBEGa7Umo\n","QWiZTAgj//61KoAAAGojfF5L6042R0AETXFP79zUF4OaGKVYlVm7Frn6v4/e5M8DpIx8xh1pU+is\n","C9wFARyykUS4fY+X0hj8iEP2aAzdJFVzrEvKVDydXkOjGdvnNQFpt3fQLzssST/BMNTqZFT4Dksy\n","iDy3PVvT6OfLSBCEsg98Mu5qfhFCv/KJboLf8O9FONwhOWD37QTQTXKdO2ZxeWeBCmMGwKfVt1Vn\n","xiiTXv7qN7MTpWy1D1CQL3qm0x4/WK15wG+2B4djLadZbGv1m0fmvil4hMh97YdSEUC3YeGjVHgE\n","bF05ZDPyl0f1EjCp7SKlNl3ZjxirJpYyEAadAAAAfUGfC0URLDv/AAADAZ6i82NEKi90kKc1ldHw\n","jGyz8VJ7QElUTxxF+naB81ZF9km+AaFpaoHwVEwI01l3mQxZO7YzPROV2+VQQov/ZHm3pAmF1OF/\n","ike1kJWg+gTxVPaWKyvJMmJE/M8SFmc8aNCWQfyzLq/Txml80J0HQe7MAAAALAGfKnRDfwAAAwJD\n","8HMGhpEsdFlxH/Sn96MWCD+lIMpTVhAZ75mRLoqF91TAAAAAUgGfLGpDfwAAAwJLjFYZwyktQoOQ\n","5tBXxhGbFm4Bwee1WRfvPMEoAPniWQWCdU1ytJ8Unx290F63eTISjBp6nuejC0n1FpxgMbH/erSq\n","ju3TL3UAAADpQZsxSahBbJlMCCP//rUqgAAAaqSf6tM8kAD4A13UXB9hoH3kbJR+dCRifLEoCVqV\n","f1vK2IuzmeC3XNRCUuzp5vMmfHm8OtoBGgS+aPDS6zBE2tnaX1CTblg9hHvYtdVh4ykJT1/UHxJy\n","aGGUZ/gh2dpU87Rxq8fF7PIb/LeIzm9YgGkwfvwjX55uQbZTNMaF+EnibVFdA28laB8uQohlC1Yo\n","N3Ocfc1ubmJANkyfP4DkdM3obYZJD1tF9zt4HLwfdHJpFkA6CoTDP1LNmg0ucl2HLNLZDv6gt5m5\n","FBHoWaYH9oAPkfWVb9UAAAByQZ9PRRUsO/8AAAMBnl8ORuBrLOC7ZPc3QAruvRSLgKPrC1Sx0PXb\n","yn9jh9FkbyppC5JoJEymjwzbZR/xMUHrR01/JYnu0WQBRIGXWnmCI/q/dOHKGeqeCNpv8FXEo58v\n","o8L90/Kn2e+Soq9nvpfFw5OBAAAARQGfbnRDfwAAAwJLMN8SAAXS+SDLOYubApOmdLQXjLOsfqPu\n","mkEVvtr60CioJjqJl19JMvfSof33A3o9rgjCUt+4RHLvwAAAAEkBn3BqQ38AAAMCSwgsGIVS6EQW\n","gbHDEAJqdfNRF+nVb05P35krDkt5czG3Z1xpZBcJAUPAAH3x3i/iVwDXRnD5N6LnzFdnGo3AAAAA\n","00GbdUmoQWyZTAgj//61KoAAAGq5D44h0M9AQ4AboeiMXXySfaQ6y2GHHvZUPJNZOBFr1xgDnNKg\n","JRHm7Nl7DmC/Vp8WErdOZHW+DpTIE1qzH11/qzv1MkmjxKZ1iGFblVKDFD3FEBQyfxhvGcudF4VY\n","Wg1XPo7lYKv5dw3jG3mANOJy7fDcpO8xSZBvhoPe459TE+QoSOODiCJApiIt4wlEmGD9uj6+0jiW\n","sLUjtPu1K7EYUMBe9CXgTxnjcaayB7lSwe8aKpPPmZ7a1ebi3gvgnUsAAABvQZ+TRRUsO/8AAAMB\n","nv1wmyva2qShO/7BbsNk2/sE3w1LG+q11Ud37w8aKYllSqV+gDEBObpz4jl4+GX+p29OtpT+UkgP\n","2QcAACRsMtx/uuhMF8Lcr2eYNWiefg6uX2OwyFEXRSErV/knhl0lpOOAAAAAUwGfsnRDfwAAAwJL\n","ZZMrKQFyG1Nkfzenjjn6kdGXseg08Np5mNsf375QBC+21bYHiyga8cqVLtguuAAaeJGgJnwgU9Ix\n","kMMCPHWPWQKPXMN4ec/LAAAAWwGftGpDfwAAAwJLA8+IAOi/nWQ0FJfk5PFXy00Amnm3P6uh38vx\n","6GXri03K9iqFbiUHgfNxZTb3c0122XUnkvm4RjN2gz6vD6u7heUT15SjVwQW9r3bmCycVsEAAAD8\n","QZu5SahBbJlMCCP//rUqgAAAZ/YuGvBz5wA4YfEBu2M5OiFoxQT/E7qEFmPAJ2s9sgy/TOxWyyzu\n","fj08JPRTyiysNOXYb5gjL23dbwQLCeOhAjbXemBIcun62ynNdpITUC8g6KuG7P7QK/jsjF2w0Gc0\n","umKE243lDsBCn5GDRTIHYx4b8v7md7gD6RTcgPVnHh4kVPUVE/4zoyQwzSQvnFMayOIxQh2egO9b\n","vIrZApVpe/hzHlesgX36p7Bi5OERe9Ox21DnbE4RzXzoumTQpv2tuelPMbUO0YwPekpsRJm9R9G5\n","EWic1DVfpuHE1Jrfwt+Mge7lCOOBvDakAAAAcEGf10UVLDv/AAADAZRSrOMEAKxCSAODIu1PPQ3l\n","uABImb14RdSDAE3KSbxTDQ8lwH8R1LhqV7s+EVhIaEiE+HVtHWWsvfOw+mGFdAquPRSePinsbdpO\n","a6xKnMeKBtnm8ljtWphrxDp87jnPLIrDFbEAAABLAZ/2dEN/AAADAjqSpFozQHKpQB88mD2Lw7Ws\n","DmaTPihqqrQYEYrHDjUD3NgFFHCBi5Yh0LpNpA6TsOAspHCUqd9iWsU4XzhBtmVtAAAAUgGf+GpD\n","fwAAAwI7VGi1rEHIq0ns23N7Sr6V1KQOBzsu23gACDrFX9LguOeuWiZqeOiubNMgi6k7kxhuPIMP\n","jWHZwVdSB14zcVfHEUoRbFZmR/wAAAD2QZv9SahBbJlMCCP//rUqgAAAaBxbPK/JJBXcwARJlU0j\n","t291q6uENunsoZ3wawqr8mU/j2aKYHHdprsdRJUs31xL2Q5cK017Rqis2kEMWirv++g8MS1b+Pmy\n","VbUhdwzVzTHXj4o2vkhCrGrtR5KnMxULAm6P+A+dXNFIv6u3WPWaRdnT24dZGEAmz/R7n8kOs7Sc\n","P9pPnhAJyOtnKetLoCbUGN8na5uS8k6jy//wkWBFlSfIwn5UVnpC14Cle6Qa2MCAqvm6OpWmXBkC\n","HsDDO7rL88x6ih6W5Nt/cu72MPrjq5GLTdGW2v6UsWpnXZIBOT+pO0HBAAAAWEGeG0UVLDv/AAAD\n","AZT9cNtUh8ANFgIvVEd3Zfo6abHiciCHDICUZ2/mLBQOIj0q3vOJVKD8AZ63TeTYuwVikxGCrJ/8\n","1Xo+05BJZP1SHxauVvro7YUugeAAAABcAZ46dEN/AAADAjNY01il1J8tMU5U4cgJo1JxM3HuAuuW\n","QfmW7aQvuOit0NQReBaPcPizzYzljYyTce14vI18rXNethjE2XY3x+k6CYYWwymSkiFLLxI9FN9l\n","6EEAAABmAZ48akN/AAADAjsDJupAAtOvzZ40n9CD1Vf52Dp95JTtR5OrwuYsjSwVQWQ8ixwSkVuo\n","WX16pAJATsXHFs7gAD/LLf4Y2z1sBrl3rVnRAGSFb6Gi8EwleEEiSuAYekSwtkvsgcvBAAAAwkGa\n","IUmoQWyZTAgh//6qVQAAAwDQcX7xTRwfIPE+QzC2thZzraDNTTWUvYHMCZq8HMAtdEHzXyJp/j2A\n","vU+9pPkJX/4UNqN/O4SZBnXF/DTpB11NmdG8kNyH3tFB5RQUzimGVfC4yGi4KDGsGR+6MS8ubtfJ\n","IlXGsOnhJCtesuyZY7kwncFSd3x8v8HrqRyLaBoxXoJHE4ePAAYHTRrUgTBaGpN7ggMYvw8U3FIP\n","p7MIXjbfBiDC1hccn5bXExDW3AT3AAAAUEGeX0UVLDv/AAADAZTaPLNncin1gRMAIBqMDK+nSzny\n","iX+h15qjjmnRB4oy4PeynTdCFhYSIL7YOjJj9f36VAnCW3kQ4rYpp3mMEDgC2/UgAAAAVgGefnRD\n","fwAAAwI7CxpQsoJDw41D6Raf+L99wyCT0E+U+9OINgCvcolgmYwATOirD2+XmuDMhYvlv7D+vugA\n","S4gR+F80DrShfObz5dp1c1hHofRGv+m5AAAAVAGeYGpDfwAAAwIq1Ju6QvzgmkIO80oKmhQiMXtv\n","mrbdZOH782qLcZ39Sah591VHn9Xm1BkDNr22B5ue+Hza4AOTRp41irmZnqtL2cDQnHdxf4oIHwAA\n","AM5BmmVJqEFsmUwIf//+qZYAAAMDGInHHISUNQA/KhNDgxxR35FPPFPo1SqNg0exeOuQ6sLe7R0n\n","3cSwOTyAGCsVDq0Q+HJTe6Jm7DbpvvvWsNFX/U/avOQNfQdNWq0s/O0T6zBoqnPaCEKVQl2+uuWt\n","PyLuqS2epuzax4XQY853lFCZR1h8eQA0TFTQsDmr+G7ormBb3gbK/yhqztD3wW2ant42WAVkY7lE\n","WScxjoQLst2k8X1rtevg2v8QvIfyFdxCpRboGphPdryjiOWIgQAAAGpBnoNFFSw7/wAAAwGKyid9\n","gAF1QfAZZ472tEKWTw3tFyrWwJqVkTYD0R3WLzh//2vBHhN90BwnND+BJBhkyihDvXyyT4CQTiEa\n","NDRVeKEE+eEMt0Ow4+H3mPARxr4wex8pEYzMXQfeCk/MAAAAZQGeonRDfwAAAwIq6/QCWuULGIhl\n","63kAnvV57QskQhjdr/qqCPIgBNedo2W6VuLXk4K/ucbUYRPxhvVZmhsoNjsrAXa2+XVeouY+qQBt\n","65OVOIBCPbHMvIFlIDybYqATe64b41gHAAAAbAGepGpDfwAAAwIrUA3pmYVMYhzbfyIStgVfN9Io\n","APP75HSAEbN/1+UfwmCuc2FehuhDvhJmDG0Afm7Heq+qRdUHLpvpKlZbiHsX9nLlunictI6XpEsl\n","ZJp9Y56RVa9SeDVnudtzRNw6OsffgQAAAG5BmqhJqEFsmUwIb//+p4QAAAYn2TXnqHnwZbXR+4YX\n","shrqDNRWWd0PYKvHgQM0FnV89O7rBiLiyY1U9mfngAOy8rTs3LhQP+DuwHsAoMBKkDt1ZY0yLSGx\n","/PkLkCX+5qQK+1nbOuSPkJzNm1a1YQAAAHdBnsZFFSw3/wAAAwIp8xnT3BQA7pHNJ+ZS0q41XxOd\n","x7gcf9QpPQmFRvFeg7Lu4SgUPvkQCcMD1vaMV4JOaPwyAMU+s1SNPT5CGVECbrFYe2Y6wdFL9IQm\n","dg5KG66zluE4ujBNLtvxESc5q1cTF1dEY8o7Bgx1QQAAAFwBnudqQ38AAAMCKxcNiKKct1nTw7oz\n","scymCGQQAmvNy3HAtCsXQvwwauMRwSRdKSSIvwQV/lL/Cmp7LwSR24qA+wH9sb2L1pjyybOEg+/8\n","94yK+VHDaGvPVvxllAAABoNtb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAAchAABAAABAAAA\n","AAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAA\n","AAAAAAAAAAAAAAAAAAACAAAFrXRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAAchAAA\n","AAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAACWAAAAZAA\n","AAAAACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAAHIQAAAgAAAEAAAAABSVtZGlhAAAAIG1kaGQAAAAA\n","AAAAAAAAAAAAACgAAAEkAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVv\n","SGFuZGxlcgAAAATQbWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAA\n","AAABAAAADHVybCAAAAABAAAEkHN0YmwAAAC0c3RzZAAAAAAAAAABAAAApGF2YzEAAAAAAAAAAQAA\n","AAAAAAAAAAAAAAAAAAACWAGQAEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n","AAAAAAAAAAAY//8AAAAyYXZjQwFkABb/4QAZZ2QAFqzZQJgz5eEAAAMAAQAAAwAUDxYtlgEABmjr\n","48siwAAAABx1dWlka2hA8l8kT8W6OaUbzwMj8wAAAAAAAAAYc3R0cwAAAAAAAAABAAAASQAABAAA\n","AAAUc3RzcwAAAAAAAAABAAAAAQAAAkBjdHRzAAAAAAAAAEYAAAABAAAIAAAAAAEAABQAAAAAAQAA\n","CAAAAAABAAAAAAAAAAEAAAQAAAAAAQAAFAAAAAABAAAIAAAAAAEAAAAAAAAAAQAABAAAAAABAAAU\n","AAAAAAEAAAgAAAAAAQAAAAAAAAABAAAEAAAAAAEAABAAAAAAAgAABAAAAAABAAAUAAAAAAEAAAgA\n","AAAAAQAAAAAAAAABAAAEAAAAAAEAABAAAAAAAgAABAAAAAABAAAUAAAAAAEAAAgAAAAAAQAAAAAA\n","AAABAAAEAAAAAAEAAAgAAAAAAQAAFAAAAAABAAAIAAAAAAEAAAAAAAAAAQAABAAAAAABAAAUAAAA\n","AAEAAAgAAAAAAQAAAAAAAAABAAAEAAAAAAEAAAwAAAAAAQAABAAAAAABAAAUAAAAAAEAAAgAAAAA\n","AQAAAAAAAAABAAAEAAAAAAEAABQAAAAAAQAACAAAAAABAAAAAAAAAAEAAAQAAAAAAQAAFAAAAAAB\n","AAAIAAAAAAEAAAAAAAAAAQAABAAAAAABAAAUAAAAAAEAAAgAAAAAAQAAAAAAAAABAAAEAAAAAAEA\n","ABQAAAAAAQAACAAAAAABAAAAAAAAAAEAAAQAAAAAAQAAFAAAAAABAAAIAAAAAAEAAAAAAAAAAQAA\n","BAAAAAABAAAUAAAAAAEAAAgAAAAAAQAAAAAAAAABAAAEAAAAAAEAABQAAAAAAQAACAAAAAABAAAA\n","AAAAAAEAAAQAAAAAAQAAEAAAAAACAAAEAAAAABxzdHNjAAAAAAAAAAEAAAABAAAASQAAAAEAAAE4\n","c3RzegAAAAAAAAAAAAAASQAABL0AAAEQAAAAQwAAACgAAAArAAABLwAAAGUAAAA4AAAAIwAAARQA\n","AABCAAAANAAAACMAAABhAAAASQAAAB8AAADyAAAAaAAAAEQAAABKAAAA0AAAADcAAAA5AAAA0wAA\n","AFMAAAAmAAAANQAAALUAAADlAAAASgAAAC4AAABDAAABDQAAAFcAAABJAAAATwAAAQEAAAAxAAAA\n","8QAAAGAAAAA7AAAAOgAAAQgAAACBAAAAMAAAAFYAAADtAAAAdgAAAEkAAABNAAAA1wAAAHMAAABX\n","AAAAXwAAAQAAAAB0AAAATwAAAFYAAAD6AAAAXAAAAGAAAABqAAAAxgAAAFQAAABaAAAAWAAAANIA\n","AABuAAAAaQAAAHAAAAByAAAAewAAAGAAAAAUc3RjbwAAAAAAAAABAAAALAAAAGJ1ZHRhAAAAWm1l\n","dGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAA\n","AB1kYXRhAAAAAQAAAABMYXZmNTguMjkuMTAw\n","\">\n","  Your browser does not support the video tag.\n","</video>"]},"metadata":{}}]},{"cell_type":"markdown","source":["## Q Learning\n","\n","##### **1. Implement (non-deep) Q Learning for CartPole.**"],"metadata":{"id":"kt_RfDOcogd8"},"id":"kt_RfDOcogd8"},{"cell_type":"markdown","source":["Before training can begin two things need to be set up. This first of these are bins which will represent the environment with *discrete* values instead of *continuous* values. This will lower the number of states that need to be adjusted.  \n","For CartPole-v1 the cart location, cart speed, pole and pole speed are recorded in the environment. Each of these has $n$ bins and have been given a range for each bin.\n","\n","The Q table also needs to be set up. This has the same shape as the bins with the addition axis for a number of actions that can be performed. For CartPole this is 2. "],"metadata":{"id":"6CUxlBedpT74"},"id":"6CUxlBedpT74"},{"cell_type":"code","source":["def create_bins_and_q_table():\n","    # env.observation_space.high\n","    # [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n","    # env.observation_space.low\n","    # [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n","\n","    numBins = 20\n","    obsSpaceSize = len(env.observation_space.high)\n","\n","    # Get the size of each bucket\n","    bins = [\n","          np.linspace(-4.8, 4.8, numBins),\n","          np.linspace(-4, 4, numBins),\n","          np.linspace(-.418, .418, numBins),\n","          np.linspace(-4, 4, numBins)\n","    ]\n","\n","    # Create a q table of size (20, 20, 20, 20, 2) = 320'000 total entries\n","    q_table = np.random.uniform(low=-2, high=0, size=([numBins] * obsSpaceSize + [env.action_space.n]))\n","\n","    return bins, obsSpaceSize, q_table\n","\n","\n","def get_discrete_state(state, bins, obsSpaceSize):\n","    stateIndex = []\n","    for i in range(obsSpaceSize):\n","        stateIndex.append(np.digitize(state[i], bins[i]) - 1) # -1 will turn bin into index\n","    return tuple(stateIndex)"],"metadata":{"id":"PcDIQBjkqR60","executionInfo":{"status":"ok","timestamp":1684945430174,"user_tz":-120,"elapsed":7,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"PcDIQBjkqR60","execution_count":19,"outputs":[]},{"cell_type":"code","source":["bins, obsSpaceSize, q_table = create_bins_and_q_table()\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","Iter = 300\n","\n","for i in range(1, Iter+1):\n","    discrete_state = get_discrete_state(env.reset()[0], bins, obsSpaceSize)\n","    \n","    done = False\n","    \n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[discrete_state]) # Exploit learned values\n","\n","        next_state, reward, done, truncated, info = env.step(action) \n","        discrete_next_state = get_discrete_state(next_state, bins, obsSpaceSize)\n","        \n","        old_value = q_table[discrete_state, action]\n","        next_max = np.max(q_table[discrete_next_state])\n","        \n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","        q_table[discrete_state, action] = new_value\n","\n","        state = discrete_next_state\n","        \n","        if truncated:\n","          break"],"metadata":{"id":"pDyNpDo1ootE","executionInfo":{"status":"ok","timestamp":1684945430174,"user_tz":-120,"elapsed":6,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"pDyNpDo1ootE","execution_count":20,"outputs":[]},{"cell_type":"code","source":["# rerun for 100 episodes to get average reward \n","# same function as `run` but modified to get discrete states\n","\n","n_episodes = 100\n","\n","average_reward = 0   # cumulative reward averaged over all the episodes\n","episodeframes = []\n","\n","for i in range(1, n_episodes+1):\n","    frames = []   \n","    discrete_state = get_discrete_state(env.reset()[0], bins, obsSpaceSize)\n","    ep_reward = 0\n"," \n","    done = False\n","    \n","    while not done:\n","        action = np.argmax(q_table[discrete_state])\n","        next_state, reward, done, truncated, info = env.step(action) \n","        discrete_next_state = get_discrete_state(next_state, bins, obsSpaceSize)\n","        \n","        ep_reward += reward\n","        frames.append(env.render())\n","        \n","        if truncated:\n","            break\n","\n","    average_reward += ep_reward / float(n_episodes) \n","    episodeframes.append(frames)"],"metadata":{"id":"csoMfuS9s4Sq","executionInfo":{"status":"ok","timestamp":1684945432773,"user_tz":-120,"elapsed":2604,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}}},"id":"csoMfuS9s4Sq","execution_count":21,"outputs":[]},{"cell_type":"code","source":["average_reward"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"62hxPq_fua4P","executionInfo":{"status":"ok","timestamp":1684945432774,"user_tz":-120,"elapsed":9,"user":{"displayName":"Irene Ferfoglia","userId":"04646074008398264928"}},"outputId":"28339858-4a25-447d-ae86-be4df4412b62"},"id":"62hxPq_fua4P","execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.36999999999999"]},"metadata":{},"execution_count":22}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["J_8j4zOM-sn7","jz_42J0xBCPa","vdanLewyEBt0","WcykUQdGFc3e","jfX63aZ6IawB"]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}